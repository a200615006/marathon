{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd904dc2-b816-432f-bf16-6790cb770384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (VectorStoreIndex, SimpleDirectoryReader, load_index_from_storage\n",
    "    , Document, Settings, StorageContext, PromptTemplate)\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.extractors import KeywordExtractor, SummaryExtractor\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.dashscope import DashScope\n",
    "\n",
    "from llama_index.extractors.entity import EntityExtractor\n",
    "from llama_index.readers.file import UnstructuredReader,PyMuPDFReader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "import os, re, asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8c75a3f-3416-499a-9ec9-edc255829118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-30 16:39:57,335 - INFO - Load pretrained SentenceTransformer: ./Qwen3-Embedding-0.6B\n",
      "2025-09-30 16:39:58,539 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型嵌入维度: 1024\n"
     ]
    }
   ],
   "source": [
    "embedding_model = \"./Qwen3-Embedding-0.6B\"\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=embedding_model,\n",
    "    cache_folder=None,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained(embedding_model, trust_remote_code=True, local_files_only=True)\n",
    "dimension = config.hidden_size\n",
    "print(f\"模型嵌入维度: {dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77249092-c390-4c43-a08d-44631dff2612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_model = \"./Qwen3-4B-Thinking-2507\"\n",
    "# Settings.llm = HuggingFaceLLM(\n",
    "#     model_name=main_model,\n",
    "#     tokenizer_name=main_model,\n",
    "#     generate_kwargs={\"temperature\": 0.1, \"top_p\": 0.7},\n",
    "#     device_map=\"cuda\",\n",
    "#     max_new_tokens=512\n",
    "# )\n",
    "\n",
    "Settings.llm = DashScope(\n",
    "    api_key=\"sk-7afc6caf37b64d069ef3be129e68753a\",\n",
    "    model=\"qwen3-max\",\n",
    "    generate_kwargs={\"temperature\": 0.1, \"top_p\": 0.7},\n",
    "    max_new_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e20ec199-841c-4eef-b612-7cc4422d8688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "绝对数据库路径: /root/marathon/milvus_test/milvus_lite.db\n",
      "已创建 ./milvus 目录\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    }
   ],
   "source": [
    "milvus_dir = \"./milvus_test\"\n",
    "milvus_db_path = os.path.join(milvus_dir, \"milvus_lite.db\")\n",
    "abs_db_path = os.path.abspath(milvus_db_path)\n",
    "print(f\"绝对数据库路径: {abs_db_path}\")\n",
    "\n",
    "if not os.path.exists(milvus_dir):\n",
    "    os.makedirs(milvus_dir)\n",
    "    print(\"已创建 ./milvus 目录\")\n",
    "\n",
    "milvus_vector_store = MilvusVectorStore(\n",
    "    uri=f\"{abs_db_path}\",\n",
    "    collection_name=\"rag_collection\",\n",
    "    dim=1024,\n",
    "    overwrite=True\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=milvus_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "567914c9-37a4-4739-bf70-da546fa297e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text).strip()\n",
    "    # text = re.sub(r'(\\w+\\s*){3,}\\n', '', text)\n",
    "    # text = re.sub(r'[^a-zA-Z0-9\\u4e00-\\u9fa5\\s\\.,!?]', '', text)  # 去除特殊字符，保留中英文\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b345e0de-d5ab-4f42-8a2f-da4bbb4bc79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_summary_async(text, max_words=20):\n",
    "    prompt = f\"总结以下文本，不超过{max_words}字，直接回复结果：{text}\"\n",
    "    response = await Settings.llm.acomplete(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "def generate_summary(text, max_words=20):\n",
    "    prompt = f\"总结以下文本，不超过{max_words}字，直接回复结果：{text}\"\n",
    "    response = Settings.llm.complete(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "async def add_summaries_to_nodes_async(nodes_list):\n",
    "    tasks = [generate_summary_async(node.text) for node in nodes_list]\n",
    "\n",
    "    summaries = []\n",
    "    for future in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"生成节点摘要进度\"):\n",
    "        summary = await future\n",
    "        summaries.append(summary)\n",
    "\n",
    "    for node, summary in zip(nodes_list, summaries):\n",
    "        node.metadata[\"node_summary\"] = summary\n",
    "        \n",
    "def add_summaries_to_nodes(nodes_list):\n",
    "    for node in tqdm(nodes_list, desc=\"生成摘要\"):\n",
    "        summary = generate_summary(node.text)\n",
    "        node.metadata[\"node_summary\"] = summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "619e3fe0-1330-4417-be61-b13e6867ee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件大小:326\n"
     ]
    }
   ],
   "source": [
    "qwen_tokenizer = AutoTokenizer.from_pretrained(\"./Qwen3-Embedding-0.6B\", trust_remote_code=True)\n",
    "documents_dir = \"./docs\"\n",
    "\n",
    "file_extractor = {\n",
    "    \".pdf\": PyMuPDFReader(), \n",
    "    \".docx\": UnstructuredReader()\n",
    "}\n",
    "reader = SimpleDirectoryReader(input_dir=documents_dir, recursive=True, file_extractor=file_extractor)\n",
    "documents = reader.load_data()\n",
    "\n",
    "cleaned_documents = [Document(text=clean_text(doc.text), metadata=doc.metadata) for doc in documents]\n",
    "documents = cleaned_documents\n",
    "\n",
    "print(f\"文件大小:{len(documents)}\")\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=100, tokenizer=qwen_tokenizer.tokenize)\n",
    "#nodes = node_parser.get_nodes_from_documents(documents)\n",
    "#print(f\"节点数量:{len(nodes)}\")\n",
    "\n",
    "#asyncio.run(add_summaries_to_nodes_async(nodes))\n",
    "#add_summaries_to_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbc61554-078f-4042-83c7-720d344f0f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_summaries_to_json(nodes_list, file_path=\"nodes_summaries_temp.json\"):\n",
    "    summaries_dict = {}\n",
    "    for idx, node in enumerate(nodes_list):\n",
    "        summaries_dict[str(idx)] = node.metadata.get(\"node_summary\", \"\")  # 获取摘要，若无则为空\n",
    "    \n",
    "    # 保存到 JSON\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summaries_dict, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"节点摘要已保存到 {file_path}\")\n",
    "\n",
    "def load_summaries_to_nodes(nodes_list, file_path=\"nodes_summaries.json\"):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        summaries_dict = json.load(f)\n",
    "    sorted_keys = sorted(summaries_dict.keys(), key=int)\n",
    "\n",
    "    for key in sorted_keys:\n",
    "        idx = int(key)\n",
    "        if idx < len(nodes_list):\n",
    "            nodes_list[idx].metadata[\"node_summary\"] = summaries_dict[key]\n",
    "        else:\n",
    "            print(f\"警告：索引 {idx} 超出节点列表长度，跳过。\")\n",
    "    \n",
    "    return nodes_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f7ab110-62f0-4da3-bce4-cd8775eb24d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "节点摘要已保存到 nodes_summaries.json\n"
     ]
    }
   ],
   "source": [
    "save_summaries_to_json(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fff6d82-bcf3-4272-bfdf-5c6d8209f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword_extractor = KeywordExtractor(\n",
    "#     llm=Settings.llm,  # 使用 LLM 提取关键词作为主题\n",
    "#     keywords=5,  # 提取前 5 个关键词\n",
    "#     prompt_template_str=\"\"\"\n",
    "#     从以下文本中提取 2 个主要关键词。\n",
    "#     输出格式：仅用逗号分隔的关键词列表，不要添加任何解释或额外文本。\n",
    "#     示例输出：关键词1,关键词2\n",
    "\n",
    "#     文本：{text}\n",
    "#     \"\"\"\n",
    "# )\n",
    "# max_words = 20\n",
    "# summary_extractor = SummaryExtractor(\n",
    "#     llm=Settings.llm,\n",
    "#     summaries=[\"self\"],\n",
    "#     prompt_template_str=\"\"\"\n",
    "#     总结以下文本，不超过20字，直接回复结果：\\n{text}\n",
    "#         \"\"\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "775b208d-7576-4475-89ad-5cafb8a9cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents,\n",
    "#     storage_context=storage_context,\n",
    "#     embed_model=Settings.embed_model,\n",
    "#     node_parser=node_parser,\n",
    "#     store_nodes_override=True\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "transformations = [node_parser]\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=Settings.embed_model,\n",
    "    # node_parser=node_parser,\n",
    "    transformations=transformations,\n",
    "    store_nodes_override=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e313de7b-3c72-4774-b09b-495c9f47fddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n",
      "节点数量: 353\n"
     ]
    }
   ],
   "source": [
    "nodes = list(index.docstore.docs.values())\n",
    "print(len(documents[0].text))\n",
    "print(f\"节点数量: {len(nodes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94296e03-1489-43a4-aa3a-58af278065ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_nodes = load_summaries_to_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6257d0e6-daf0-454c-aae0-cdd242a23236",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex(\n",
    "    loaded_nodes, \n",
    "    storage_context=storage_context, \n",
    "    embed_model=Settings.embed_model, \n",
    "    store_nodes_override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6866ae85-1668-4cb9-96d4-739d5c78126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_with_sums = list(index.docstore.docs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac47622a-bff0-40c5-8268-4a64e6d2f2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "李宏毅深度学习教程1.2.4版，2025年发布。\n",
      "李宏毅老师的《机器学习》课程幽默易懂，适合深度学习入门。\n",
      "三位作者简介及成就概述。\n",
      "数学与机器学习中常用符号及其含义说明。\n",
      "书籍介绍机器学习基础与实践方法论。\n",
      "深度学习基础：局部极小值、批量、动量及自适应学习率。\n",
      "学习率调度、优化总结、分类及批量归一化等内容介绍。\n",
      "卷积神经网络通过感受野检测局部模式，适用于图像各区域。\n",
      "卷积与循环神经网络简介及应用\n",
      "RNN、双向循环神经网络及LSTM原理与应用。\n",
      "自注意力机制及其在Transformer中的应用。\n",
      "第7章介绍Transformer及其在多种任务中的应用。\n",
      "Transformer结构及其编码器、解码器和训练过程介绍。\n",
      "文本讨论了引导注意力、束搜索等技术及生成对抗网络等内容。\n",
      "GAN理论、WGAN算法及训练难点，扩散模型与自监督学习简介。\n",
      "介绍生成式预训练模型和自编码器的概念及用途。\n",
      "自编码器应用与对抗攻击简介\n",
      "介绍了被动防御、主动防御及迁移学习和强化学习的概念与应用。\n",
      "介绍未知函数、定义损失、优化及多种评价动作标准和元学习概念。\n",
      "元学习的步骤、与机器学习关系、算法及应用。\n",
      "书籍内容涵盖终身学习、网络压缩及可解释性AI。\n",
      "讨论了决策树可解释性、ChatGPT及Minecraft纯视觉研究。\n",
      "介绍了一种新的图计算方法及其在行为学习中的应用。\n",
      "机器学习基础介绍及分类、回归和结构化学习概述。\n",
      "机器学习中模型参数估计与损失函数定义。\n",
      "通过计算每日预测误差并求平均得到损失L。\n",
      "机器学习中通过调整参数w和b来最小化损失L。\n",
      "通过计算损失函数的偏导数调整参数w以最小化损失。\n",
      "梯度下降可能陷入局部最小值，而非全局最小值。\n",
      "梯度下降法更新w和b以最小化损失L。\n",
      "观看人次预测模型从考虑1天改进到考虑7天，误差降低。\n",
      "线性模型预测观看人次，考虑天数增加可降低损失。\n",
      "蓝线斜坡与红线斜坡对齐，通过叠加得到红线形状。\n",
      "分段线性曲线可由多个蓝色函数组合逼近任何连续曲线。\n",
      "! 欢![](https://i.imgur.com/1![](https://i.imgur.com/) Sigmoid函数可以![](https://i.imgur.com/)\n",
      "\n",
      "看起来![](https://i.imgur.com/)\n",
      "\n",
      "看起来您提供的文本中包含了一些不连贯的信息和排版错误，我尝试整理并总结了关键点![](https://i.imgur.com![]() 信息如下：\n",
      "\n",
      "Sigmoid函数 \\(y = \\frac{c}{1 + e^{-(b+wx_1)}\\) 可以用来逼近Hard Sigmoid函数。当![](https://i.imgur.com/)\n",
      "\n",
      "当\\(x_1\\)趋![](https://i.imgur.com/) 趋近于无穷大时，\\(y\\)趋近于常数\\(c\\)；当\\(x_1\\)非常小时，\\(y\\)趋近于0。Sigmoid函数因其形状类似![](https://i.imgur.com/)似“S”型而得名。\n",
      "\n",
      "注意：原文![](![](https://i.imgur.com/) 文本中提到的图片链接无效或缺失，![](https://i![](https://i.imgur.com/) ，且部分内容似乎有误或未完整给出。希望这能帮到您理解\n",
      "通过调整Sigmoid函数参数逼近各种分段线性函数。\n",
      "使用Sigmoid函数和线性代数处理特征向量。\n",
      "讨论了参数优化、损失函数及Sigmoid数量的作用。\n",
      "梯度下降法更新参数过程详解。\n",
      "梯度下降中通过分批数据计算损失并更新参数。\n",
      "梯度下降更新参数次数取决于批量大小；模型可使用不同激活函数变形。\n",
      "增加ReLU层数可降低训练和测试数据损失。\n",
      "神经网络与深度学习的发展及除夕预测案例分析。\n",
      "ReLU实验显示3层网络优于4层，减少过拟合。\n",
      "京东购苹果书，模型存在过拟合问题。\n",
      "机器学习实践方法论：解决模型偏差与优化问题。\n",
      "模型损失低但梯度下降无效，需判断是灵活性不足还是优化问题。\n",
      "讨论了模型优化问题及如何判断模型偏差与过拟合。\n",
      "过拟合现象：模型在训练数据上表现好，但在测试数据上损失大。\n",
      "解决过拟合：增加训练数据或限制模型灵活性。\n",
      "模型复杂度与限制需平衡，避免过拟合和欠拟合。\n",
      "测试集分公开和私人以避免过拟合。\n",
      "模型选择应基于验证损失，避免公开测试集过拟合。\n",
      "模型预测2月26日观看人数出现不匹配错误。\n",
      "图像分类中遇到不匹配问题时增加数据无效。\n",
      "深度学习优化问题及解决方法，包括局部极小值、鞍点和学习率调整。\n",
      "通过泰勒级数近似判断损失函数临界点类型。\n",
      "通过海森矩阵特征值判断临界点性质。\n",
      "通过海森矩阵判断损失函数临界点类型及参数更新方向。\n",
      "通过特征向量方向更新参数可逃离鞍点，但实际操作复杂。\n",
      "实验支持假说，图3.6展示神经网络训练中损失与特征值关系。\n",
      "误差表面与鞍点，批量和动量在神经网络训练中的应用。\n",
      "批量大小影响梯度下降法的更新频率与稳定性。\n",
      "随机梯度下降易逃局部最小值，批量大小影响GPU计算时间。\n",
      "批量大小影响计算效率与模型训练效果。\n",
      "小批量梯度下降有助于避免过拟合并提高测试性能。\n",
      "小批量优化和动量法有助于避开局部最小值。\n",
      "动量法在梯度下降中考虑历史梯度，帮助逃离局部最小值。\n",
      "动量法和自适应学习率在优化算法中的作用。\n",
      "训练网络时损失和梯度变化，及学习率影响分析。\n",
      "梯度下降在凸误差表面也难训练，需自适应学习率如AdaGrad。\n",
      "参数相关学习率通过梯度均方根调整，使更新更有效。\n",
      "介绍Adagrad和RMSprop算法，自动调整学习率。\n",
      "AdaGrad与RMSProp在处理梯度重要性上的差异及动态调整机制。\n",
      "Adam是最常用优化器，学习率调度可改善训练效果。\n",
      "介绍学习率衰减、预热及其在优化算法中的作用。\n",
      "动量和均方根以不同方式使用梯度，不会互相抵消。\n",
      "分类中使用softmax函数进行归一化和增强值差距。\n",
      "softmax与sigmoid在二分类时等价，交叉熵常用于分类损失。\n",
      "交叉熵比均方误差更适合分类任务的优化。\n",
      "线性模型中输入特征值范围差异大导致斜率不同。\n",
      "特征归一化使不同维度数值范围一致，便于训练。\n",
      "特征归一化有助于梯度下降和训练顺利。\n",
      "批量归一化作为网络一部分处理特征归一化。\n",
      "批量归一化中加入β和γ以调整输出分布，初始设为0和1。\n",
      "批量归一化通过移动平均更新参数，提高训练速度和稳定性。\n",
      "批量归一化改善训练，但未必因解决内部协变量偏移。\n",
      "文献综述：大批次训练、优化及归一化方法在深度学习中的应用。\n",
      "论文介绍权重归一化和谱范数正则化技术。\n",
      "卷积神经网络用于图像分类，将图像转换为向量输入。\n",
      "图像识别模型权重多易过拟合，输出用独热向量分类。\n",
      "神经网络检测图像模式时，不需整张图，只需局部特征。\n",
      "神经元权重与感受野设计在神经网络中的应用。\n",
      "感受野不一定要相连，但通常相连以检测图像模式。\n",
      "卷积操作中，步幅和填充确保图像每个位置都被感受野覆盖。\n",
      "图像中相同模式在不同位置出现可被共享参数的神经元检测。\n",
      "神经元可共享参数，即使输入不同输出也不同。\n",
      "卷积神经网络通过感受野和参数共享处理图像。\n",
      "滤波器通过学习确定参数，用于图像特征检测生成特征映射。\n",
      "卷积层通过堆叠和滤波器检测图像模式。\n",
      "卷积神经网络中滤波器的作用及下采样不影响模式检测。\n",
      "图像识别中使用无参数的汇聚操作来减小图像尺寸。\n",
      "卷积与汇聚在图像识别中的应用及作用。\n",
      "卷积神经网络在下围棋中的应用，如AlphaGo。\n",
      "卷积神经网络适用于围棋因棋盘可视为19×19图像。\n",
      "卷积神经网络在图像、语音和文字处理中的应用与限制。\n",
      "介绍卷积神经网络在语音识别和情感分类中的应用。\n",
      "循环神经网络及其在槽填充中的应用。\n",
      "词哈希与RNN在处理序列信息时的应用。\n",
      "循环神经网络通过记忆元处理序列数据，考虑顺序影响输出。\n",
      "循环神经网络运算示例及不同类型RNN介绍。\n",
      "双向循环神经网络提高槽填充性能。\n",
      "LSTM通过输入、输出和遗忘门控制记忆单元的读写。\n",
      "LSTM结构及记忆元工作原理，含输入、遗忘与输出门控制。\n",
      "LSTM运算示例详解及过程展示。\n",
      "LSTM运算示例，通过不同输入展示记忆元变化。\n",
      "LSTM原理：一个神经元，四个输入，参数量是普通网络四倍。\n",
      "LSTM与GRU的工作原理及结构介绍。\n",
      "RNN通过定义损失函数和梯度下降进行学习。\n",
      "RNN训练中的学习曲线与梯度裁剪技巧。\n",
      "RNN训练难题源于权重在时间序列中反复使用，LSTM通过特殊结构缓解梯度消失问题。\n",
      "其他技术如顺时针RNN、SCRN可处理梯度消失问题。\n",
      "RNN在情感分析和关键术语抽取等任务中的应用。\n",
      "CTC技术在语音识别中解决叠字问题。\n",
      "CTC和Seq2Seq在RNN中的应用及训练方法。\n",
      "序列到序列学习在翻译和句法解析中的应用及词袋方法的局限。\n",
      "序列到序列自编码器将文档转换为考虑词序的向量。\n",
      "自注意力机制处理可变长度的向量序列输入。\n",
      "词嵌入和声音信号处理中向量的应用。\n",
      "社交网络和分子可视为向量或图，用于药物发现等。\n",
      "介绍三种类型的输出：等长、单标签、序列到序列。\n",
      "序列标注问题及全连接网络的局限性。\n",
      "自注意力模型能更好地处理变长序列信息。\n",
      "自注意力模型通过计算向量间关联性生成输出。\n",
      "向量关联程度的计算方法及在自注意力模型中的应用。\n",
      "自注意力机制通过查询、键和值计算向量间关联性。\n",
      "自注意力机制通过矩阵运算生成q、k、v以抽取序列重要信息。\n",
      "从矩阵乘法角度理解自注意力机制的计算过程。\n",
      "自注意力机制通过矩阵乘法计算注意力分数并加权求和。\n",
      "自注意力机制通过学习Wq、Wk、Wv参数实现，多头版本可处理多种相关性。\n",
      "自注意力层缺少位置信息，需引入位置编码。\n",
      "位置编码在自注意力机制中加入位置信息，有多种生成方法。\n",
      "自注意力在NLP和语音处理中的应用及截断自注意力的提出。\n",
      "截断自注意力可加速运算，适用于图像处理。\n",
      "自注意力考虑整图信息，卷积神经网络仅考虑感受野。\n",
      "自注意力在大数据量下优于卷积神经网络，但小数据量时易过拟合。\n",
      "自注意力与循环神经网络在处理序列数据上的对比。\n",
      "图神经网络中自注意力的应用及变形研究\n",
      "Transformer模型介绍及其在序列到序列任务中的应用。\n",
      "序列到序列模型在语音合成和聊天机器人中的应用。\n",
      "序列到序列模型在自然语言处理中的广泛应用及局限。\n",
      "序列到序列模型应用于语法分析和多标签分类。\n",
      "序列到序列模型及其在多标签分类中的应用。\n",
      "Transformer编码器通过自注意力机制处理向量序列。\n",
      "Transformer使用层归一化而非批量归一化以提高性能。\n",
      "自回归解码器将编码器输出转换为文字序列。\n",
      "解码器错误识别会导致误差传播，使用掩蔽自注意力避免未来信息影响。\n",
      "解码器需加掩码以逐个生成输出并使用<EOS>结束。\n",
      "非自回归解码器：并行生成句子，控制输出长度。\n",
      "编码器-解码器注意力机制连接编码器与解码器，传递信息。\n",
      "解码器输出与标准答案的交叉熵越小越好，训练采用教师强制。\n",
      "复制机制、引导注意力和束搜索在序列到序列模型中的应用。\n",
      "解码器每次选择分数最高的A或B作为输出。\n",
      "贪心搜索与束搜索在解码器中的应用及比较。\n",
      "束搜索适用于明确任务，语音合成需加噪声和随机性。\n",
      "文本为关于序列预测、对象检测等的参考文献列表。\n",
      "生成模型介绍，包括生成对抗网络和生成器。\n",
      "视频预测小精灵游戏画面，需解决角色转向预测模糊问题。\n",
      "生成模型让机器拥有创造性输出能力，如GAN生成多样化结果。\n",
      "GAN中生成器与判别器的工作原理及训练过程。\n",
      "GAN通过生成器和判别器的对抗学习生成逼真图像。\n",
      "GAN算法通过交替训练生成器和判别器来生成逼真图片。\n",
      "GAN训练过程及生成人脸效果展示。\n",
      "GAN理论介绍及生成器与判别器交互产生人脸图片原理\n",
      "GAN训练目标是减小生成分布与真实分布的差异。\n",
      "GAN通过判别器最大化目标函数来区分真实与生成数据。\n",
      "WGAN改进了GAN训练，解决了JS散度的问题。\n",
      "GAN训练缺乏直观指标，Wasserstein距离提供新衡量方式。\n",
      "Wasserstein距离通过优化问题定义，能有效反映分布差异。\n",
      "WGAN使用Wasserstein距离替代JS距离，并需满足1-Lipschitz限制。\n",
      "Improved WGAN通过梯度惩罚使判别器成为1-Lipschitz函数。\n",
      "GAN训练难，尤其在文字生成上，需序列到序列模型。\n",
      "GAN训练文字生成难，ScratchGAN提出新方法。\n",
      "早期GAN论文缺乏客观评价指标，存在模式崩塌等问题。\n",
      "GAN生成的人脸虽真实但多样性不足，存在模式丢失问题。\n",
      "FID计算过程及GAN生成图片质量评估方法。\n",
      "条件型GAN通过给生成器添加文字条件以生成对应图片。\n",
      "条件型GAN通过配对数据训练，使生成图像与条件匹配。\n",
      "声音和图像成对数据易获取，可用于训练及生成动态图片。\n",
      "使用GAN在无成对数据情况下进行图像风格转换。\n",
      "Cycle GAN通过两个生成器实现图像风格转换并保持一致性。\n",
      "Cycle GAN双向架构及其他风格转换GAN介绍。\n",
      "Cycle GAN用于文字风格转换及其它无监督学习任务。\n",
      "扩散模型通过逐步去噪生成图像，如DDPM等。\n",
      "扩散模型通过预测噪声并去除来实现图片去噪。\n",
      "扩散模型结合文字生成图像，需图文配对数据训练。\n",
      "文生图的去噪过程及加入文字描述的前向过程。\n",
      "自监督学习是一种无标注数据的学习方式。\n",
      "自监督模型参数量对比及BERT模型介绍。\n",
      "BERT训练通过掩码和下一句预测两种方法实现。\n",
      "BERT的下一句预测任务被认为帮助不大，句序预测更有效。\n",
      "BERT通过填空等任务预训练，再微调以适应各种下游任务。\n",
      "GLUE包含9个任务，用于评估BERT等模型性能。\n",
      "模型与人类在自然语言处理任务上的差距及BERT的应用。\n",
      "BERT用于情感分析等任务，预训练加微调视为半监督学习。\n",
      "BERT处理词性标注和自然语言推理任务的方法。\n",
      "BERT在自然语言推理和基于提取的问答中的应用。\n",
      "BERT预训练模型用于问答任务，通过计算向量内积确定答案位置。\n",
      "BERT用于问答模型，通过注意力机制找答案起始和结束位置。\n",
      "BERT训练需大量资源，观察其过程可探索优化方法。\n",
      "T5模型实验与BERT嵌入向量解释。\n",
      "BERT根据上下文生成不同嵌入，相似上下文的嵌入更接近。\n",
      "BERT通过上下文学习词意，输出语境化词嵌入。\n",
      "探讨BERT在处理无意义序列如DNA时的有效性和原理。\n",
      "多语言BERT能跨语言自动学习问答能力。\n",
      "多语言BERT通过大量数据学习不同语言的对齐。\n",
      "多语言BERT用于无监督翻译及语言信息处理。\n",
      "多语言BERT在中文问答任务上表现优异。\n",
      "GPT模型基于Transformer解码器，用于预测下一个词元以生成文本。\n",
      "GPT模型的小样本学习能力及其在翻译等任务中的应用。\n",
      "GPT系列在不同样本学习任务中的表现及自监督学习的应用。\n",
      "自监督学习在自然语言、语音处理中的应用及评估方法。\n",
      "自编码器：一种自监督学习的预训练方法，通过编码解码过程学习数据表示。\n",
      "自编码器通过降维和重构实现无监督学习，适用于下游任务。\n",
      "自编码器通过简化表示减少训练数据需求。\n",
      "文本总结：该文讨论![](https://u.jd.com/89sdf) 段落主要讨论了自编码器，特别是去噪自编码器和BERT模型的结构与功能，强调了解码器需去除噪声还原原始输入，并提及了特征解耦的概念。\n",
      "自编码器与特征解耦技术概述\n",
      "语音转换需特征解耦技术以实现不同说话人间的声音转换。\n",
      "自编码器在离散隐表征中的应用及向量量化技术。\n",
      "向量量化变分自编码器及其在语音和文本中的应用。\n",
      "Seq2Seq自编码器用于无监督文本摘要，但需GAN辅助以生成可读摘要。\n",
      "自编码器可用于图片压缩，但为有损压缩。\n",
      "本章介绍人工智能中的对抗攻击及其研究必要性。\n",
      "网络攻击通过添加微小噪声使图像分类出错。\n",
      "噪声对ResNet识别影响及攻击方法，目标最小化损失并限制噪声。\n",
      "L-无穷范数更符合人类感知，适用于图像攻击。\n",
      "梯度更新后需限制x与x0间距离，介绍FGSM攻击方法。\n",
      "白盒与黑盒攻击方法及其对模型安全的影响。\n",
      "黑盒攻击通过代理网络实现，成功率较高。\n",
      "黑盒攻击易成功因数据特征，单像素攻击局限性大。\n",
      "探讨了通用攻击信号对所有图片及语音合成检测系统的攻击可能性。\n",
      "现实世界中的人脸识别和交通标志可能遭受隐蔽性攻击。\n",
      "对抗重编程操控ImageNet模型识别方块数量。\n",
      "对抗重编程通过在训练数据中加入后门，使模型错误识别特定图片。\n",
      "通过滤波器削弱攻击信号保护图像识别系统。\n",
      "介绍了被动防御和主动防御方法及其局限性。\n",
      "对抗训练通过加入攻击数据提高模型鲁棒性。\n",
      "迁移学习应对数据标注成本高、训练与测试数据分布不同问题。\n",
      "领域自适应解决不同领域间特征分布不一致问题。\n",
      "领域对抗训练使特征提取器产生领域无关表示。\n",
      "领域对抗训练通过特征提取器减少源与目标领域差异。\n",
      "领域对抗训练让目标域数据分布接近源域，远离决策边界。\n",
      "决策边界与领域适应问题概述。\n",
      "领域自适应训练需平衡特征对齐与分类准确性。\n",
      "文档介绍了领域泛化示例及相关研究文献。\n",
      "论文介绍了一种学习单域泛化的方法。\n",
      "强化学习通过与环境互动获得奖励来学习最优行为。\n",
      "强化学习应用于玩视频游戏如《太空侵略者》。\n",
      "强化学习应用于游戏和围棋，目标是最大化奖励。\n",
      "强化学习在围棋中的应用及框架介绍。\n",
      "强化学习中通过随机采样增加动作多样性以优化策略。\n",
      "强化学习中，智能体通过与环境互动获得奖励以最大化回报。\n",
      "强化学习与生成对抗网络在参数调整上有相似之处但也有区别。\n",
      "定义损失函数以训练智能体执行期望动作。\n",
      "强化学习通过策略梯度等方法优化，评价动作好坏以指导智能体。\n",
      "使用累积折扣奖励评估动作长期效果。\n",
      "使用折扣累积奖励评价标准，越早动作影响越大。\n",
      "折扣累积奖励适用于围棋等结尾才有分数的游戏。\n",
      "策略梯度算法在每次更新参数后需重新收集数据。\n",
      "强化学习中同策略与异策略学习的区别及探索重要性。\n",
      "Critic评估状态价值，预测累积奖励，有MC和TD两种训练方法。\n",
      "时序差分与蒙特卡洛方法在价值计算上的差异。\n",
      "讨论sa后不一定接sb时的处理方法及强化学习中采样重要性。\n",
      "Critic Vπθ(s)作为基线评估动作好坏，改进版为优势Actor-Critic。\n",
      "优势Actor-Critic通过评估动作相对优劣来指导策略更新。\n",
      "介绍彩虹方法及视觉强化学习相关资源和参考文献。\n",
      "元学习：让机器自动调整超参数，学习最优模型和架构。\n",
      "元学习包括三个步骤：学习算法、定义损失函数及优化。\n",
      "元学习通过多任务测试优化学习算法，以减小总损失L(ϕ)。\n",
      "元学习通过训练任务生成算法Fϕ∗，以适应新任务的少量数据分类。\n",
      "元学习与机器学习在目标、测试方式及损失函数上的区别。\n",
      "元学习与机器学习的异同及优化方法。\n",
      "从训练数据采样批次，迭代更新参数至满意结果。\n",
      "MAML通过两次梯度下降优化模型对超参数的敏感性。\n",
      "MAML有多种变体，并可学习初始化参数及优化器。\n",
      "文章介绍了可学习的优化器、NAS及自动数据增强等元学习方法。\n",
      "元学习中采样权重策略及直接学习网络的可能性。\n",
      "N类别K样例分类任务解释及Omniglot数据集应用。\n",
      "终身学习：让AI持续学习新技能而不遗忘旧技能。\n",
      "终身学习在相似任务间迁移时会遇到遗忘先前任务的问题。\n",
      "模型依次学习多任务时会出现灾难性遗忘问题。\n",
      "多任务学习与终身学习的区别及评估方法。\n",
      "终身学习主要解决灾难性遗忘问题，方法包括选择性突触可塑性。\n",
      "选择性突触可塑性通过参数重要性系数避免灾难性遗忘。\n",
      "GEM方法需存储过去数据，与终身学习初衷相悖。\n",
      "网络压缩技术介绍及在边缘设备上的应用原因。\n",
      "神经网络通过逐步修剪不重要参数来减小规模并保持性能。\n",
      "权重剪枝实现难且GPU加速效果不佳。\n",
      "大网络易训练且剪枝后准确率高，小网络直接训练效果不佳。\n",
      "彩票假说：特定初始化参数使小网络可训练。\n",
      "探讨网络剪枝与知识蒸馏在模型压缩中的应用及效果。\n",
      "知识蒸馏技术介绍及其作用和应用。\n",
      "知识蒸馏通过调整Softmax温度使学生网络更好学习教师网络。\n",
      "可使用Softmax前输出训练，参数量化减少存储空间。\n",
      "二值网络限制容量防过拟合，权重聚类需训练时考虑。\n",
      "深度卷积保持通道数一致，点卷积处理跨通道关系。\n",
      "深度卷积和点卷积参数量比较及低秩近似减少参数方法。\n",
      "网络拆分后参数量减少，K远小于M和N时尤为明显。\n",
      "深度可分离卷积通过拆分卷积层减少参数量。\n",
      "低秩近似和动态计算在网络架构中的应用。\n",
      "探讨网络如何根据计算资源动态调整其深度和宽度。\n",
      "动态调整网络深度和宽度以适应不同难度的图像分类任务。\n",
      "网络压缩技术可结合使用以减小模型大小和节省计算资源。\n",
      "这段文本似乎是一系列学术引用，提![](http://u.jd.com/taMD) 提到的链接是用于在京东购买纸质版苹果产品的链接。其余部分列举了多个![]()几篇关于二进制卷积网络、多尺度密集网络、Skipnet和动态残差网络等深度学习技术的学术论文引用。\n",
      "介绍可解释性人工智能的重要性及其应用场景。\n",
      "讨论了机器学习模型的可解释性及其重要性。\n",
      "可解释性机器学习的目标与局部解释方法。\n",
      "改造或删除后网络输出变化大的部分最重要。\n",
      "可解释性机器学习通过遮挡和梯度计算评估图像区域重要性。\n",
      "可解释性AI重要，SmoothGrad减少显著图噪声。\n",
      "显著图和梯度方法在判断特征重要性时的局限及改进方法。\n",
      "网络处理重复语音输入，通过多层学习实现精确分类。\n",
      "探针方法在BERT和TTS中的应用及注意事项。\n",
      "全局解释通过分析模型参数理解其识别特征。\n",
      "卷积神经网络在MNIST数据集上识别手写数字的滤波器分析。\n",
      "通过加限制和使用图像生成器优化机器学习模型的解释性。\n",
      "可解释性AI通过简单模型模仿复杂模型行为进行解释。\n",
      "介绍了可解释性机器学习的局部与全局解释技术。\n",
      "本章介绍ChatGPT原理、功能及常见误解。\n",
      "ChatGPT答案非网络搜索，有随机性，需自行核实。\n",
      "ChatGPT通过文字接龙方式生成文本，基于复杂函数和大量参数。\n",
      "ChatGPT通过预训练技术学习，无需网络搜索即可生成回答。\n",
      "GPT模型通过大量网络数据学习，提升文本生成与问答能力。\n",
      "GPT3基于大量数据训练，能回答问题并执行多种任务。\n",
      "GPT3通过自监督学习写代码，ChatGPT则加入了人工监督微调。\n",
      "多语言模型Multi-BERT展示出跨语言学习能力。\n",
      "ChatGPT学习三步骤及带来的新研究方向。\n",
      "ChatGPT可能因训练方式导致对特定问题给出错误答案。\n",
      "神经编辑与AI生成内容检测及ChatGPT使用态度探讨。\n",
      "探讨ChatGPT在隐私保护及机器反学习方面的挑战。\n",
      "新研究方向：精准提需求、更正错误、判断AI内容及防泄密。\n",
      "LS-Imagine：单卡3090纯视觉玩Minecraft的RL方法。\n",
      "LS-Imagine：结合长短期状态转换的强化学习方法。\n",
      "基于多模态U-Net的高效功用性图生成与应用\n",
      "LS-Imagine结合短期即时与长期跳跃状态转换模型。\n",
      "LS-Imagine在Minecraft任务中表现优于其他模型。\n",
      "不同模型完成任务的成功率和所需交互步数对比。\n",
      "LS-Imagine方法提升视觉强化学习智能体在高维开放世界的探索效率。\n",
      "文本为机器学习相关术语及其出现页码。\n",
      "机器学习与深度学习相关术语及概念汇总。\n",
      "机器学习相关术语及概念汇总。\n",
      "机器学习与神经网络相关术语及概念汇总。\n",
      "机器学习与深度学习相关术语及页码索引。\n"
     ]
    }
   ],
   "source": [
    "for node in nodes_with_sums:\n",
    "    print(node.metadata[\"node_summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8fa98e-7a3a-48af-8060-7dcff2c47e37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

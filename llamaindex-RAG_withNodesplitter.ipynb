{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee24946b-b151-4429-9ac6-79491fb9f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama_index transformers unstructured pymilvus\n",
    "!pip install llama-index-core\n",
    "!pip install llama-index-extractors-entity\n",
    "!pip install llama-index-vector-stores-milvus\n",
    "!pip install llama-index-embeddings-huggingface\n",
    "!pip install llama-index-llms-huggingface\n",
    "!pip install llama-index-llms-dashscope\n",
    "!pip install llama-index-extractors\n",
    "!pip install pymilvus[milvus_lite]\n",
    "!pip install unstructured[docx]\n",
    "!pip install unstructured[doc]\n",
    "!pip install unstructured[txt]\n",
    "!pip install unstructured[md]\n",
    "!pip install fitz frontend tools\n",
    "!pip uninstall fitz pymupdf -y\n",
    "!pip install pymupdf\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd904dc2-b816-432f-bf16-6790cb770384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validate_default' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'validate_default' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (VectorStoreIndex, SimpleDirectoryReader, load_index_from_storage\n",
    "    , Document, Settings, StorageContext, PromptTemplate)\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.extractors import KeywordExtractor, SummaryExtractor\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.dashscope import DashScope\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from llama_index.extractors.entity import EntityExtractor\n",
    "from llama_index.readers.file import UnstructuredReader,PyMuPDFReader,PDFReader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "import os, re, asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "794b343e-a87c-4676-8a87-9f4c499b83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python pdf2md.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c75a3f-3416-499a-9ec9-edc255829118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-11 09:49:31,908 - INFO - Load pretrained SentenceTransformer: ./Qwen3-Embedding-0.6B\n",
      "2025-10-11 09:49:32,886 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型嵌入维度: 1024\n"
     ]
    }
   ],
   "source": [
    "embedding_model = \"./Qwen3-Embedding-0.6B\"\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=embedding_model,\n",
    "    cache_folder=None,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained(embedding_model, trust_remote_code=True, local_files_only=True)\n",
    "dimension = config.hidden_size\n",
    "print(f\"模型嵌入维度: {dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77249092-c390-4c43-a08d-44631dff2612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！我是通义千问（Qwen），是阿里巴巴集团旗下的通义实验室自主研发的超大规模语言模型。我能够回答问题、创作文字，比如写故事、写公文、写邮件、写剧本、逻辑推理、编程等等，还能表达观点，玩游戏等。如果你有任何问题或需要帮助，欢迎随时告诉我！\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import (\n",
    "    CustomLLM,\n",
    "    CompletionResponse,\n",
    "    LLMMetadata,\n",
    ")\n",
    "from llama_index.core.llms.callbacks import llm_completion_callback\n",
    "from llama_index.core import Settings\n",
    "from typing import Any\n",
    "import requests\n",
    "\n",
    "\n",
    "class SiliconFlowLLM(CustomLLM):\n",
    "    \"\"\"硅基流动自定义 LLM\"\"\"\n",
    "    \n",
    "    model: str = \"Qwen/Qwen3-Next-80B-A3B-Instruct\"\n",
    "    api_key: str = \"\"\n",
    "    api_base: str = \"https://api.siliconflow.cn/v1\"\n",
    "    max_tokens: int = 4096\n",
    "    temperature: float = 0.1\n",
    "    \n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"获取 LLM 元数据\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=32768,  # 根据具体模型调整\n",
    "            num_output=self.max_tokens,\n",
    "            model_name=self.model,\n",
    "        )\n",
    "    \n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        \"\"\"完成请求\"\"\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"max_tokens\": kwargs.get(\"max_tokens\", self.max_tokens),\n",
    "            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{self.api_base}/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=data\n",
    "        )\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        \n",
    "        return CompletionResponse(\n",
    "            text=result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        )\n",
    "    \n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(self, prompt: str, **kwargs: Any):\n",
    "        \"\"\"流式完成（未实现，但需要定义）\"\"\"\n",
    "        # 调用非流式方法\n",
    "        response = self.complete(prompt, **kwargs)\n",
    "        yield response\n",
    "\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 创建自定义 LLM 实例\n",
    "    llm = SiliconFlowLLM(\n",
    "        model=\"Qwen/Qwen3-30B-A3B-Instruct-2507\",  # 可选其他模型\n",
    "        api_key=\"sk-ionsbeieleeekwlstqotkyrmictdzshgnbaytavcudxkixcs\",  # 替换为你的 API Ke\n",
    "        api_base = \"https://api.siliconflow.cn/v1\",\n",
    "        max_tokens=1024,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    # 2. 设置到 Settings\n",
    "    Settings.llm = llm\n",
    "    \n",
    "    # 3. 测试使用\n",
    "    response = llm.complete(\"你好，请介绍一下你自己\")\n",
    "    print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a5cc3ae-1e31-4c71-aa92-7924a9545e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "绝对数据库路径: /root/milvus_test/milvus_lite.db\n"
     ]
    }
   ],
   "source": [
    "milvus_dir = \"./milvus_test\"\n",
    "milvus_db_path = os.path.join(milvus_dir, \"milvus_lite.db\")\n",
    "abs_db_path = os.path.abspath(milvus_db_path)\n",
    "print(f\"绝对数据库路径: {abs_db_path}\")\n",
    "\n",
    "if not os.path.exists(milvus_dir):\n",
    "    os.makedirs(milvus_dir)\n",
    "    print(\"已创建 ./milvus 目录\")\n",
    "\n",
    "\n",
    "\n",
    "# milvus_vector_store = MilvusVectorStore(\n",
    "#     uri=f\"{abs_db_path}\",\n",
    "#     collection_name=\"rag_collection\",\n",
    "#     dim=1024,\n",
    "#     overwrite=True\n",
    "# )\n",
    "# storage_context = StorageContext.from_defaults(vector_store=milvus_vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5baa8d-7468-432a-9932-35ba29730ec0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 首次运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0475e9bf-6391-498f-bc2c-241297bf1b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "绝对数据库路径: /root/milvus_test/milvus_lite.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    }
   ],
   "source": [
    "###首次运行\n",
    "milvus_dir = \"./milvus_test\"\n",
    "milvus_db_path = os.path.join(milvus_dir, \"milvus_lite.db\")\n",
    "abs_db_path = os.path.abspath(milvus_db_path)\n",
    "print(f\"绝对数据库路径: {abs_db_path}\")\n",
    "\n",
    "if not os.path.exists(milvus_dir):\n",
    "    os.makedirs(milvus_dir)\n",
    "    print(\"已创建 ./milvus 目录\")\n",
    "\n",
    "\n",
    "\n",
    "milvus_vector_store = MilvusVectorStore(\n",
    "    uri=f\"{abs_db_path}\",\n",
    "    collection_name=\"rag_collection\",\n",
    "    dim=1024,\n",
    "    overwrite=True\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=milvus_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "567914c9-37a4-4739-bf70-da546fa297e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text).strip()\n",
    "    # text = re.sub(r'(\\w+\\s*){3,}\\n', '', text)\n",
    "    # text = re.sub(r'[^a-zA-Z0-9\\u4e00-\\u9fa5\\s\\.,!?]', '', text)  # 去除特殊字符，保留中英文\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc0b85-f8aa-4101-9b40-b2df8a3aa7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13a22cbd-aedb-4285-906f-fde3ae0022d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_summary_async(text, max_words=30):\n",
    "    prompt = f\"总结以下文本，不超过{max_words}字，直接回复结果：{text}\"\n",
    "    response = await Settings.llm.acomplete(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "def generate_summary(text, max_words=30):\n",
    "    prompt = f\"总结以下文本，不超过{max_words}字，直接回复结果：{text}\"\n",
    "    response = Settings.llm.complete(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "async def add_summaries_to_nodes_async(nodes_list):\n",
    "    tasks = [generate_summary_async(node.text) for node in nodes_list]\n",
    "\n",
    "    summaries = []\n",
    "    for future in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"生成节点摘要进度\"):\n",
    "        summary = await future\n",
    "        summaries.append(summary)\n",
    "\n",
    "    for node, summary in zip(nodes_list, summaries):\n",
    "        node.metadata[\"node_summary\"] = summary\n",
    "        \n",
    "def add_summaries_to_nodes(nodes_list):\n",
    "    for node in tqdm(nodes_list, desc=\"生成摘要\"):\n",
    "        summary = generate_summary(node.text)\n",
    "        node.metadata[\"node_summary\"] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "619e3fe0-1330-4417-be61-b13e6867ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_tokenizer = AutoTokenizer.from_pretrained(\"./Qwen3-Embedding-0.6B\", trust_remote_code=True)\n",
    "documents_dir = \"./docs\"\n",
    "\n",
    "file_extractor = {\n",
    "    \".docx\": UnstructuredReader(),\n",
    "    \".doc\": UnstructuredReader(),\n",
    "    \".txt\": UnstructuredReader(),\n",
    "    \".md\": UnstructuredReader(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bf8d575-4093-4782-81c5-c6d2175fae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "\n",
    "def load_single_file(file_path, file_extractor):\n",
    "    \"\"\"加载单个文件\"\"\"\n",
    "    try:\n",
    "        ext = Path(file_path).suffix.lower()\n",
    "        if ext in file_extractor:\n",
    "            reader = file_extractor[ext]\n",
    "            print('loading:',file_path)\n",
    "            docs = reader.load_data(file_path)\n",
    "            return docs\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"加载文件 {file_path} 失败: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_documents_parallel(documents_dir, file_extractor, max_workers=4):\n",
    "    \"\"\"并行加载文档\"\"\"\n",
    "    all_files = []\n",
    "    for ext in file_extractor.keys():\n",
    "        all_files.extend(Path(documents_dir).rglob(f\"*{ext}\"))\n",
    "    \n",
    "    documents = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(load_single_file, str(f), file_extractor): f \n",
    "                   for f in all_files}\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"加载文件\"):\n",
    "            docs = future.result()\n",
    "            documents.extend(docs)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91cf755f-70db-417a-8649-90a166ae9c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_long_documents(documents, max_length=100000, overlap=0):\n",
    "    \"\"\"预处理超长文档，避免 tokenizer 处理超长文本\"\"\"\n",
    "    processed_docs = []\n",
    "    for doc in documents:\n",
    "        text_length = len(doc.text)\n",
    "        # 如果文档太长，先粗切分\n",
    "        if text_length > max_length:\n",
    "            print(f\"检测到超长文档: {text_length} 字符，进行预切分\")\n",
    "            # 按固定长度切分，带重叠\n",
    "            chunks = []\n",
    "            start = 0\n",
    "            chunk_index = 0\n",
    "            \n",
    "            while start < text_length:\n",
    "                end = min(start + max_length, text_length)\n",
    "                chunk_text = doc.text[start:end]\n",
    "                \n",
    "                # 创建新的 metadata，添加切片信息\n",
    "                new_metadata = doc.metadata.copy() if doc.metadata else {}\n",
    "                new_metadata['chunk_index'] = chunk_index\n",
    "                new_metadata['total_chunks'] = (text_length + max_length - overlap - 1) // (max_length - overlap)\n",
    "                new_metadata['is_chunked'] = True\n",
    "                \n",
    "                chunks.append(Document(text=chunk_text, metadata=new_metadata))\n",
    "                \n",
    "                # 下一个起点：当前起点 + (max_length - overlap)\n",
    "                # 这样可以保证前后重叠 overlap 个字符\n",
    "                start += (max_length - overlap)\n",
    "                chunk_index += 1\n",
    "            \n",
    "            processed_docs.extend(chunks)\n",
    "            print(f\"  切分为 {len(chunks)} 个块，每块最大 {max_length} 字符，重叠 {overlap} 字符\")\n",
    "        else:\n",
    "            processed_docs.append(doc)\n",
    "    \n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9fcf3cd-934a-4566-b27a-3a969b786365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading:loading: docs/中华人民共和国中国人民银行法.txt\n",
      " docs/test_ml.docx\n",
      "loading: docs/中国人民银行决定实行差别存款准备金率制度.txt\n",
      "loading: docs/中文新闻 2.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "加载文件:   0%|          | 0/16 [00:00<?, ?it/s]2025-10-10 15:11:56,254 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "加载文件:   6%|▋         | 1/16 [00:00<00:01,  9.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/中文新闻.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:11:56,550 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "加载文件:  12%|█▎        | 2/16 [00:00<00:03,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/人民币银行结算账户管理办法.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:11:57,834 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "加载文件:  19%|█▉        | 3/16 [00:01<00:08,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/金融机构客户尽职调查和客户身份资料及交易记录保存管理办法.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:11:58,990 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "加载文件:  25%|██▌       | 4/16 [00:02<00:10,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/.ipynb_checkpoints/中国人民银行决定实行差别存款准备金率制度-checkpoint.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:11:59,640 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "加载文件:  31%|███▏      | 5/16 [00:03<00:08,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/README-qwen3next.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:12:02,209 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "加载文件:  38%|███▊      | 6/16 [00:06<00:14,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/upinfo2.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:12:03,318 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "加载文件:  44%|████▍     | 7/16 [00:07<00:11,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/外企财报 1.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:12:11,206 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "加载文件:  50%|█████     | 8/16 [00:15<00:27,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/外企财报 2.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:12:14,526 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "加载文件:  56%|█████▋    | 9/16 [00:18<00:23,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/外企财报 3.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:12:16,533 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "加载文件:  62%|██████▎   | 10/16 [00:20<00:17,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/支付结算办法.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:12:16,851 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "2025-10-10 15:12:16,863 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "加载文件:  69%|██████▉   | 11/16 [00:20<00:10,  2.17s/it]2025-10-10 15:12:16,996 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/腾讯年报大繁体.md\n",
      "loading: docs/腾讯财报 2025 简体.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:12:18,384 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "加载文件:  88%|████████▊ | 14/16 [00:22<00:02,  1.21s/it]2025-10-10 15:12:19,573 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "加载文件:  94%|█████████▍| 15/16 [00:23<00:01,  1.20s/it]2025-10-10 15:12:19,836 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "加载文件: 100%|██████████| 16/16 [00:23<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件大小:16\n",
      "节点数量:2412\n"
     ]
    }
   ],
   "source": [
    "# 使用方法\n",
    "documents = load_documents_parallel(documents_dir, file_extractor, max_workers=1)\n",
    "\n",
    "cleaned_documents = [Document(text=clean_text(doc.text), metadata=doc.metadata) \n",
    "                     for doc in documents]\n",
    "\n",
    "# 添加这一步：最大长度100000，前后重叠1000\n",
    "# cleaned_documents = preprocess_long_documents(\n",
    "#     cleaned_documents, \n",
    "#     max_length=100000, \n",
    "#     overlap=0\n",
    "# )\n",
    "documents = cleaned_documents\n",
    "\n",
    "print(f\"文件大小:{len(documents)}\")\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=100, tokenizer=qwen_tokenizer.tokenize)  \n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "print(f\"节点数量:{len(nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786076ab-5328-4eba-9b90-9e5669da92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_summaries_to_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48734949-0a3b-44d5-92b2-6b4d45100eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_summaries_to_json(nodes_list, file_path=\"nodes_summaries_temp.json\"):\n",
    "#     summaries_dict = {}\n",
    "#     for idx, node in enumerate(nodes_list):\n",
    "#         summaries_dict[str(idx)] = node.metadata.get(\"node_summary\", \"\")  # 获取摘要，若无则为空\n",
    "    \n",
    "#     # 保存到 JSON\n",
    "#     with open(file_path, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(summaries_dict, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "#     print(f\"节点摘要已保存到 {file_path}\")\n",
    "\n",
    "# def load_summaries_to_nodes(nodes_list, file_path=\"nodes_summaries.json\"):\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         summaries_dict = json.load(f)\n",
    "#     sorted_keys = sorted(summaries_dict.keys(), key=int)\n",
    "\n",
    "#     for key in sorted_keys:\n",
    "#         idx = int(key)\n",
    "#         if idx < len(nodes_list):\n",
    "#             nodes_list[idx].metadata[\"node_summary\"] = summaries_dict[key]\n",
    "#         else:\n",
    "#             print(f\"警告：索引 {idx} 超出节点列表长度，跳过。\")\n",
    "    \n",
    "#     return nodes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca35dc24-2ae5-4cb1-8adf-14b5645bcd39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1da1464-2d9c-4c15-a3b0-cb2da61a26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_summaries_to_json(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f633b2f-4bff-4ae3-9327-1d6a18ded997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 保存 Nodes ============\n",
    "def save_nodes(nodes, save_dir=\"./saved_nodes\"):\n",
    "    \"\"\"保存节点数据（支持pickle和json两种格式）\"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 方法1: 使用 pickle 保存完整节点对象（推荐）\n",
    "    pickle_file = save_path / \"nodes.pkl\"\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(nodes, f)\n",
    "    print(f\"Nodes已保存到: {pickle_file}\")\n",
    "\n",
    "def load_nodes(save_dir=\"./saved_data\"):\n",
    "    \"\"\"加载节点数据\"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    pickle_file = save_path / \"nodes.pkl\"\n",
    "    \n",
    "    if not pickle_file.exists():\n",
    "        raise FileNotFoundError(f\"❌ 找不到节点文件: {pickle_file}\")\n",
    "    \n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        nodes = pickle.load(f)\n",
    "    \n",
    "    print(f\"✅ 已加载 {len(nodes)} 个节点\")\n",
    "    \n",
    "    # 验证数据\n",
    "    print(f\"📊 节点验证:\")\n",
    "    print(f\"  - 总节点数: {len(nodes)}\")\n",
    "    return nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62b589d3-18f7-4672-9653-2be5ba001a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes已保存到: saved_nodes/nodes.pkl\n"
     ]
    }
   ],
   "source": [
    "save_nodes(nodes, save_dir=\"./saved_nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "775b208d-7576-4475-89ad-5cafb8a9cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents,\n",
    "#     storage_context=storage_context,\n",
    "#     embed_model=Settings.embed_model,\n",
    "#     node_parser=node_parser,\n",
    "#     store_nodes_override=True\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "transformations = [node_parser]\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=Settings.embed_model,\n",
    "    node_parser=node_parser,\n",
    "    transformations=transformations,\n",
    "    store_nodes_override=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d8e8761-356e-4966-b632-003edb975da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 索引已保存到: ./milvus_storage\n",
      "✅ 索引信息已保存到: milvus_storage/index_info.json\n",
      "📊 索引信息: {'collection_name': 'rag_collection', 'milvus_db_path': '/root/milvus_test/milvus_lite.db', 'embedding_dim': 1024, 'total_documents': 2493, 'index_type': 'VectorStoreIndex'}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from pathlib import Path\n",
    "# ============ 保存 Milvus 索引 ============\n",
    "def save_milvus_index(index, persist_dir=\"./milvus_storage\"):\n",
    "    \"\"\"保存Milvus索引（持久化到本地）\"\"\"\n",
    "    persist_path = Path(persist_dir)\n",
    "    persist_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # LlamaIndex会自动保存索引结构和docstore\n",
    "    index.storage_context.persist(persist_dir=persist_dir)\n",
    "    \n",
    "    print(f\"✅ 索引已保存到: {persist_dir}\")\n",
    "    \n",
    "    # 保存索引元信息\n",
    "    index_info = {\n",
    "        'collection_name': 'rag_collection',\n",
    "        'milvus_db_path': abs_db_path,\n",
    "        'embedding_dim': dimension,\n",
    "        'total_documents': len(index.docstore.docs),\n",
    "        'index_type': 'VectorStoreIndex'\n",
    "    }\n",
    "    \n",
    "    info_file = persist_path / \"index_info.json\"\n",
    "    with open(info_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(index_info, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"✅ 索引信息已保存到: {info_file}\")\n",
    "    print(f\"📊 索引信息: {index_info}\")\n",
    "\n",
    "# ============ 使用示例 ============\n",
    "# 保存索引\n",
    "save_milvus_index(index, persist_dir=\"./milvus_storage\")\n",
    "\n",
    "# 加载索引\n",
    "# index = load_milvus_index(persist_dir=\"./milvus_storage\", milvus_db_path=abs_db_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17816c31-5fe8-4cb9-9237-f17cefd16c2c",
   "metadata": {},
   "source": [
    "### 非第一次运行 加载持久化运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "103c2661-6d60-4e1c-a448-0b588aaaab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "def load_nodes(save_dir=\"./saved_data\"):\n",
    "    \"\"\"加载节点数据\"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    pickle_file = save_path / \"nodes.pkl\"\n",
    "    \n",
    "    if not pickle_file.exists():\n",
    "        raise FileNotFoundError(f\"❌ 找不到节点文件: {pickle_file}\")\n",
    "    \n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        nodes = pickle.load(f)\n",
    "    \n",
    "    print(f\"✅ 已加载 {len(nodes)} 个节点\")\n",
    "    \n",
    "    # 验证数据\n",
    "    print(f\"📊 节点验证:\")\n",
    "    print(f\"  - 总节点数: {len(nodes)}\")\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "155ebe6a-db42-4973-875e-7e484a786aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from pathlib import Path\n",
    "# ============ 加载 Milvus 索引 ============\n",
    "def load_milvus_index(persist_dir=\"./storage\", milvus_db_path=None):\n",
    "    \"\"\"加载已保存的Milvus索引\"\"\"\n",
    "    persist_path = Path(persist_dir)\n",
    "    \n",
    "    if not persist_path.exists():\n",
    "        raise FileNotFoundError(f\"❌ 找不到索引目录: {persist_dir}\")\n",
    "    \n",
    "    # 读取索引信息\n",
    "    info_file = persist_path / \"index_info.json\"\n",
    "    if info_file.exists():\n",
    "        with open(info_file, 'r', encoding='utf-8') as f:\n",
    "            index_info = json.load(f)\n",
    "        print(f\"📊 索引信息: {index_info}\")\n",
    "        milvus_db_path = milvus_db_path or index_info.get('milvus_db_path')\n",
    "    \n",
    "    # 重建 Milvus vector store\n",
    "    milvus_vector_store = MilvusVectorStore(\n",
    "        uri=milvus_db_path,\n",
    "        collection_name=\"rag_collection\",\n",
    "        dim=dimension,\n",
    "        overwrite=False  # 不覆盖已有数据\n",
    "    )\n",
    "    \n",
    "    # 重建 storage context\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        vector_store=milvus_vector_store,\n",
    "        persist_dir=persist_dir\n",
    "    )\n",
    "    \n",
    "    # 加载索引\n",
    "    index = load_index_from_storage(\n",
    "        storage_context=storage_context,\n",
    "        embed_model=Settings.embed_model\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ 索引已加载\")\n",
    "    print(f\"  - 文档数量: {len(index.docstore.docs)}\")\n",
    "    \n",
    "    return index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ae0603-9369-494a-a304-a44b37d76ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 已加载 2493 个节点\n",
      "📊 节点验证:\n",
      "  - 总节点数: 2493\n",
      "📊 索引信息: {'collection_name': 'rag_collection', 'milvus_db_path': '/root/milvus_test/milvus_lite.db', 'embedding_dim': 1024, 'total_documents': 2493, 'index_type': 'VectorStoreIndex'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n",
      "2025-10-11 09:49:37,774 - INFO - Loading all indices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./milvus_storage/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./milvus_storage/index_store.json.\n",
      "✅ 索引已加载\n",
      "  - 文档数量: 2493\n"
     ]
    }
   ],
   "source": [
    "nodes = load_nodes(save_dir=\"./saved_nodes\")\n",
    "index = load_milvus_index(persist_dir=\"./milvus_storage\", milvus_db_path=abs_db_path)\n",
    "index.embed_model=Settings.embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cedb969-684f-4273-88a3-45e4c5edca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] 分数: 0.5139 | 文件: None\n",
      "内容: · 我們升級了小遊戲的技術底座，能夠兼容更多的遊戲引擎，提升了圖像渲染效果，降低了加載時長，助⼒遊戲 開發者將複雜的手機遊戲應⽤適配至小遊戲。⼆零⼆五年第⼆季小遊戲的總流水同比增長 20% 。\n",
      "\n",
      "· 本土遊戲方面， 《三角洲行動》 是我們⼆零⼆四年九月在移動端和個人電腦端推出的第-人稱射擊遊戲， ⼆零⼆五年七月的平均日活躍賬戶數突破 2,000 萬，位居行業日活躍賬戶數前五，流水前三 。 1\n",
      "\n",
      "·\n",
      "\n",
      "[2] 分数: 0.5011 | 文件: None\n",
      "内容: 411 1,371 3% 1,402 0.6% QQ 的移动终端月活跃账户数 532 571 -7% 534 -0.4% 收费增值服务付费会员数 264 263 0.4% 268 -1% 3 公司数据， QuestMobile ， Sensor Tower 4 2Q2025 付费会员数的日均值 5 2Q2025 每月最后一日的平均付费会员数 6 发布于 https://huggingface.co\n",
      "\n",
      "[3] 分数: 0.4681 | 文件: None\n",
      "内容: 534 亿元。截止 2025 年 6 月 30 日，我们于非上市 投资公司（不包括附属公司）权益的账面价值为人民币 3,423 亿元，相较于截止 2025 年 3 月 31 日的账 面价值为人民币 3,379 亿元。\n",
      "\n",
      "▪ 本公司于 2Q2025 于香港联交所以约 194 亿港元的总代价回购约 3,888 万股股份。 1 非国际财务报告准则撇除股份酬金、并购带来的效应，如来自投资公司的（收益） 亏\n"
     ]
    }
   ],
   "source": [
    "# 单次快速检索\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "nodes_test = retriever.retrieve(\"腾讯游戏 三角洲行动\")\n",
    "\n",
    "\n",
    "# 打印结果\n",
    "for i, node in enumerate(nodes_test, 1):\n",
    "    print(f\"\\n[{i}] 分数: {node.score:.4f} | 文件: {node.metadata.get('file_name')}\")\n",
    "    print(f\"内容: {node.text[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6b2a35-74d4-496c-afed-46c7795044ea",
   "metadata": {},
   "source": [
    "## search and rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf4934f-e7ed-479a-9c08-f0d91be86b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-retrievers-bm25\n",
    "# !pip install llama-index-packs-fusion-retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d73f2f98-4f58-400c-8ba3-f7ca568dea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.packs.fusion_retriever import HybridFusionRetrieverPack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "933289c0-cd96-4170-b131-649aeada250a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb54dd869f544b5a8485c04e9ab711d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at autodl-tmp/Qwen3-Reranker-4B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token: [PAD]\n",
      "Pad token ID: 151669\n",
      "Model config pad_token_id: 151669\n"
     ]
    }
   ],
   "source": [
    "reranker_model_path = \"autodl-tmp/Qwen3-Reranker-4B\"\n",
    "\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=reranker_model_path,\n",
    "    top_n=5,\n",
    "    device=\"cuda\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "cross_encoder = reranker._model\n",
    "reranker_tokenizer = cross_encoder.tokenizer\n",
    "reranker_model = cross_encoder.model\n",
    "\n",
    "special_tokens = {'pad_token': '[PAD]'}\n",
    "num_added_tokens = reranker_tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "reranker_model.resize_token_embeddings(len(reranker_tokenizer))\n",
    "\n",
    "reranker_tokenizer.pad_token = '[PAD]'\n",
    "reranker_tokenizer.pad_token_id = reranker_tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "reranker_model.config.pad_token_id = reranker_tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Pad token: {reranker_tokenizer.pad_token}\")\n",
    "print(f\"Pad token ID: {reranker_tokenizer.pad_token_id}\")\n",
    "print(f\"Model config pad_token_id: {reranker_model.config.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e148b4f-7730-4b49-bc81-db658115c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reranker_tokenize(text):\n",
    "    rerank_tokenizer = AutoTokenizer.from_pretrained(\"autodl-tmp/Qwen3-Reranker-8B\", padding_side='left')\n",
    "    if not text.strip():\n",
    "        return []\n",
    "    tokens = rerank_tokenizer.tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55e55514-7ff9-40fa-b4b0-f8a310914217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-11 10:37:41,718 - WARNING - The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "2025-10-11 10:37:42,416 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    nodes=nodes, \n",
    "    similarity_top_k=10,\n",
    "    tokenizer=reranker_tokenize)\n",
    "vector_retriever = VectorIndexRetriever(index=index, similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a729b8c7-8842-4eb9-b29a-9870ef23cb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-11 10:40:30,352 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "hybrid_pack = HybridFusionRetrieverPack(\n",
    "    nodes=nodes,\n",
    "    bm25_retriever=bm25_retriever,\n",
    "    vector_retriever=vector_retriever,\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    similarity_top_k=20\n",
    ")\n",
    "hybrid_retriever = hybrid_pack.fusion_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c25c162-cde9-413e-b26a-eea12ef1637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_qa_template_str = (\n",
    "    \"上下文信息如下：\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"基于提供的上下文，用中文直接回答查询，答案只能从上下文知识中获取，不要自己发挥。\\n\"\n",
    "    \"查询：{query_str}\\n\"\n",
    "    \"回答：\"\n",
    ")\n",
    "text_qa_template = PromptTemplate(text_qa_template_str)\n",
    "\n",
    "refine_template_str = (\n",
    "    \"原始查询是：{query_str}\\n\"\n",
    "    \"我们已有回答：{existing_answer}\\n\"\n",
    "    \"基于以下新上下文，用中文精炼现有回答，问题的核心回答要放在最前边，然后是解释，确保完整性和准确性：\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"精炼后的回答：\"\n",
    ")\n",
    "refine_template = PromptTemplate(refine_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18a7a043-1de9-41a8-9ce2-f51ee7157700",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer = get_response_synthesizer(\n",
    "    text_qa_template=text_qa_template,\n",
    "    refine_template=refine_template,\n",
    "    response_mode=\"compact\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48b14fdb-5b11-4de9-8d21-05918c0e989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=hybrid_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[reranker]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18c73b8c-df5b-40b9-bc8c-ca15e31f3745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. 根据《支付结算办法》第28条和第35条，单位卡单笔交易超过10万元人民币是否需进行额外身份核实与交易背景审查，且该艺术品购买是否属于“异常大额交易”而触发反洗钱监测？\n",
      "2. 依据《金融机构客户尽职调查和客户身份资料及交易记录保存管理办法》第12条和第18条，外国政要关联客户单笔交易超过5万元人民币是否构成“高风险客户”交易，需启动强化尽职调查程序？\n",
      "3. 结合《客户尽职调查和客户身份资料及交易记录保存管理办法》第15条，对于涉及“外国政要”身份的客户，其通过单位卡进行11万元艺术品购买是否构成“非正常交易目的”并触发强制性客户身份资料保存与报告义务？\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e67d03ac1ba47c991efb1e10a9462f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据提供的文件内容，对第一笔交易（使用腾讯发行的单位卡购买一件价值11万元人民币的艺术品）进行分析：\n",
      "\n",
      "1. **关于《支付结算办法》的合规性分析**：\n",
      "   - 文件中未提及单位卡购买艺术品是否属于禁止或受限的支付结算行为。\n",
      "   - 交易金额为11万元人民币，未超过任何明确的限额或触发特殊收费或限制条款。\n",
      "   - 电报费、手续费、邮电费等均按标准收取，未发现违反《支付结算办法》中关于收费或结算流程的规定。\n",
      "   - 未发现该交易违反《支付结算办法》第二百五十八条、第二百五十九条、第二百六十条等关于执行优先级、解释权和施行时间的规定。\n",
      "\n",
      "   ✅ **结论**：该交易**不违反《支付结算办法》**。\n",
      "\n",
      "2. **关于《金融机构客户尽职调查和客户身份资料及交易记录保存管理办法》的强制措施触发情况**：\n",
      "   - 客户身份：该客户为腾讯高管，且因家庭关系被列为“外国政要”（PEP）。\n",
      "   - 根据《金融机构客户尽职调查和客户身份资料及交易记录保存管理办法》**第四十三条**，金融机构在开展客户尽职调查时，应关注客户所在国家或地区风险状况，且对高风险客户（如外国政要）需加强尽职调查。\n",
      "   - 虽然未明确说明“外国政要”直接触发报告义务，但**第四十三条第（二）项**规定：  \n",
      "     > “有明显理由怀疑客户建立业务关系的目的和性质与洗钱和恐怖融资等违法犯罪活动相关的”  \n",
      "     应当报告可疑行为。\n",
      "   - 该客户为“外国政要”，且交易涉及单位卡购买高价值艺术品（11万元），虽未达大额交易报告标准（如10万元人民币以上现金交易），但结合其身份特殊性，存在较高洗钱或腐败风险。\n",
      "   - 根据**第四十三条**，若金融机构有理由怀疑该交易目的与洗钱或恐怖融资相关，应向中国反洗钱监测分析中心和中国人民银行当地分支机构报告。\n",
      "\n",
      "   ✅ **结论**：该交易**触发《金融机构客户尽职调查和客户身份资料及交易记录保存管理办法》第四十三条的可疑行为报告义务**，因客户为“外国政要”，且交易性质（单位卡购买高价值艺术品）存在与洗钱或恐怖融资相关的合理怀疑。\n",
      "\n",
      "---\n",
      "\n",
      "### 最终回答：\n",
      "- **违反《支付结算办法》的情况**：无。\n",
      "- **触发《金融机构客户尽职调查和客户身份资料及交易记录保存管理办法》强制措施的特定门槛**：  \n",
      "  **第四十三条**（因客户为“外国政要”，且交易存在与洗钱或恐怖融资相关的合理怀疑，应报告可疑行为）。\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "您是一家大型商业银行的首席合规官。您的一位客户是腾讯的一位高管，由于其家庭关系，他也被列为“外国政要”。在腾讯2025年第二季度财报发布后的一周内，他通过贵行进行了以下交易：\n",
    "\n",
    "他使用腾讯发行的单位卡购买了一件价值11万元人民币的艺术品，摆放在办公室。\n",
    "\n",
    "他将8万元人民币现金存入个人账户，并注明这笔资金来自个人股息。\n",
    "\n",
    "他作为付款人签署了一张金额为600万元人民币的商业承兑汇票，付款期限为90天。该草案旨在为一家3D打印公司提供新的融资，该公司将使用腾讯的“混元3D模型”人工智能服务，该服务在最近的财报中被重点提及。\n",
    "\n",
    "您的任务：\n",
    "\n",
    "仅根据提供的文件，回答以下问题。\n",
    "\n",
    "对于这第一笔交易，请分别找出任何可能违反“支付结算办法”的情况，或任何触发“多家客户尽职调查和客户身份资料及交易记录保存管理办法”强制措施的特定门槛。请引用文件中的具体条款编号来支持您的发现。\"\"\"\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06741b50-ba2e-4380-8e2d-4268b1703e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1597/2662524485.py:101: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  class ChildToParentPostprocessor(BaseNodePostprocessor):\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.postprocessor.types import BaseNodePostprocessor\n",
    "from llama_index.core.schema import NodeWithScore, QueryBundle, TextNode\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import List, Optional, Dict\n",
    "import copy\n",
    "\n",
    "# ==================== 1. 节点分割器 ====================\n",
    "class NodeSplitter:\n",
    "    \"\"\"将长节点分割成多个子节点,保持父子关系\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 512, overlap_ratio: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            chunk_size: 子节点的目标长度\n",
    "            overlap_ratio: 重叠比例 (0.1 表示 10%)\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap_size = int(chunk_size * overlap_ratio)\n",
    "        \n",
    "    def split_node(self, node: NodeWithScore, parent_id: str = None) -> List[NodeWithScore]:\n",
    "        \"\"\"\n",
    "        将单个节点分割成多个子节点\n",
    "        \n",
    "        Args:\n",
    "            node: 原始节点\n",
    "            parent_id: 父节点ID (如果为None,使用node.node.node_id)\n",
    "            \n",
    "        Returns:\n",
    "            子节点列表,每个子节点都保留父节点引用\n",
    "        \"\"\"\n",
    "        text = node.node.text\n",
    "        text_length = len(text)\n",
    "        \n",
    "        # 如果文本长度小于chunk_size,直接返回原节点\n",
    "        if text_length <= self.chunk_size:\n",
    "            # 添加父节点ID到metadata\n",
    "            node.node.metadata['parent_node_id'] = parent_id or node.node.node_id\n",
    "            node.node.metadata['is_child_node'] = False\n",
    "            return [node]\n",
    "        \n",
    "        parent_node_id = parent_id or node.node.node_id\n",
    "        child_nodes = []\n",
    "        start = 0\n",
    "        chunk_index = 0\n",
    "        \n",
    "        while start < text_length:\n",
    "            end = min(start + self.chunk_size, text_length)\n",
    "            chunk_text = text[start:end]\n",
    "            \n",
    "            # 创建子节点\n",
    "            child_node = TextNode(\n",
    "                text=chunk_text,\n",
    "                metadata={\n",
    "                    **node.node.metadata,  # 继承父节点的metadata\n",
    "                    'parent_node_id': parent_node_id,\n",
    "                    'chunk_index': chunk_index,\n",
    "                    'is_child_node': True,\n",
    "                    'parent_text_length': text_length,\n",
    "                    'chunk_start': start,\n",
    "                    'chunk_end': end\n",
    "                },\n",
    "                excluded_embed_metadata_keys=node.node.excluded_embed_metadata_keys,\n",
    "                excluded_llm_metadata_keys=node.node.excluded_llm_metadata_keys,\n",
    "            )\n",
    "            \n",
    "            # 保持原始评分\n",
    "            child_node_with_score = NodeWithScore(\n",
    "                node=child_node,\n",
    "                score=node.score\n",
    "            )\n",
    "            \n",
    "            child_nodes.append(child_node_with_score)\n",
    "            \n",
    "            # 计算下一个起点 (带重叠)\n",
    "            start += (self.chunk_size - self.overlap_size)\n",
    "            chunk_index += 1\n",
    "        \n",
    "        return child_nodes\n",
    "    \n",
    "    def split_nodes(self, nodes: List[NodeWithScore]) -> tuple[List[NodeWithScore], Dict[str, NodeWithScore]]:\n",
    "        \"\"\"\n",
    "        批量分割节点\n",
    "        \n",
    "        Returns:\n",
    "            (子节点列表, 父节点映射字典)\n",
    "        \"\"\"\n",
    "        all_child_nodes = []\n",
    "        parent_node_map = {}  # parent_node_id -> 原始父节点\n",
    "        \n",
    "        for node in nodes:\n",
    "            parent_id = node.node.node_id\n",
    "            parent_node_map[parent_id] = node  # 保存原始父节点\n",
    "            \n",
    "            child_nodes = self.split_node(node, parent_id)\n",
    "            all_child_nodes.extend(child_nodes)\n",
    "        \n",
    "        return all_child_nodes, parent_node_map\n",
    "\n",
    "\n",
    "# ==================== 2. 子节点到父节点的后处理器 ====================\n",
    "class ChildToParentPostprocessor(BaseNodePostprocessor):\n",
    "    \"\"\"\n",
    "    将rerank后的子节点还原为父节点\n",
    "    策略: 如果多个子节点来自同一父节点,取最高分的子节点分数作为父节点分数\n",
    "    \"\"\"\n",
    "    \n",
    "    # 使用 Pydantic 的方式声明字段\n",
    "    parent_node_map: Dict[str, Any] = {}\n",
    "    keep_top_k: int = 5\n",
    "    \n",
    "    def __init__(self, parent_node_map: Dict[str, NodeWithScore], keep_top_k: int = 5, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            parent_node_map: 父节点ID到父节点的映射\n",
    "            keep_top_k: 最终保留的父节点数量\n",
    "        \"\"\"\n",
    "        # 使用 Pydantic 的初始化方式\n",
    "        super().__init__(\n",
    "            parent_node_map=parent_node_map,\n",
    "            keep_top_k=keep_top_k,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    def _postprocess_nodes(\n",
    "        self, \n",
    "        nodes: List[NodeWithScore], \n",
    "        query_bundle: Optional[QueryBundle] = None\n",
    "    ) -> List[NodeWithScore]:\n",
    "        \"\"\"\n",
    "        将子节点还原为父节点\n",
    "        \"\"\"\n",
    "        # 按父节点ID分组,记录每个父节点的最高分数\n",
    "        parent_scores: Dict[str, float] = {}\n",
    "        parent_child_nodes: Dict[str, List[NodeWithScore]] = {}\n",
    "        \n",
    "        for node in nodes:\n",
    "            parent_id = node.node.metadata.get('parent_node_id')\n",
    "            \n",
    "            if not parent_id:\n",
    "                # 如果没有父节点ID,说明是原始节点,直接保留\n",
    "                parent_scores[node.node.node_id] = node.score\n",
    "                parent_child_nodes[node.node.node_id] = [node]\n",
    "                continue\n",
    "            \n",
    "            # 记录最高分数\n",
    "            if parent_id not in parent_scores:\n",
    "                parent_scores[parent_id] = node.score\n",
    "                parent_child_nodes[parent_id] = [node]\n",
    "            else:\n",
    "                # 取最高分\n",
    "                parent_scores[parent_id] = max(parent_scores[parent_id], node.score)\n",
    "                parent_child_nodes[parent_id].append(node)\n",
    "        \n",
    "        # 构建父节点列表\n",
    "        parent_nodes = []\n",
    "        for parent_id, score in parent_scores.items():\n",
    "            if parent_id in self.parent_node_map:\n",
    "                # 使用保存的原始父节点\n",
    "                parent_node = copy.deepcopy(self.parent_node_map[parent_id])\n",
    "                parent_node.score = score\n",
    "                \n",
    "                # 可选: 在metadata中记录匹配的子节点信息\n",
    "                child_info = [\n",
    "                    {\n",
    "                        'chunk_index': n.node.metadata.get('chunk_index'),\n",
    "                        'score': n.score,\n",
    "                        'text_preview': n.node.text[:100]\n",
    "                    }\n",
    "                    for n in parent_child_nodes[parent_id]\n",
    "                ]\n",
    "                parent_node.node.metadata['matched_children'] = child_info\n",
    "                \n",
    "                parent_nodes.append(parent_node)\n",
    "            else:\n",
    "                # 如果找不到父节点,使用第一个子节点(不应该发生)\n",
    "                print(f\"警告: 找不到父节点 {parent_id}, 使用子节点代替\")\n",
    "                parent_nodes.append(parent_child_nodes[parent_id][0])\n",
    "        \n",
    "        # 按分数排序并返回top_k\n",
    "        parent_nodes.sort(key=lambda x: x.score, reverse=True)\n",
    "        return parent_nodes[:self.keep_top_k]\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True  # 允许任意类型\n",
    "\n",
    "# ==================== 3. 自定义检索器包装器 ====================\n",
    "class SplitNodeRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    包装原始检索器,自动处理节点分割\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        base_retriever: BaseRetriever,\n",
    "        chunk_size: int = 512,\n",
    "        overlap_ratio: float = 0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_retriever: 原始混合检索器\n",
    "            chunk_size: 子节点大小\n",
    "            overlap_ratio: 重叠比例\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.base_retriever = base_retriever\n",
    "        self.node_splitter = NodeSplitter(chunk_size, overlap_ratio)\n",
    "        self.parent_node_map = {}\n",
    "    \n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"\n",
    "        检索并分割节点\n",
    "        \"\"\"\n",
    "        # 1. 使用原始检索器检索\n",
    "        nodes = self.base_retriever.retrieve(query_bundle)\n",
    "        \n",
    "        # 2. 分割节点\n",
    "        child_nodes, self.parent_node_map = self.node_splitter.split_nodes(nodes)\n",
    "        \n",
    "        print(f\"原始节点数: {len(nodes)}, 分割后子节点数: {len(child_nodes)}\")\n",
    "        \n",
    "        return child_nodes\n",
    "    \n",
    "    def get_parent_node_map(self) -> Dict[str, NodeWithScore]:\n",
    "        \"\"\"获取父节点映射,供后处理器使用\"\"\"\n",
    "        return self.parent_node_map\n",
    "\n",
    "\n",
    "def create_parent_postprocessor(retriever: SplitNodeRetriever, keep_top_k: int = 5):\n",
    "    \"\"\"动态创建父节点后处理器\"\"\"\n",
    "    return ChildToParentPostprocessor(\n",
    "        parent_node_map=retriever.get_parent_node_map(),\n",
    "        keep_top_k=keep_top_k\n",
    "    )\n",
    "\n",
    "class DynamicQueryEngine:\n",
    "    \"\"\"支持动态后处理器的查询引擎\"\"\"\n",
    "    \n",
    "    def __init__(self, retriever, response_synthesizer, reranker, keep_top_k=5):\n",
    "        self.retriever = retriever\n",
    "        self.response_synthesizer = response_synthesizer\n",
    "        self.reranker = reranker\n",
    "        self.keep_top_k = keep_top_k\n",
    "    \n",
    "    def query(self, query_str: str):\n",
    "        from llama_index.core.schema import QueryBundle\n",
    "        \n",
    "        # 1. 检索 (自动分割节点)\n",
    "        query_bundle = QueryBundle(query_str=query_str)\n",
    "        nodes = self.retriever.retrieve(query_bundle)\n",
    "        \n",
    "        # 2. Rerank子节点\n",
    "        reranked_nodes = self.reranker.postprocess_nodes(nodes, query_bundle)\n",
    "        \n",
    "        # 3. 动态创建父节点后处理器\n",
    "        parent_postprocessor = create_parent_postprocessor(\n",
    "            self.retriever, \n",
    "            keep_top_k=self.keep_top_k\n",
    "        )\n",
    "        \n",
    "        # 4. 还原为父节点\n",
    "        parent_nodes = parent_postprocessor.postprocess_nodes(reranked_nodes, query_bundle)\n",
    "        \n",
    "        # 5. 生成回答\n",
    "        response = self.response_synthesizer.synthesize(\n",
    "            query=query_str,\n",
    "            nodes=parent_nodes\n",
    "        )\n",
    "        \n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "236213d2-b889-4a13-89ce-8d5939dce30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_retriever = SplitNodeRetriever(\n",
    "    base_retriever=hybrid_retriever,\n",
    "    chunk_size=512,      # 分割为512长度 (或256)\n",
    "    overlap_ratio=0.1    # 10%重叠\n",
    ")\n",
    "\n",
    "dynamic_query_engine = DynamicQueryEngine(\n",
    "    retriever=split_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    reranker=reranker,\n",
    "    keep_top_k=5  # 最终返回5个父节点\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4879465-574b-4ca0-b7e6-dcd6c1a67aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. 根据《支付结算办法》第28条和第32条，单位卡用于购买高价值艺术品是否构成大额交易或可疑交易？是否存在未按规定进行交易背景审查的情形？\n",
      "2. 依据《金融机构客户尽职调查和客户身份资料及交易记录保存管理办法》第12条和第17条，外国政要关联客户单笔11万元人民币的非日常交易是否触发强化尽职调查义务？是否符合“大额交易”标准？\n",
      "3. 结合《支付结算办法》第15条及《客户尽职调查管理办法》第14条，使用单位卡进行非经营性大额消费是否违反单位卡使用范围规定，且是否构成需报告的可疑交易？\n",
      "原始节点数: 2, 分割后子节点数: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934675d13d10411b9555b7dc2b5c06eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据提供的《支付结算办法》内容，对第一笔交易（使用腾讯发行的单位卡购买一件价值11万元人民币的艺术品）进行分析如下：\n",
      "\n",
      "1. **违反“支付结算办法”第-百四十二条**：\n",
      "   - 该条款明确规定：“单位卡不得用于１０万元以上的商品交易、劳务供应款项的结算。”\n",
      "   - 本次交易金额为11万元人民币，超过10万元限额，因此**直接违反**该条款。\n",
      "\n",
      "2. **触发“客户尽职调查和客户身份资料及交易记录保存管理办法”的强制措施门槛**：\n",
      "   - 文件中未提及《多家客户尽职调查和客户身份资料及交易记录保存管理办法》的具体内容或相关条款。\n",
      "   - 因此，**无法根据现有文件判断是否触发该办法中的强制措施门槛**。\n",
      "\n",
      "综上，仅依据《支付结算办法》：\n",
      "- **存在明确违反行为**：违反第-百四十二条关于单位卡结算限额的规定。\n",
      "- **无依据支持触发客户尽职调查相关强制措施**。\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "您是一家大型商业银行的首席合规官。您的一位客户是腾讯的一位高管，由于其家庭关系，他也被列为“外国政要”。在腾讯2025年第二季度财报发布后的一周内，他通过贵行进行了以下交易：\n",
    "\n",
    "他使用腾讯发行的单位卡购买了一件价值11万元人民币的艺术品，摆放在办公室。\n",
    "\n",
    "他将8万元人民币现金存入个人账户，并注明这笔资金来自个人股息。\n",
    "\n",
    "他作为付款人签署了一张金额为600万元人民币的商业承兑汇票，付款期限为90天。该草案旨在为一家3D打印公司提供新的融资，该公司将使用腾讯的“混元3D模型”人工智能服务，该服务在最近的财报中被重点提及。\n",
    "\n",
    "您的任务：\n",
    "\n",
    "仅根据提供的文件，回答以下问题。\n",
    "\n",
    "对于这第一笔交易，请分别找出任何可能违反“支付结算办法”的情况，或任何触发“多家客户尽职调查和客户身份资料及交易记录保存管理办法”强制措施的特定门槛。请引用文件中的具体条款编号来支持您的发现。\"\"\"\n",
    "response = dynamic_query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad005fea-9d8a-48ce-aa8d-7bc4fbc79331",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

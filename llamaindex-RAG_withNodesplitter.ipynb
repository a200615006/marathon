{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee24946b-b151-4429-9ac6-79491fb9f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama_index transformers unstructured pymilvus\n",
    "!pip install llama-index-core\n",
    "!pip install llama-index-extractors-entity\n",
    "!pip install llama-index-vector-stores-milvus\n",
    "!pip install llama-index-embeddings-huggingface\n",
    "!pip install llama-index-llms-huggingface\n",
    "!pip install llama-index-llms-dashscope\n",
    "!pip install llama-index-extractors\n",
    "!pip install pymilvus[milvus_lite]\n",
    "!pip install unstructured[docx]\n",
    "!pip install unstructured[doc]\n",
    "!pip install unstructured[txt]\n",
    "!pip install unstructured[md]\n",
    "!pip install fitz frontend tools\n",
    "!pip uninstall fitz pymupdf -y\n",
    "!pip install pymupdf\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd904dc2-b816-432f-bf16-6790cb770384",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validate_default' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'validate_default' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (VectorStoreIndex, SimpleDirectoryReader, load_index_from_storage\n",
    "    , Document, Settings, StorageContext, PromptTemplate)\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.extractors import KeywordExtractor, SummaryExtractor\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.dashscope import DashScope\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "from llama_index.extractors.entity import EntityExtractor\n",
    "from llama_index.readers.file import UnstructuredReader,PyMuPDFReader,PDFReader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "import os, re, asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "794b343e-a87c-4676-8a87-9f4c499b83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python pdf2md.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8c75a3f-3416-499a-9ec9-edc255829118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-11 09:49:31,908 - INFO - Load pretrained SentenceTransformer: ./Qwen3-Embedding-0.6B\n",
      "2025-10-11 09:49:32,886 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹åµŒå…¥ç»´åº¦: 1024\n"
     ]
    }
   ],
   "source": [
    "embedding_model = \"./Qwen3-Embedding-0.6B\"\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=embedding_model,\n",
    "    cache_folder=None,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained(embedding_model, trust_remote_code=True, local_files_only=True)\n",
    "dimension = config.hidden_size\n",
    "print(f\"æ¨¡å‹åµŒå…¥ç»´åº¦: {dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77249092-c390-4c43-a08d-44631dff2612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼ˆQwenï¼‰ï¼Œæ˜¯é˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„é€šä¹‰å®éªŒå®¤è‡ªä¸»ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œæ¯”å¦‚å†™æ•…äº‹ã€å†™å…¬æ–‡ã€å†™é‚®ä»¶ã€å†™å‰§æœ¬ã€é€»è¾‘æ¨ç†ã€ç¼–ç¨‹ç­‰ç­‰ï¼Œè¿˜èƒ½è¡¨è¾¾è§‚ç‚¹ï¼Œç©æ¸¸æˆç­‰ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.llms import (\n",
    "    CustomLLM,\n",
    "    CompletionResponse,\n",
    "    LLMMetadata,\n",
    ")\n",
    "from llama_index.core.llms.callbacks import llm_completion_callback\n",
    "from llama_index.core import Settings\n",
    "from typing import Any\n",
    "import requests\n",
    "\n",
    "\n",
    "class SiliconFlowLLM(CustomLLM):\n",
    "    \"\"\"ç¡…åŸºæµåŠ¨è‡ªå®šä¹‰ LLM\"\"\"\n",
    "    \n",
    "    model: str = \"Qwen/Qwen3-Next-80B-A3B-Instruct\"\n",
    "    api_key: str = \"\"\n",
    "    api_base: str = \"https://api.siliconflow.cn/v1\"\n",
    "    max_tokens: int = 4096\n",
    "    temperature: float = 0.1\n",
    "    \n",
    "    @property\n",
    "    def metadata(self) -> LLMMetadata:\n",
    "        \"\"\"è·å– LLM å…ƒæ•°æ®\"\"\"\n",
    "        return LLMMetadata(\n",
    "            context_window=32768,  # æ ¹æ®å…·ä½“æ¨¡å‹è°ƒæ•´\n",
    "            num_output=self.max_tokens,\n",
    "            model_name=self.model,\n",
    "        )\n",
    "    \n",
    "    @llm_completion_callback()\n",
    "    def complete(self, prompt: str, **kwargs: Any) -> CompletionResponse:\n",
    "        \"\"\"å®Œæˆè¯·æ±‚\"\"\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        \n",
    "        data = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            \"max_tokens\": kwargs.get(\"max_tokens\", self.max_tokens),\n",
    "            \"temperature\": kwargs.get(\"temperature\", self.temperature),\n",
    "            \"stream\": False\n",
    "        }\n",
    "        \n",
    "        response = requests.post(\n",
    "            f\"{self.api_base}/chat/completions\",\n",
    "            headers=headers,\n",
    "            json=data\n",
    "        )\n",
    "        \n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        \n",
    "        return CompletionResponse(\n",
    "            text=result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        )\n",
    "    \n",
    "    @llm_completion_callback()\n",
    "    def stream_complete(self, prompt: str, **kwargs: Any):\n",
    "        \"\"\"æµå¼å®Œæˆï¼ˆæœªå®ç°ï¼Œä½†éœ€è¦å®šä¹‰ï¼‰\"\"\"\n",
    "        # è°ƒç”¨éæµå¼æ–¹æ³•\n",
    "        response = self.complete(prompt, **kwargs)\n",
    "        yield response\n",
    "\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. åˆ›å»ºè‡ªå®šä¹‰ LLM å®ä¾‹\n",
    "    llm = SiliconFlowLLM(\n",
    "        model=\"Qwen/Qwen3-30B-A3B-Instruct-2507\",  # å¯é€‰å…¶ä»–æ¨¡å‹\n",
    "        api_key=\"sk-ionsbeieleeekwlstqotkyrmictdzshgnbaytavcudxkixcs\",  # æ›¿æ¢ä¸ºä½ çš„ API Ke\n",
    "        api_base = \"https://api.siliconflow.cn/v1\",\n",
    "        max_tokens=1024,\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    # 2. è®¾ç½®åˆ° Settings\n",
    "    Settings.llm = llm\n",
    "    \n",
    "    # 3. æµ‹è¯•ä½¿ç”¨\n",
    "    response = llm.complete(\"ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±\")\n",
    "    print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a5cc3ae-1e31-4c71-aa92-7924a9545e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç»å¯¹æ•°æ®åº“è·¯å¾„: /root/milvus_test/milvus_lite.db\n"
     ]
    }
   ],
   "source": [
    "milvus_dir = \"./milvus_test\"\n",
    "milvus_db_path = os.path.join(milvus_dir, \"milvus_lite.db\")\n",
    "abs_db_path = os.path.abspath(milvus_db_path)\n",
    "print(f\"ç»å¯¹æ•°æ®åº“è·¯å¾„: {abs_db_path}\")\n",
    "\n",
    "if not os.path.exists(milvus_dir):\n",
    "    os.makedirs(milvus_dir)\n",
    "    print(\"å·²åˆ›å»º ./milvus ç›®å½•\")\n",
    "\n",
    "\n",
    "\n",
    "# milvus_vector_store = MilvusVectorStore(\n",
    "#     uri=f\"{abs_db_path}\",\n",
    "#     collection_name=\"rag_collection\",\n",
    "#     dim=1024,\n",
    "#     overwrite=True\n",
    "# )\n",
    "# storage_context = StorageContext.from_defaults(vector_store=milvus_vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5baa8d-7468-432a-9932-35ba29730ec0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### é¦–æ¬¡è¿è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0475e9bf-6391-498f-bc2c-241297bf1b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç»å¯¹æ•°æ®åº“è·¯å¾„: /root/milvus_test/milvus_lite.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    }
   ],
   "source": [
    "###é¦–æ¬¡è¿è¡Œ\n",
    "milvus_dir = \"./milvus_test\"\n",
    "milvus_db_path = os.path.join(milvus_dir, \"milvus_lite.db\")\n",
    "abs_db_path = os.path.abspath(milvus_db_path)\n",
    "print(f\"ç»å¯¹æ•°æ®åº“è·¯å¾„: {abs_db_path}\")\n",
    "\n",
    "if not os.path.exists(milvus_dir):\n",
    "    os.makedirs(milvus_dir)\n",
    "    print(\"å·²åˆ›å»º ./milvus ç›®å½•\")\n",
    "\n",
    "\n",
    "\n",
    "milvus_vector_store = MilvusVectorStore(\n",
    "    uri=f\"{abs_db_path}\",\n",
    "    collection_name=\"rag_collection\",\n",
    "    dim=1024,\n",
    "    overwrite=True\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=milvus_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "567914c9-37a4-4739-bf70-da546fa297e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text).strip()\n",
    "    # text = re.sub(r'(\\w+\\s*){3,}\\n', '', text)\n",
    "    # text = re.sub(r'[^a-zA-Z0-9\\u4e00-\\u9fa5\\s\\.,!?]', '', text)  # å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­è‹±æ–‡\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc0b85-f8aa-4101-9b40-b2df8a3aa7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13a22cbd-aedb-4285-906f-fde3ae0022d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_summary_async(text, max_words=30):\n",
    "    prompt = f\"æ€»ç»“ä»¥ä¸‹æ–‡æœ¬ï¼Œä¸è¶…è¿‡{max_words}å­—ï¼Œç›´æ¥å›å¤ç»“æœï¼š{text}\"\n",
    "    response = await Settings.llm.acomplete(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "def generate_summary(text, max_words=30):\n",
    "    prompt = f\"æ€»ç»“ä»¥ä¸‹æ–‡æœ¬ï¼Œä¸è¶…è¿‡{max_words}å­—ï¼Œç›´æ¥å›å¤ç»“æœï¼š{text}\"\n",
    "    response = Settings.llm.complete(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "async def add_summaries_to_nodes_async(nodes_list):\n",
    "    tasks = [generate_summary_async(node.text) for node in nodes_list]\n",
    "\n",
    "    summaries = []\n",
    "    for future in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"ç”ŸæˆèŠ‚ç‚¹æ‘˜è¦è¿›åº¦\"):\n",
    "        summary = await future\n",
    "        summaries.append(summary)\n",
    "\n",
    "    for node, summary in zip(nodes_list, summaries):\n",
    "        node.metadata[\"node_summary\"] = summary\n",
    "        \n",
    "def add_summaries_to_nodes(nodes_list):\n",
    "    for node in tqdm(nodes_list, desc=\"ç”Ÿæˆæ‘˜è¦\"):\n",
    "        summary = generate_summary(node.text)\n",
    "        node.metadata[\"node_summary\"] = summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "619e3fe0-1330-4417-be61-b13e6867ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_tokenizer = AutoTokenizer.from_pretrained(\"./Qwen3-Embedding-0.6B\", trust_remote_code=True)\n",
    "documents_dir = \"./docs\"\n",
    "\n",
    "file_extractor = {\n",
    "    \".docx\": UnstructuredReader(),\n",
    "    \".doc\": UnstructuredReader(),\n",
    "    \".txt\": UnstructuredReader(),\n",
    "    \".md\": UnstructuredReader(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bf8d575-4093-4782-81c5-c6d2175fae21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from pathlib import Path\n",
    "\n",
    "def load_single_file(file_path, file_extractor):\n",
    "    \"\"\"åŠ è½½å•ä¸ªæ–‡ä»¶\"\"\"\n",
    "    try:\n",
    "        ext = Path(file_path).suffix.lower()\n",
    "        if ext in file_extractor:\n",
    "            reader = file_extractor[ext]\n",
    "            print('loading:',file_path)\n",
    "            docs = reader.load_data(file_path)\n",
    "            return docs\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"åŠ è½½æ–‡ä»¶ {file_path} å¤±è´¥: {e}\")\n",
    "        return []\n",
    "\n",
    "def load_documents_parallel(documents_dir, file_extractor, max_workers=4):\n",
    "    \"\"\"å¹¶è¡ŒåŠ è½½æ–‡æ¡£\"\"\"\n",
    "    all_files = []\n",
    "    for ext in file_extractor.keys():\n",
    "        all_files.extend(Path(documents_dir).rglob(f\"*{ext}\"))\n",
    "    \n",
    "    documents = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(load_single_file, str(f), file_extractor): f \n",
    "                   for f in all_files}\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"åŠ è½½æ–‡ä»¶\"):\n",
    "            docs = future.result()\n",
    "            documents.extend(docs)\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91cf755f-70db-417a-8649-90a166ae9c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_long_documents(documents, max_length=100000, overlap=0):\n",
    "    \"\"\"é¢„å¤„ç†è¶…é•¿æ–‡æ¡£ï¼Œé¿å… tokenizer å¤„ç†è¶…é•¿æ–‡æœ¬\"\"\"\n",
    "    processed_docs = []\n",
    "    for doc in documents:\n",
    "        text_length = len(doc.text)\n",
    "        # å¦‚æœæ–‡æ¡£å¤ªé•¿ï¼Œå…ˆç²—åˆ‡åˆ†\n",
    "        if text_length > max_length:\n",
    "            print(f\"æ£€æµ‹åˆ°è¶…é•¿æ–‡æ¡£: {text_length} å­—ç¬¦ï¼Œè¿›è¡Œé¢„åˆ‡åˆ†\")\n",
    "            # æŒ‰å›ºå®šé•¿åº¦åˆ‡åˆ†ï¼Œå¸¦é‡å \n",
    "            chunks = []\n",
    "            start = 0\n",
    "            chunk_index = 0\n",
    "            \n",
    "            while start < text_length:\n",
    "                end = min(start + max_length, text_length)\n",
    "                chunk_text = doc.text[start:end]\n",
    "                \n",
    "                # åˆ›å»ºæ–°çš„ metadataï¼Œæ·»åŠ åˆ‡ç‰‡ä¿¡æ¯\n",
    "                new_metadata = doc.metadata.copy() if doc.metadata else {}\n",
    "                new_metadata['chunk_index'] = chunk_index\n",
    "                new_metadata['total_chunks'] = (text_length + max_length - overlap - 1) // (max_length - overlap)\n",
    "                new_metadata['is_chunked'] = True\n",
    "                \n",
    "                chunks.append(Document(text=chunk_text, metadata=new_metadata))\n",
    "                \n",
    "                # ä¸‹ä¸€ä¸ªèµ·ç‚¹ï¼šå½“å‰èµ·ç‚¹ + (max_length - overlap)\n",
    "                # è¿™æ ·å¯ä»¥ä¿è¯å‰åé‡å  overlap ä¸ªå­—ç¬¦\n",
    "                start += (max_length - overlap)\n",
    "                chunk_index += 1\n",
    "            \n",
    "            processed_docs.extend(chunks)\n",
    "            print(f\"  åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªå—ï¼Œæ¯å—æœ€å¤§ {max_length} å­—ç¬¦ï¼Œé‡å  {overlap} å­—ç¬¦\")\n",
    "        else:\n",
    "            processed_docs.append(doc)\n",
    "    \n",
    "    return processed_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9fcf3cd-934a-4566-b27a-3a969b786365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading:loading: docs/ä¸­åäººæ°‘å…±å’Œå›½ä¸­å›½äººæ°‘é“¶è¡Œæ³•.txt\n",
      " docs/test_ml.docx\n",
      "loading: docs/ä¸­å›½äººæ°‘é“¶è¡Œå†³å®šå®è¡Œå·®åˆ«å­˜æ¬¾å‡†å¤‡é‡‘ç‡åˆ¶åº¦.txt\n",
      "loading: docs/ä¸­æ–‡æ–°é—» 2.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "åŠ è½½æ–‡ä»¶:   0%|          | 0/16 [00:00<?, ?it/s]2025-10-10 15:11:56,254 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "åŠ è½½æ–‡ä»¶:   6%|â–‹         | 1/16 [00:00<00:01,  9.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/ä¸­æ–‡æ–°é—».txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:11:56,550 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "åŠ è½½æ–‡ä»¶:  12%|â–ˆâ–        | 2/16 [00:00<00:03,  3.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/äººæ°‘å¸é“¶è¡Œç»“ç®—è´¦æˆ·ç®¡ç†åŠæ³•.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:11:57,834 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "åŠ è½½æ–‡ä»¶:  19%|â–ˆâ–‰        | 3/16 [00:01<00:08,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/é‡‘èæœºæ„å®¢æˆ·å°½èŒè°ƒæŸ¥å’Œå®¢æˆ·èº«ä»½èµ„æ–™åŠäº¤æ˜“è®°å½•ä¿å­˜ç®¡ç†åŠæ³•.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:11:58,990 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "åŠ è½½æ–‡ä»¶:  25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:02<00:10,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/.ipynb_checkpoints/ä¸­å›½äººæ°‘é“¶è¡Œå†³å®šå®è¡Œå·®åˆ«å­˜æ¬¾å‡†å¤‡é‡‘ç‡åˆ¶åº¦-checkpoint.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:11:59,640 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "åŠ è½½æ–‡ä»¶:  31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:03<00:08,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/README-qwen3next.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:12:02,209 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "åŠ è½½æ–‡ä»¶:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:06<00:14,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/upinfo2.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:12:03,318 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "åŠ è½½æ–‡ä»¶:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:07<00:11,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/å¤–ä¼è´¢æŠ¥ 1.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:12:11,206 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "åŠ è½½æ–‡ä»¶:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:15<00:27,  3.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/å¤–ä¼è´¢æŠ¥ 2.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:12:14,526 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "åŠ è½½æ–‡ä»¶:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:18<00:23,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/å¤–ä¼è´¢æŠ¥ 3.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:12:16,533 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "åŠ è½½æ–‡ä»¶:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 10/16 [00:20<00:17,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/æ”¯ä»˜ç»“ç®—åŠæ³•.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:12:16,851 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "2025-10-10 15:12:16,863 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "åŠ è½½æ–‡ä»¶:  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:20<00:10,  2.17s/it]2025-10-10 15:12:16,996 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading: docs/è…¾è®¯å¹´æŠ¥å¤§ç¹ä½“.md\n",
      "loading: docs/è…¾è®¯è´¢æŠ¥ 2025 ç®€ä½“.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-10 15:12:18,384 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "åŠ è½½æ–‡ä»¶:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:22<00:02,  1.21s/it]2025-10-10 15:12:19,573 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "åŠ è½½æ–‡ä»¶:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:23<00:01,  1.20s/it]2025-10-10 15:12:19,836 - WARNING - 'doc_id' is deprecated and 'id_' will be used instead\n",
      "åŠ è½½æ–‡ä»¶: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:23<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–‡ä»¶å¤§å°:16\n",
      "èŠ‚ç‚¹æ•°é‡:2412\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨æ–¹æ³•\n",
    "documents = load_documents_parallel(documents_dir, file_extractor, max_workers=1)\n",
    "\n",
    "cleaned_documents = [Document(text=clean_text(doc.text), metadata=doc.metadata) \n",
    "                     for doc in documents]\n",
    "\n",
    "# æ·»åŠ è¿™ä¸€æ­¥ï¼šæœ€å¤§é•¿åº¦100000ï¼Œå‰åé‡å 1000\n",
    "# cleaned_documents = preprocess_long_documents(\n",
    "#     cleaned_documents, \n",
    "#     max_length=100000, \n",
    "#     overlap=0\n",
    "# )\n",
    "documents = cleaned_documents\n",
    "\n",
    "print(f\"æ–‡ä»¶å¤§å°:{len(documents)}\")\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=100, tokenizer=qwen_tokenizer.tokenize)  \n",
    "nodes = node_parser.get_nodes_from_documents(documents)\n",
    "print(f\"èŠ‚ç‚¹æ•°é‡:{len(nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786076ab-5328-4eba-9b90-9e5669da92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_summaries_to_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48734949-0a3b-44d5-92b2-6b4d45100eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_summaries_to_json(nodes_list, file_path=\"nodes_summaries_temp.json\"):\n",
    "#     summaries_dict = {}\n",
    "#     for idx, node in enumerate(nodes_list):\n",
    "#         summaries_dict[str(idx)] = node.metadata.get(\"node_summary\", \"\")  # è·å–æ‘˜è¦ï¼Œè‹¥æ— åˆ™ä¸ºç©º\n",
    "    \n",
    "#     # ä¿å­˜åˆ° JSON\n",
    "#     with open(file_path, 'w', encoding='utf-8') as f:\n",
    "#         json.dump(summaries_dict, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "#     print(f\"èŠ‚ç‚¹æ‘˜è¦å·²ä¿å­˜åˆ° {file_path}\")\n",
    "\n",
    "# def load_summaries_to_nodes(nodes_list, file_path=\"nodes_summaries.json\"):\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         summaries_dict = json.load(f)\n",
    "#     sorted_keys = sorted(summaries_dict.keys(), key=int)\n",
    "\n",
    "#     for key in sorted_keys:\n",
    "#         idx = int(key)\n",
    "#         if idx < len(nodes_list):\n",
    "#             nodes_list[idx].metadata[\"node_summary\"] = summaries_dict[key]\n",
    "#         else:\n",
    "#             print(f\"è­¦å‘Šï¼šç´¢å¼• {idx} è¶…å‡ºèŠ‚ç‚¹åˆ—è¡¨é•¿åº¦ï¼Œè·³è¿‡ã€‚\")\n",
    "    \n",
    "#     return nodes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca35dc24-2ae5-4cb1-8adf-14b5645bcd39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1da1464-2d9c-4c15-a3b0-cb2da61a26b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_summaries_to_json(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f633b2f-4bff-4ae3-9327-1d6a18ded997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ ä¿å­˜ Nodes ============\n",
    "def save_nodes(nodes, save_dir=\"./saved_nodes\"):\n",
    "    \"\"\"ä¿å­˜èŠ‚ç‚¹æ•°æ®ï¼ˆæ”¯æŒpickleå’Œjsonä¸¤ç§æ ¼å¼ï¼‰\"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # æ–¹æ³•1: ä½¿ç”¨ pickle ä¿å­˜å®Œæ•´èŠ‚ç‚¹å¯¹è±¡ï¼ˆæ¨èï¼‰\n",
    "    pickle_file = save_path / \"nodes.pkl\"\n",
    "    with open(pickle_file, 'wb') as f:\n",
    "        pickle.dump(nodes, f)\n",
    "    print(f\"Nodeså·²ä¿å­˜åˆ°: {pickle_file}\")\n",
    "\n",
    "def load_nodes(save_dir=\"./saved_data\"):\n",
    "    \"\"\"åŠ è½½èŠ‚ç‚¹æ•°æ®\"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    pickle_file = save_path / \"nodes.pkl\"\n",
    "    \n",
    "    if not pickle_file.exists():\n",
    "        raise FileNotFoundError(f\"âŒ æ‰¾ä¸åˆ°èŠ‚ç‚¹æ–‡ä»¶: {pickle_file}\")\n",
    "    \n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        nodes = pickle.load(f)\n",
    "    \n",
    "    print(f\"âœ… å·²åŠ è½½ {len(nodes)} ä¸ªèŠ‚ç‚¹\")\n",
    "    \n",
    "    # éªŒè¯æ•°æ®\n",
    "    print(f\"ğŸ“Š èŠ‚ç‚¹éªŒè¯:\")\n",
    "    print(f\"  - æ€»èŠ‚ç‚¹æ•°: {len(nodes)}\")\n",
    "    return nodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62b589d3-18f7-4672-9653-2be5ba001a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodeså·²ä¿å­˜åˆ°: saved_nodes/nodes.pkl\n"
     ]
    }
   ],
   "source": [
    "save_nodes(nodes, save_dir=\"./saved_nodes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "775b208d-7576-4475-89ad-5cafb8a9cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents,\n",
    "#     storage_context=storage_context,\n",
    "#     embed_model=Settings.embed_model,\n",
    "#     node_parser=node_parser,\n",
    "#     store_nodes_override=True\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "transformations = [node_parser]\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=Settings.embed_model,\n",
    "    node_parser=node_parser,\n",
    "    transformations=transformations,\n",
    "    store_nodes_override=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d8e8761-356e-4966-b632-003edb975da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ç´¢å¼•å·²ä¿å­˜åˆ°: ./milvus_storage\n",
      "âœ… ç´¢å¼•ä¿¡æ¯å·²ä¿å­˜åˆ°: milvus_storage/index_info.json\n",
      "ğŸ“Š ç´¢å¼•ä¿¡æ¯: {'collection_name': 'rag_collection', 'milvus_db_path': '/root/milvus_test/milvus_lite.db', 'embedding_dim': 1024, 'total_documents': 2493, 'index_type': 'VectorStoreIndex'}\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from pathlib import Path\n",
    "# ============ ä¿å­˜ Milvus ç´¢å¼• ============\n",
    "def save_milvus_index(index, persist_dir=\"./milvus_storage\"):\n",
    "    \"\"\"ä¿å­˜Milvusç´¢å¼•ï¼ˆæŒä¹…åŒ–åˆ°æœ¬åœ°ï¼‰\"\"\"\n",
    "    persist_path = Path(persist_dir)\n",
    "    persist_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # LlamaIndexä¼šè‡ªåŠ¨ä¿å­˜ç´¢å¼•ç»“æ„å’Œdocstore\n",
    "    index.storage_context.persist(persist_dir=persist_dir)\n",
    "    \n",
    "    print(f\"âœ… ç´¢å¼•å·²ä¿å­˜åˆ°: {persist_dir}\")\n",
    "    \n",
    "    # ä¿å­˜ç´¢å¼•å…ƒä¿¡æ¯\n",
    "    index_info = {\n",
    "        'collection_name': 'rag_collection',\n",
    "        'milvus_db_path': abs_db_path,\n",
    "        'embedding_dim': dimension,\n",
    "        'total_documents': len(index.docstore.docs),\n",
    "        'index_type': 'VectorStoreIndex'\n",
    "    }\n",
    "    \n",
    "    info_file = persist_path / \"index_info.json\"\n",
    "    with open(info_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(index_info, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"âœ… ç´¢å¼•ä¿¡æ¯å·²ä¿å­˜åˆ°: {info_file}\")\n",
    "    print(f\"ğŸ“Š ç´¢å¼•ä¿¡æ¯: {index_info}\")\n",
    "\n",
    "# ============ ä½¿ç”¨ç¤ºä¾‹ ============\n",
    "# ä¿å­˜ç´¢å¼•\n",
    "save_milvus_index(index, persist_dir=\"./milvus_storage\")\n",
    "\n",
    "# åŠ è½½ç´¢å¼•\n",
    "# index = load_milvus_index(persist_dir=\"./milvus_storage\", milvus_db_path=abs_db_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17816c31-5fe8-4cb9-9237-f17cefd16c2c",
   "metadata": {},
   "source": [
    "### éç¬¬ä¸€æ¬¡è¿è¡Œ åŠ è½½æŒä¹…åŒ–è¿è¡Œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "103c2661-6d60-4e1c-a448-0b588aaaab13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "def load_nodes(save_dir=\"./saved_data\"):\n",
    "    \"\"\"åŠ è½½èŠ‚ç‚¹æ•°æ®\"\"\"\n",
    "    save_path = Path(save_dir)\n",
    "    pickle_file = save_path / \"nodes.pkl\"\n",
    "    \n",
    "    if not pickle_file.exists():\n",
    "        raise FileNotFoundError(f\"âŒ æ‰¾ä¸åˆ°èŠ‚ç‚¹æ–‡ä»¶: {pickle_file}\")\n",
    "    \n",
    "    with open(pickle_file, 'rb') as f:\n",
    "        nodes = pickle.load(f)\n",
    "    \n",
    "    print(f\"âœ… å·²åŠ è½½ {len(nodes)} ä¸ªèŠ‚ç‚¹\")\n",
    "    \n",
    "    # éªŒè¯æ•°æ®\n",
    "    print(f\"ğŸ“Š èŠ‚ç‚¹éªŒè¯:\")\n",
    "    print(f\"  - æ€»èŠ‚ç‚¹æ•°: {len(nodes)}\")\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "155ebe6a-db42-4973-875e-7e484a786aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from pathlib import Path\n",
    "# ============ åŠ è½½ Milvus ç´¢å¼• ============\n",
    "def load_milvus_index(persist_dir=\"./storage\", milvus_db_path=None):\n",
    "    \"\"\"åŠ è½½å·²ä¿å­˜çš„Milvusç´¢å¼•\"\"\"\n",
    "    persist_path = Path(persist_dir)\n",
    "    \n",
    "    if not persist_path.exists():\n",
    "        raise FileNotFoundError(f\"âŒ æ‰¾ä¸åˆ°ç´¢å¼•ç›®å½•: {persist_dir}\")\n",
    "    \n",
    "    # è¯»å–ç´¢å¼•ä¿¡æ¯\n",
    "    info_file = persist_path / \"index_info.json\"\n",
    "    if info_file.exists():\n",
    "        with open(info_file, 'r', encoding='utf-8') as f:\n",
    "            index_info = json.load(f)\n",
    "        print(f\"ğŸ“Š ç´¢å¼•ä¿¡æ¯: {index_info}\")\n",
    "        milvus_db_path = milvus_db_path or index_info.get('milvus_db_path')\n",
    "    \n",
    "    # é‡å»º Milvus vector store\n",
    "    milvus_vector_store = MilvusVectorStore(\n",
    "        uri=milvus_db_path,\n",
    "        collection_name=\"rag_collection\",\n",
    "        dim=dimension,\n",
    "        overwrite=False  # ä¸è¦†ç›–å·²æœ‰æ•°æ®\n",
    "    )\n",
    "    \n",
    "    # é‡å»º storage context\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        vector_store=milvus_vector_store,\n",
    "        persist_dir=persist_dir\n",
    "    )\n",
    "    \n",
    "    # åŠ è½½ç´¢å¼•\n",
    "    index = load_index_from_storage(\n",
    "        storage_context=storage_context,\n",
    "        embed_model=Settings.embed_model\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… ç´¢å¼•å·²åŠ è½½\")\n",
    "    print(f\"  - æ–‡æ¡£æ•°é‡: {len(index.docstore.docs)}\")\n",
    "    \n",
    "    return index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ae0603-9369-494a-a304-a44b37d76ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²åŠ è½½ 2493 ä¸ªèŠ‚ç‚¹\n",
      "ğŸ“Š èŠ‚ç‚¹éªŒè¯:\n",
      "  - æ€»èŠ‚ç‚¹æ•°: 2493\n",
      "ğŸ“Š ç´¢å¼•ä¿¡æ¯: {'collection_name': 'rag_collection', 'milvus_db_path': '/root/milvus_test/milvus_lite.db', 'embedding_dim': 1024, 'total_documents': 2493, 'index_type': 'VectorStoreIndex'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n",
      "2025-10-11 09:49:37,774 - INFO - Loading all indices.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./milvus_storage/docstore.json.\n",
      "Loading llama_index.core.storage.kvstore.simple_kvstore from ./milvus_storage/index_store.json.\n",
      "âœ… ç´¢å¼•å·²åŠ è½½\n",
      "  - æ–‡æ¡£æ•°é‡: 2493\n"
     ]
    }
   ],
   "source": [
    "nodes = load_nodes(save_dir=\"./saved_nodes\")\n",
    "index = load_milvus_index(persist_dir=\"./milvus_storage\", milvus_db_path=abs_db_path)\n",
    "index.embed_model=Settings.embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cedb969-684f-4273-88a3-45e4c5edca58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1] åˆ†æ•°: 0.5139 | æ–‡ä»¶: None\n",
      "å†…å®¹: Â· æˆ‘å€‘å‡ç´šäº†å°éŠæˆ²çš„æŠ€è¡“åº•åº§ï¼Œèƒ½å¤ å…¼å®¹æ›´å¤šçš„éŠæˆ²å¼•æ“ï¼Œæå‡äº†åœ–åƒæ¸²æŸ“æ•ˆæœï¼Œé™ä½äº†åŠ è¼‰æ™‚é•·ï¼ŒåŠ©â¼’éŠæˆ² é–‹ç™¼è€…å°‡è¤‡é›œçš„æ‰‹æ©ŸéŠæˆ²æ‡‰â½¤é©é…è‡³å°éŠæˆ²ã€‚â¼†é›¶â¼†äº”å¹´ç¬¬â¼†å­£å°éŠæˆ²çš„ç¸½æµæ°´åŒæ¯”å¢é•· 20% ã€‚\n",
      "\n",
      "Â· æœ¬åœŸéŠæˆ²æ–¹é¢ï¼Œ ã€Šä¸‰è§’æ´²è¡Œå‹•ã€‹ æ˜¯æˆ‘å€‘â¼†é›¶â¼†å››å¹´ä¹æœˆåœ¨ç§»å‹•ç«¯å’Œå€‹äººé›»è…¦ç«¯æ¨å‡ºçš„ç¬¬-äººç¨±å°„æ“ŠéŠæˆ²ï¼Œ â¼†é›¶â¼†äº”å¹´ä¸ƒæœˆçš„å¹³å‡æ—¥æ´»èºè³¬æˆ¶æ•¸çªç ´ 2,000 è¬ï¼Œä½å±…è¡Œæ¥­æ—¥æ´»èºè³¬æˆ¶æ•¸å‰äº”ï¼Œæµæ°´å‰ä¸‰ ã€‚ 1\n",
      "\n",
      "Â·\n",
      "\n",
      "[2] åˆ†æ•°: 0.5011 | æ–‡ä»¶: None\n",
      "å†…å®¹: 411 1,371 3% 1,402 0.6% QQ çš„ç§»åŠ¨ç»ˆç«¯æœˆæ´»è·ƒè´¦æˆ·æ•° 532 571 -7% 534 -0.4% æ”¶è´¹å¢å€¼æœåŠ¡ä»˜è´¹ä¼šå‘˜æ•° 264 263 0.4% 268 -1% 3 å…¬å¸æ•°æ®ï¼Œ QuestMobile ï¼Œ Sensor Tower 4 2Q2025 ä»˜è´¹ä¼šå‘˜æ•°çš„æ—¥å‡å€¼ 5 2Q2025 æ¯æœˆæœ€åä¸€æ—¥çš„å¹³å‡ä»˜è´¹ä¼šå‘˜æ•° 6 å‘å¸ƒäº https://huggingface.co\n",
      "\n",
      "[3] åˆ†æ•°: 0.4681 | æ–‡ä»¶: None\n",
      "å†…å®¹: 534 äº¿å…ƒã€‚æˆªæ­¢ 2025 å¹´ 6 æœˆ 30 æ—¥ï¼Œæˆ‘ä»¬äºéä¸Šå¸‚ æŠ•èµ„å…¬å¸ï¼ˆä¸åŒ…æ‹¬é™„å±å…¬å¸ï¼‰æƒç›Šçš„è´¦é¢ä»·å€¼ä¸ºäººæ°‘å¸ 3,423 äº¿å…ƒï¼Œç›¸è¾ƒäºæˆªæ­¢ 2025 å¹´ 3 æœˆ 31 æ—¥çš„è´¦ é¢ä»·å€¼ä¸ºäººæ°‘å¸ 3,379 äº¿å…ƒã€‚\n",
      "\n",
      "â–ª æœ¬å…¬å¸äº 2Q2025 äºé¦™æ¸¯è”äº¤æ‰€ä»¥çº¦ 194 äº¿æ¸¯å…ƒçš„æ€»ä»£ä»·å›è´­çº¦ 3,888 ä¸‡è‚¡è‚¡ä»½ã€‚ 1 éå›½é™…è´¢åŠ¡æŠ¥å‘Šå‡†åˆ™æ’‡é™¤è‚¡ä»½é…¬é‡‘ã€å¹¶è´­å¸¦æ¥çš„æ•ˆåº”ï¼Œå¦‚æ¥è‡ªæŠ•èµ„å…¬å¸çš„ï¼ˆæ”¶ç›Šï¼‰ äº\n"
     ]
    }
   ],
   "source": [
    "# å•æ¬¡å¿«é€Ÿæ£€ç´¢\n",
    "retriever = index.as_retriever(similarity_top_k=3)\n",
    "nodes_test = retriever.retrieve(\"è…¾è®¯æ¸¸æˆ ä¸‰è§’æ´²è¡ŒåŠ¨\")\n",
    "\n",
    "\n",
    "# æ‰“å°ç»“æœ\n",
    "for i, node in enumerate(nodes_test, 1):\n",
    "    print(f\"\\n[{i}] åˆ†æ•°: {node.score:.4f} | æ–‡ä»¶: {node.metadata.get('file_name')}\")\n",
    "    print(f\"å†…å®¹: {node.text[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6b2a35-74d4-496c-afed-46c7795044ea",
   "metadata": {},
   "source": [
    "## search and rerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf4934f-e7ed-479a-9c08-f0d91be86b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-index-retrievers-bm25\n",
    "# !pip install llama-index-packs-fusion-retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d73f2f98-4f58-400c-8ba3-f7ca568dea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.packs.fusion_retriever import HybridFusionRetrieverPack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "933289c0-cd96-4170-b131-649aeada250a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afb54dd869f544b5a8485c04e9ab711d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at autodl-tmp/Qwen3-Reranker-4B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token: [PAD]\n",
      "Pad token ID: 151669\n",
      "Model config pad_token_id: 151669\n"
     ]
    }
   ],
   "source": [
    "reranker_model_path = \"autodl-tmp/Qwen3-Reranker-4B\"\n",
    "\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=reranker_model_path,\n",
    "    top_n=5,\n",
    "    device=\"cuda\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "cross_encoder = reranker._model\n",
    "reranker_tokenizer = cross_encoder.tokenizer\n",
    "reranker_model = cross_encoder.model\n",
    "\n",
    "special_tokens = {'pad_token': '[PAD]'}\n",
    "num_added_tokens = reranker_tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "reranker_model.resize_token_embeddings(len(reranker_tokenizer))\n",
    "\n",
    "reranker_tokenizer.pad_token = '[PAD]'\n",
    "reranker_tokenizer.pad_token_id = reranker_tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "reranker_model.config.pad_token_id = reranker_tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Pad token: {reranker_tokenizer.pad_token}\")\n",
    "print(f\"Pad token ID: {reranker_tokenizer.pad_token_id}\")\n",
    "print(f\"Model config pad_token_id: {reranker_model.config.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e148b4f-7730-4b49-bc81-db658115c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reranker_tokenize(text):\n",
    "    rerank_tokenizer = AutoTokenizer.from_pretrained(\"autodl-tmp/Qwen3-Reranker-8B\", padding_side='left')\n",
    "    if not text.strip():\n",
    "        return []\n",
    "    tokens = rerank_tokenizer.tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55e55514-7ff9-40fa-b4b0-f8a310914217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-11 10:37:41,718 - WARNING - The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "2025-10-11 10:37:42,416 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    nodes=nodes, \n",
    "    similarity_top_k=10,\n",
    "    tokenizer=reranker_tokenize)\n",
    "vector_retriever = VectorIndexRetriever(index=index, similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a729b8c7-8842-4eb9-b29a-9870ef23cb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-11 10:40:30,352 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "hybrid_pack = HybridFusionRetrieverPack(\n",
    "    nodes=nodes,\n",
    "    bm25_retriever=bm25_retriever,\n",
    "    vector_retriever=vector_retriever,\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    similarity_top_k=20\n",
    ")\n",
    "hybrid_retriever = hybrid_pack.fusion_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1c25c162-cde9-413e-b26a-eea12ef1637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_qa_template_str = (\n",
    "    \"ä¸Šä¸‹æ–‡ä¿¡æ¯å¦‚ä¸‹ï¼š\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"åŸºäºæä¾›çš„ä¸Šä¸‹æ–‡ï¼Œç”¨ä¸­æ–‡ç›´æ¥å›ç­”æŸ¥è¯¢ï¼Œç­”æ¡ˆåªèƒ½ä»ä¸Šä¸‹æ–‡çŸ¥è¯†ä¸­è·å–ï¼Œä¸è¦è‡ªå·±å‘æŒ¥ã€‚\\n\"\n",
    "    \"æŸ¥è¯¢ï¼š{query_str}\\n\"\n",
    "    \"å›ç­”ï¼š\"\n",
    ")\n",
    "text_qa_template = PromptTemplate(text_qa_template_str)\n",
    "\n",
    "refine_template_str = (\n",
    "    \"åŸå§‹æŸ¥è¯¢æ˜¯ï¼š{query_str}\\n\"\n",
    "    \"æˆ‘ä»¬å·²æœ‰å›ç­”ï¼š{existing_answer}\\n\"\n",
    "    \"åŸºäºä»¥ä¸‹æ–°ä¸Šä¸‹æ–‡ï¼Œç”¨ä¸­æ–‡ç²¾ç‚¼ç°æœ‰å›ç­”ï¼Œé—®é¢˜çš„æ ¸å¿ƒå›ç­”è¦æ”¾åœ¨æœ€å‰è¾¹ï¼Œç„¶åæ˜¯è§£é‡Šï¼Œç¡®ä¿å®Œæ•´æ€§å’Œå‡†ç¡®æ€§ï¼š\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"ç²¾ç‚¼åçš„å›ç­”ï¼š\"\n",
    ")\n",
    "refine_template = PromptTemplate(refine_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18a7a043-1de9-41a8-9ce2-f51ee7157700",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer = get_response_synthesizer(\n",
    "    text_qa_template=text_qa_template,\n",
    "    refine_template=refine_template,\n",
    "    response_mode=\"compact\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48b14fdb-5b11-4de9-8d21-05918c0e989a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=hybrid_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[reranker]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "18c73b8c-df5b-40b9-bc8c-ca15e31f3745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. æ ¹æ®ã€Šæ”¯ä»˜ç»“ç®—åŠæ³•ã€‹ç¬¬28æ¡å’Œç¬¬35æ¡ï¼Œå•ä½å¡å•ç¬”äº¤æ˜“è¶…è¿‡10ä¸‡å…ƒäººæ°‘å¸æ˜¯å¦éœ€è¿›è¡Œé¢å¤–èº«ä»½æ ¸å®ä¸äº¤æ˜“èƒŒæ™¯å®¡æŸ¥ï¼Œä¸”è¯¥è‰ºæœ¯å“è´­ä¹°æ˜¯å¦å±äºâ€œå¼‚å¸¸å¤§é¢äº¤æ˜“â€è€Œè§¦å‘åæ´—é’±ç›‘æµ‹ï¼Ÿ\n",
      "2. ä¾æ®ã€Šé‡‘èæœºæ„å®¢æˆ·å°½èŒè°ƒæŸ¥å’Œå®¢æˆ·èº«ä»½èµ„æ–™åŠäº¤æ˜“è®°å½•ä¿å­˜ç®¡ç†åŠæ³•ã€‹ç¬¬12æ¡å’Œç¬¬18æ¡ï¼Œå¤–å›½æ”¿è¦å…³è”å®¢æˆ·å•ç¬”äº¤æ˜“è¶…è¿‡5ä¸‡å…ƒäººæ°‘å¸æ˜¯å¦æ„æˆâ€œé«˜é£é™©å®¢æˆ·â€äº¤æ˜“ï¼Œéœ€å¯åŠ¨å¼ºåŒ–å°½èŒè°ƒæŸ¥ç¨‹åºï¼Ÿ\n",
      "3. ç»“åˆã€Šå®¢æˆ·å°½èŒè°ƒæŸ¥å’Œå®¢æˆ·èº«ä»½èµ„æ–™åŠäº¤æ˜“è®°å½•ä¿å­˜ç®¡ç†åŠæ³•ã€‹ç¬¬15æ¡ï¼Œå¯¹äºæ¶‰åŠâ€œå¤–å›½æ”¿è¦â€èº«ä»½çš„å®¢æˆ·ï¼Œå…¶é€šè¿‡å•ä½å¡è¿›è¡Œ11ä¸‡å…ƒè‰ºæœ¯å“è´­ä¹°æ˜¯å¦æ„æˆâ€œéæ­£å¸¸äº¤æ˜“ç›®çš„â€å¹¶è§¦å‘å¼ºåˆ¶æ€§å®¢æˆ·èº«ä»½èµ„æ–™ä¿å­˜ä¸æŠ¥å‘Šä¹‰åŠ¡ï¼Ÿ\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e67d03ac1ba47c991efb1e10a9462f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ ¹æ®æä¾›çš„æ–‡ä»¶å†…å®¹ï¼Œå¯¹ç¬¬ä¸€ç¬”äº¤æ˜“ï¼ˆä½¿ç”¨è…¾è®¯å‘è¡Œçš„å•ä½å¡è´­ä¹°ä¸€ä»¶ä»·å€¼11ä¸‡å…ƒäººæ°‘å¸çš„è‰ºæœ¯å“ï¼‰è¿›è¡Œåˆ†æï¼š\n",
      "\n",
      "1. **å…³äºã€Šæ”¯ä»˜ç»“ç®—åŠæ³•ã€‹çš„åˆè§„æ€§åˆ†æ**ï¼š\n",
      "   - æ–‡ä»¶ä¸­æœªæåŠå•ä½å¡è´­ä¹°è‰ºæœ¯å“æ˜¯å¦å±äºç¦æ­¢æˆ–å—é™çš„æ”¯ä»˜ç»“ç®—è¡Œä¸ºã€‚\n",
      "   - äº¤æ˜“é‡‘é¢ä¸º11ä¸‡å…ƒäººæ°‘å¸ï¼Œæœªè¶…è¿‡ä»»ä½•æ˜ç¡®çš„é™é¢æˆ–è§¦å‘ç‰¹æ®Šæ”¶è´¹æˆ–é™åˆ¶æ¡æ¬¾ã€‚\n",
      "   - ç”µæŠ¥è´¹ã€æ‰‹ç»­è´¹ã€é‚®ç”µè´¹ç­‰å‡æŒ‰æ ‡å‡†æ”¶å–ï¼Œæœªå‘ç°è¿åã€Šæ”¯ä»˜ç»“ç®—åŠæ³•ã€‹ä¸­å…³äºæ”¶è´¹æˆ–ç»“ç®—æµç¨‹çš„è§„å®šã€‚\n",
      "   - æœªå‘ç°è¯¥äº¤æ˜“è¿åã€Šæ”¯ä»˜ç»“ç®—åŠæ³•ã€‹ç¬¬äºŒç™¾äº”åå…«æ¡ã€ç¬¬äºŒç™¾äº”åä¹æ¡ã€ç¬¬äºŒç™¾å…­åæ¡ç­‰å…³äºæ‰§è¡Œä¼˜å…ˆçº§ã€è§£é‡Šæƒå’Œæ–½è¡Œæ—¶é—´çš„è§„å®šã€‚\n",
      "\n",
      "   âœ… **ç»“è®º**ï¼šè¯¥äº¤æ˜“**ä¸è¿åã€Šæ”¯ä»˜ç»“ç®—åŠæ³•ã€‹**ã€‚\n",
      "\n",
      "2. **å…³äºã€Šé‡‘èæœºæ„å®¢æˆ·å°½èŒè°ƒæŸ¥å’Œå®¢æˆ·èº«ä»½èµ„æ–™åŠäº¤æ˜“è®°å½•ä¿å­˜ç®¡ç†åŠæ³•ã€‹çš„å¼ºåˆ¶æªæ–½è§¦å‘æƒ…å†µ**ï¼š\n",
      "   - å®¢æˆ·èº«ä»½ï¼šè¯¥å®¢æˆ·ä¸ºè…¾è®¯é«˜ç®¡ï¼Œä¸”å› å®¶åº­å…³ç³»è¢«åˆ—ä¸ºâ€œå¤–å›½æ”¿è¦â€ï¼ˆPEPï¼‰ã€‚\n",
      "   - æ ¹æ®ã€Šé‡‘èæœºæ„å®¢æˆ·å°½èŒè°ƒæŸ¥å’Œå®¢æˆ·èº«ä»½èµ„æ–™åŠäº¤æ˜“è®°å½•ä¿å­˜ç®¡ç†åŠæ³•ã€‹**ç¬¬å››åä¸‰æ¡**ï¼Œé‡‘èæœºæ„åœ¨å¼€å±•å®¢æˆ·å°½èŒè°ƒæŸ¥æ—¶ï¼Œåº”å…³æ³¨å®¢æˆ·æ‰€åœ¨å›½å®¶æˆ–åœ°åŒºé£é™©çŠ¶å†µï¼Œä¸”å¯¹é«˜é£é™©å®¢æˆ·ï¼ˆå¦‚å¤–å›½æ”¿è¦ï¼‰éœ€åŠ å¼ºå°½èŒè°ƒæŸ¥ã€‚\n",
      "   - è™½ç„¶æœªæ˜ç¡®è¯´æ˜â€œå¤–å›½æ”¿è¦â€ç›´æ¥è§¦å‘æŠ¥å‘Šä¹‰åŠ¡ï¼Œä½†**ç¬¬å››åä¸‰æ¡ç¬¬ï¼ˆäºŒï¼‰é¡¹**è§„å®šï¼š  \n",
      "     > â€œæœ‰æ˜æ˜¾ç†ç”±æ€€ç–‘å®¢æˆ·å»ºç«‹ä¸šåŠ¡å…³ç³»çš„ç›®çš„å’Œæ€§è´¨ä¸æ´—é’±å’Œææ€–èèµ„ç­‰è¿æ³•çŠ¯ç½ªæ´»åŠ¨ç›¸å…³çš„â€  \n",
      "     åº”å½“æŠ¥å‘Šå¯ç–‘è¡Œä¸ºã€‚\n",
      "   - è¯¥å®¢æˆ·ä¸ºâ€œå¤–å›½æ”¿è¦â€ï¼Œä¸”äº¤æ˜“æ¶‰åŠå•ä½å¡è´­ä¹°é«˜ä»·å€¼è‰ºæœ¯å“ï¼ˆ11ä¸‡å…ƒï¼‰ï¼Œè™½æœªè¾¾å¤§é¢äº¤æ˜“æŠ¥å‘Šæ ‡å‡†ï¼ˆå¦‚10ä¸‡å…ƒäººæ°‘å¸ä»¥ä¸Šç°é‡‘äº¤æ˜“ï¼‰ï¼Œä½†ç»“åˆå…¶èº«ä»½ç‰¹æ®Šæ€§ï¼Œå­˜åœ¨è¾ƒé«˜æ´—é’±æˆ–è…è´¥é£é™©ã€‚\n",
      "   - æ ¹æ®**ç¬¬å››åä¸‰æ¡**ï¼Œè‹¥é‡‘èæœºæ„æœ‰ç†ç”±æ€€ç–‘è¯¥äº¤æ˜“ç›®çš„ä¸æ´—é’±æˆ–ææ€–èèµ„ç›¸å…³ï¼Œåº”å‘ä¸­å›½åæ´—é’±ç›‘æµ‹åˆ†æä¸­å¿ƒå’Œä¸­å›½äººæ°‘é“¶è¡Œå½“åœ°åˆ†æ”¯æœºæ„æŠ¥å‘Šã€‚\n",
      "\n",
      "   âœ… **ç»“è®º**ï¼šè¯¥äº¤æ˜“**è§¦å‘ã€Šé‡‘èæœºæ„å®¢æˆ·å°½èŒè°ƒæŸ¥å’Œå®¢æˆ·èº«ä»½èµ„æ–™åŠäº¤æ˜“è®°å½•ä¿å­˜ç®¡ç†åŠæ³•ã€‹ç¬¬å››åä¸‰æ¡çš„å¯ç–‘è¡Œä¸ºæŠ¥å‘Šä¹‰åŠ¡**ï¼Œå› å®¢æˆ·ä¸ºâ€œå¤–å›½æ”¿è¦â€ï¼Œä¸”äº¤æ˜“æ€§è´¨ï¼ˆå•ä½å¡è´­ä¹°é«˜ä»·å€¼è‰ºæœ¯å“ï¼‰å­˜åœ¨ä¸æ´—é’±æˆ–ææ€–èèµ„ç›¸å…³çš„åˆç†æ€€ç–‘ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "### æœ€ç»ˆå›ç­”ï¼š\n",
      "- **è¿åã€Šæ”¯ä»˜ç»“ç®—åŠæ³•ã€‹çš„æƒ…å†µ**ï¼šæ— ã€‚\n",
      "- **è§¦å‘ã€Šé‡‘èæœºæ„å®¢æˆ·å°½èŒè°ƒæŸ¥å’Œå®¢æˆ·èº«ä»½èµ„æ–™åŠäº¤æ˜“è®°å½•ä¿å­˜ç®¡ç†åŠæ³•ã€‹å¼ºåˆ¶æªæ–½çš„ç‰¹å®šé—¨æ§›**ï¼š  \n",
      "  **ç¬¬å››åä¸‰æ¡**ï¼ˆå› å®¢æˆ·ä¸ºâ€œå¤–å›½æ”¿è¦â€ï¼Œä¸”äº¤æ˜“å­˜åœ¨ä¸æ´—é’±æˆ–ææ€–èèµ„ç›¸å…³çš„åˆç†æ€€ç–‘ï¼Œåº”æŠ¥å‘Šå¯ç–‘è¡Œä¸ºï¼‰ã€‚\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "æ‚¨æ˜¯ä¸€å®¶å¤§å‹å•†ä¸šé“¶è¡Œçš„é¦–å¸­åˆè§„å®˜ã€‚æ‚¨çš„ä¸€ä½å®¢æˆ·æ˜¯è…¾è®¯çš„ä¸€ä½é«˜ç®¡ï¼Œç”±äºå…¶å®¶åº­å…³ç³»ï¼Œä»–ä¹Ÿè¢«åˆ—ä¸ºâ€œå¤–å›½æ”¿è¦â€ã€‚åœ¨è…¾è®¯2025å¹´ç¬¬äºŒå­£åº¦è´¢æŠ¥å‘å¸ƒåçš„ä¸€å‘¨å†…ï¼Œä»–é€šè¿‡è´µè¡Œè¿›è¡Œäº†ä»¥ä¸‹äº¤æ˜“ï¼š\n",
    "\n",
    "ä»–ä½¿ç”¨è…¾è®¯å‘è¡Œçš„å•ä½å¡è´­ä¹°äº†ä¸€ä»¶ä»·å€¼11ä¸‡å…ƒäººæ°‘å¸çš„è‰ºæœ¯å“ï¼Œæ‘†æ”¾åœ¨åŠå…¬å®¤ã€‚\n",
    "\n",
    "ä»–å°†8ä¸‡å…ƒäººæ°‘å¸ç°é‡‘å­˜å…¥ä¸ªäººè´¦æˆ·ï¼Œå¹¶æ³¨æ˜è¿™ç¬”èµ„é‡‘æ¥è‡ªä¸ªäººè‚¡æ¯ã€‚\n",
    "\n",
    "ä»–ä½œä¸ºä»˜æ¬¾äººç­¾ç½²äº†ä¸€å¼ é‡‘é¢ä¸º600ä¸‡å…ƒäººæ°‘å¸çš„å•†ä¸šæ‰¿å…‘æ±‡ç¥¨ï¼Œä»˜æ¬¾æœŸé™ä¸º90å¤©ã€‚è¯¥è‰æ¡ˆæ—¨åœ¨ä¸ºä¸€å®¶3Dæ‰“å°å…¬å¸æä¾›æ–°çš„èèµ„ï¼Œè¯¥å…¬å¸å°†ä½¿ç”¨è…¾è®¯çš„â€œæ··å…ƒ3Dæ¨¡å‹â€äººå·¥æ™ºèƒ½æœåŠ¡ï¼Œè¯¥æœåŠ¡åœ¨æœ€è¿‘çš„è´¢æŠ¥ä¸­è¢«é‡ç‚¹æåŠã€‚\n",
    "\n",
    "æ‚¨çš„ä»»åŠ¡ï¼š\n",
    "\n",
    "ä»…æ ¹æ®æä¾›çš„æ–‡ä»¶ï¼Œå›ç­”ä»¥ä¸‹é—®é¢˜ã€‚\n",
    "\n",
    "å¯¹äºè¿™ç¬¬ä¸€ç¬”äº¤æ˜“ï¼Œè¯·åˆ†åˆ«æ‰¾å‡ºä»»ä½•å¯èƒ½è¿åâ€œæ”¯ä»˜ç»“ç®—åŠæ³•â€çš„æƒ…å†µï¼Œæˆ–ä»»ä½•è§¦å‘â€œå¤šå®¶å®¢æˆ·å°½èŒè°ƒæŸ¥å’Œå®¢æˆ·èº«ä»½èµ„æ–™åŠäº¤æ˜“è®°å½•ä¿å­˜ç®¡ç†åŠæ³•â€å¼ºåˆ¶æªæ–½çš„ç‰¹å®šé—¨æ§›ã€‚è¯·å¼•ç”¨æ–‡ä»¶ä¸­çš„å…·ä½“æ¡æ¬¾ç¼–å·æ¥æ”¯æŒæ‚¨çš„å‘ç°ã€‚\"\"\"\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06741b50-ba2e-4380-8e2d-4268b1703e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1597/2662524485.py:101: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.12/migration/\n",
      "  class ChildToParentPostprocessor(BaseNodePostprocessor):\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.postprocessor.types import BaseNodePostprocessor\n",
    "from llama_index.core.schema import NodeWithScore, QueryBundle, TextNode\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import List, Optional, Dict\n",
    "import copy\n",
    "\n",
    "# ==================== 1. èŠ‚ç‚¹åˆ†å‰²å™¨ ====================\n",
    "class NodeSplitter:\n",
    "    \"\"\"å°†é•¿èŠ‚ç‚¹åˆ†å‰²æˆå¤šä¸ªå­èŠ‚ç‚¹,ä¿æŒçˆ¶å­å…³ç³»\"\"\"\n",
    "    \n",
    "    def __init__(self, chunk_size: int = 512, overlap_ratio: float = 0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            chunk_size: å­èŠ‚ç‚¹çš„ç›®æ ‡é•¿åº¦\n",
    "            overlap_ratio: é‡å æ¯”ä¾‹ (0.1 è¡¨ç¤º 10%)\n",
    "        \"\"\"\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap_size = int(chunk_size * overlap_ratio)\n",
    "        \n",
    "    def split_node(self, node: NodeWithScore, parent_id: str = None) -> List[NodeWithScore]:\n",
    "        \"\"\"\n",
    "        å°†å•ä¸ªèŠ‚ç‚¹åˆ†å‰²æˆå¤šä¸ªå­èŠ‚ç‚¹\n",
    "        \n",
    "        Args:\n",
    "            node: åŸå§‹èŠ‚ç‚¹\n",
    "            parent_id: çˆ¶èŠ‚ç‚¹ID (å¦‚æœä¸ºNone,ä½¿ç”¨node.node.node_id)\n",
    "            \n",
    "        Returns:\n",
    "            å­èŠ‚ç‚¹åˆ—è¡¨,æ¯ä¸ªå­èŠ‚ç‚¹éƒ½ä¿ç•™çˆ¶èŠ‚ç‚¹å¼•ç”¨\n",
    "        \"\"\"\n",
    "        text = node.node.text\n",
    "        text_length = len(text)\n",
    "        \n",
    "        # å¦‚æœæ–‡æœ¬é•¿åº¦å°äºchunk_size,ç›´æ¥è¿”å›åŸèŠ‚ç‚¹\n",
    "        if text_length <= self.chunk_size:\n",
    "            # æ·»åŠ çˆ¶èŠ‚ç‚¹IDåˆ°metadata\n",
    "            node.node.metadata['parent_node_id'] = parent_id or node.node.node_id\n",
    "            node.node.metadata['is_child_node'] = False\n",
    "            return [node]\n",
    "        \n",
    "        parent_node_id = parent_id or node.node.node_id\n",
    "        child_nodes = []\n",
    "        start = 0\n",
    "        chunk_index = 0\n",
    "        \n",
    "        while start < text_length:\n",
    "            end = min(start + self.chunk_size, text_length)\n",
    "            chunk_text = text[start:end]\n",
    "            \n",
    "            # åˆ›å»ºå­èŠ‚ç‚¹\n",
    "            child_node = TextNode(\n",
    "                text=chunk_text,\n",
    "                metadata={\n",
    "                    **node.node.metadata,  # ç»§æ‰¿çˆ¶èŠ‚ç‚¹çš„metadata\n",
    "                    'parent_node_id': parent_node_id,\n",
    "                    'chunk_index': chunk_index,\n",
    "                    'is_child_node': True,\n",
    "                    'parent_text_length': text_length,\n",
    "                    'chunk_start': start,\n",
    "                    'chunk_end': end\n",
    "                },\n",
    "                excluded_embed_metadata_keys=node.node.excluded_embed_metadata_keys,\n",
    "                excluded_llm_metadata_keys=node.node.excluded_llm_metadata_keys,\n",
    "            )\n",
    "            \n",
    "            # ä¿æŒåŸå§‹è¯„åˆ†\n",
    "            child_node_with_score = NodeWithScore(\n",
    "                node=child_node,\n",
    "                score=node.score\n",
    "            )\n",
    "            \n",
    "            child_nodes.append(child_node_with_score)\n",
    "            \n",
    "            # è®¡ç®—ä¸‹ä¸€ä¸ªèµ·ç‚¹ (å¸¦é‡å )\n",
    "            start += (self.chunk_size - self.overlap_size)\n",
    "            chunk_index += 1\n",
    "        \n",
    "        return child_nodes\n",
    "    \n",
    "    def split_nodes(self, nodes: List[NodeWithScore]) -> tuple[List[NodeWithScore], Dict[str, NodeWithScore]]:\n",
    "        \"\"\"\n",
    "        æ‰¹é‡åˆ†å‰²èŠ‚ç‚¹\n",
    "        \n",
    "        Returns:\n",
    "            (å­èŠ‚ç‚¹åˆ—è¡¨, çˆ¶èŠ‚ç‚¹æ˜ å°„å­—å…¸)\n",
    "        \"\"\"\n",
    "        all_child_nodes = []\n",
    "        parent_node_map = {}  # parent_node_id -> åŸå§‹çˆ¶èŠ‚ç‚¹\n",
    "        \n",
    "        for node in nodes:\n",
    "            parent_id = node.node.node_id\n",
    "            parent_node_map[parent_id] = node  # ä¿å­˜åŸå§‹çˆ¶èŠ‚ç‚¹\n",
    "            \n",
    "            child_nodes = self.split_node(node, parent_id)\n",
    "            all_child_nodes.extend(child_nodes)\n",
    "        \n",
    "        return all_child_nodes, parent_node_map\n",
    "\n",
    "\n",
    "# ==================== 2. å­èŠ‚ç‚¹åˆ°çˆ¶èŠ‚ç‚¹çš„åå¤„ç†å™¨ ====================\n",
    "class ChildToParentPostprocessor(BaseNodePostprocessor):\n",
    "    \"\"\"\n",
    "    å°†rerankåçš„å­èŠ‚ç‚¹è¿˜åŸä¸ºçˆ¶èŠ‚ç‚¹\n",
    "    ç­–ç•¥: å¦‚æœå¤šä¸ªå­èŠ‚ç‚¹æ¥è‡ªåŒä¸€çˆ¶èŠ‚ç‚¹,å–æœ€é«˜åˆ†çš„å­èŠ‚ç‚¹åˆ†æ•°ä½œä¸ºçˆ¶èŠ‚ç‚¹åˆ†æ•°\n",
    "    \"\"\"\n",
    "    \n",
    "    # ä½¿ç”¨ Pydantic çš„æ–¹å¼å£°æ˜å­—æ®µ\n",
    "    parent_node_map: Dict[str, Any] = {}\n",
    "    keep_top_k: int = 5\n",
    "    \n",
    "    def __init__(self, parent_node_map: Dict[str, NodeWithScore], keep_top_k: int = 5, **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            parent_node_map: çˆ¶èŠ‚ç‚¹IDåˆ°çˆ¶èŠ‚ç‚¹çš„æ˜ å°„\n",
    "            keep_top_k: æœ€ç»ˆä¿ç•™çš„çˆ¶èŠ‚ç‚¹æ•°é‡\n",
    "        \"\"\"\n",
    "        # ä½¿ç”¨ Pydantic çš„åˆå§‹åŒ–æ–¹å¼\n",
    "        super().__init__(\n",
    "            parent_node_map=parent_node_map,\n",
    "            keep_top_k=keep_top_k,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    def _postprocess_nodes(\n",
    "        self, \n",
    "        nodes: List[NodeWithScore], \n",
    "        query_bundle: Optional[QueryBundle] = None\n",
    "    ) -> List[NodeWithScore]:\n",
    "        \"\"\"\n",
    "        å°†å­èŠ‚ç‚¹è¿˜åŸä¸ºçˆ¶èŠ‚ç‚¹\n",
    "        \"\"\"\n",
    "        # æŒ‰çˆ¶èŠ‚ç‚¹IDåˆ†ç»„,è®°å½•æ¯ä¸ªçˆ¶èŠ‚ç‚¹çš„æœ€é«˜åˆ†æ•°\n",
    "        parent_scores: Dict[str, float] = {}\n",
    "        parent_child_nodes: Dict[str, List[NodeWithScore]] = {}\n",
    "        \n",
    "        for node in nodes:\n",
    "            parent_id = node.node.metadata.get('parent_node_id')\n",
    "            \n",
    "            if not parent_id:\n",
    "                # å¦‚æœæ²¡æœ‰çˆ¶èŠ‚ç‚¹ID,è¯´æ˜æ˜¯åŸå§‹èŠ‚ç‚¹,ç›´æ¥ä¿ç•™\n",
    "                parent_scores[node.node.node_id] = node.score\n",
    "                parent_child_nodes[node.node.node_id] = [node]\n",
    "                continue\n",
    "            \n",
    "            # è®°å½•æœ€é«˜åˆ†æ•°\n",
    "            if parent_id not in parent_scores:\n",
    "                parent_scores[parent_id] = node.score\n",
    "                parent_child_nodes[parent_id] = [node]\n",
    "            else:\n",
    "                # å–æœ€é«˜åˆ†\n",
    "                parent_scores[parent_id] = max(parent_scores[parent_id], node.score)\n",
    "                parent_child_nodes[parent_id].append(node)\n",
    "        \n",
    "        # æ„å»ºçˆ¶èŠ‚ç‚¹åˆ—è¡¨\n",
    "        parent_nodes = []\n",
    "        for parent_id, score in parent_scores.items():\n",
    "            if parent_id in self.parent_node_map:\n",
    "                # ä½¿ç”¨ä¿å­˜çš„åŸå§‹çˆ¶èŠ‚ç‚¹\n",
    "                parent_node = copy.deepcopy(self.parent_node_map[parent_id])\n",
    "                parent_node.score = score\n",
    "                \n",
    "                # å¯é€‰: åœ¨metadataä¸­è®°å½•åŒ¹é…çš„å­èŠ‚ç‚¹ä¿¡æ¯\n",
    "                child_info = [\n",
    "                    {\n",
    "                        'chunk_index': n.node.metadata.get('chunk_index'),\n",
    "                        'score': n.score,\n",
    "                        'text_preview': n.node.text[:100]\n",
    "                    }\n",
    "                    for n in parent_child_nodes[parent_id]\n",
    "                ]\n",
    "                parent_node.node.metadata['matched_children'] = child_info\n",
    "                \n",
    "                parent_nodes.append(parent_node)\n",
    "            else:\n",
    "                # å¦‚æœæ‰¾ä¸åˆ°çˆ¶èŠ‚ç‚¹,ä½¿ç”¨ç¬¬ä¸€ä¸ªå­èŠ‚ç‚¹(ä¸åº”è¯¥å‘ç”Ÿ)\n",
    "                print(f\"è­¦å‘Š: æ‰¾ä¸åˆ°çˆ¶èŠ‚ç‚¹ {parent_id}, ä½¿ç”¨å­èŠ‚ç‚¹ä»£æ›¿\")\n",
    "                parent_nodes.append(parent_child_nodes[parent_id][0])\n",
    "        \n",
    "        # æŒ‰åˆ†æ•°æ’åºå¹¶è¿”å›top_k\n",
    "        parent_nodes.sort(key=lambda x: x.score, reverse=True)\n",
    "        return parent_nodes[:self.keep_top_k]\n",
    "    \n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True  # å…è®¸ä»»æ„ç±»å‹\n",
    "\n",
    "# ==================== 3. è‡ªå®šä¹‰æ£€ç´¢å™¨åŒ…è£…å™¨ ====================\n",
    "class SplitNodeRetriever(BaseRetriever):\n",
    "    \"\"\"\n",
    "    åŒ…è£…åŸå§‹æ£€ç´¢å™¨,è‡ªåŠ¨å¤„ç†èŠ‚ç‚¹åˆ†å‰²\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        base_retriever: BaseRetriever,\n",
    "        chunk_size: int = 512,\n",
    "        overlap_ratio: float = 0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_retriever: åŸå§‹æ··åˆæ£€ç´¢å™¨\n",
    "            chunk_size: å­èŠ‚ç‚¹å¤§å°\n",
    "            overlap_ratio: é‡å æ¯”ä¾‹\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.base_retriever = base_retriever\n",
    "        self.node_splitter = NodeSplitter(chunk_size, overlap_ratio)\n",
    "        self.parent_node_map = {}\n",
    "    \n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"\n",
    "        æ£€ç´¢å¹¶åˆ†å‰²èŠ‚ç‚¹\n",
    "        \"\"\"\n",
    "        # 1. ä½¿ç”¨åŸå§‹æ£€ç´¢å™¨æ£€ç´¢\n",
    "        nodes = self.base_retriever.retrieve(query_bundle)\n",
    "        \n",
    "        # 2. åˆ†å‰²èŠ‚ç‚¹\n",
    "        child_nodes, self.parent_node_map = self.node_splitter.split_nodes(nodes)\n",
    "        \n",
    "        print(f\"åŸå§‹èŠ‚ç‚¹æ•°: {len(nodes)}, åˆ†å‰²åå­èŠ‚ç‚¹æ•°: {len(child_nodes)}\")\n",
    "        \n",
    "        return child_nodes\n",
    "    \n",
    "    def get_parent_node_map(self) -> Dict[str, NodeWithScore]:\n",
    "        \"\"\"è·å–çˆ¶èŠ‚ç‚¹æ˜ å°„,ä¾›åå¤„ç†å™¨ä½¿ç”¨\"\"\"\n",
    "        return self.parent_node_map\n",
    "\n",
    "\n",
    "def create_parent_postprocessor(retriever: SplitNodeRetriever, keep_top_k: int = 5):\n",
    "    \"\"\"åŠ¨æ€åˆ›å»ºçˆ¶èŠ‚ç‚¹åå¤„ç†å™¨\"\"\"\n",
    "    return ChildToParentPostprocessor(\n",
    "        parent_node_map=retriever.get_parent_node_map(),\n",
    "        keep_top_k=keep_top_k\n",
    "    )\n",
    "\n",
    "class DynamicQueryEngine:\n",
    "    \"\"\"æ”¯æŒåŠ¨æ€åå¤„ç†å™¨çš„æŸ¥è¯¢å¼•æ“\"\"\"\n",
    "    \n",
    "    def __init__(self, retriever, response_synthesizer, reranker, keep_top_k=5):\n",
    "        self.retriever = retriever\n",
    "        self.response_synthesizer = response_synthesizer\n",
    "        self.reranker = reranker\n",
    "        self.keep_top_k = keep_top_k\n",
    "    \n",
    "    def query(self, query_str: str):\n",
    "        from llama_index.core.schema import QueryBundle\n",
    "        \n",
    "        # 1. æ£€ç´¢ (è‡ªåŠ¨åˆ†å‰²èŠ‚ç‚¹)\n",
    "        query_bundle = QueryBundle(query_str=query_str)\n",
    "        nodes = self.retriever.retrieve(query_bundle)\n",
    "        \n",
    "        # 2. Rerankå­èŠ‚ç‚¹\n",
    "        reranked_nodes = self.reranker.postprocess_nodes(nodes, query_bundle)\n",
    "        \n",
    "        # 3. åŠ¨æ€åˆ›å»ºçˆ¶èŠ‚ç‚¹åå¤„ç†å™¨\n",
    "        parent_postprocessor = create_parent_postprocessor(\n",
    "            self.retriever, \n",
    "            keep_top_k=self.keep_top_k\n",
    "        )\n",
    "        \n",
    "        # 4. è¿˜åŸä¸ºçˆ¶èŠ‚ç‚¹\n",
    "        parent_nodes = parent_postprocessor.postprocess_nodes(reranked_nodes, query_bundle)\n",
    "        \n",
    "        # 5. ç”Ÿæˆå›ç­”\n",
    "        response = self.response_synthesizer.synthesize(\n",
    "            query=query_str,\n",
    "            nodes=parent_nodes\n",
    "        )\n",
    "        \n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "236213d2-b889-4a13-89ce-8d5939dce30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_retriever = SplitNodeRetriever(\n",
    "    base_retriever=hybrid_retriever,\n",
    "    chunk_size=512,      # åˆ†å‰²ä¸º512é•¿åº¦ (æˆ–256)\n",
    "    overlap_ratio=0.1    # 10%é‡å \n",
    ")\n",
    "\n",
    "dynamic_query_engine = DynamicQueryEngine(\n",
    "    retriever=split_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    reranker=reranker,\n",
    "    keep_top_k=5  # æœ€ç»ˆè¿”å›5ä¸ªçˆ¶èŠ‚ç‚¹\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4879465-574b-4ca0-b7e6-dcd6c1a67aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "1. æ ¹æ®ã€Šæ”¯ä»˜ç»“ç®—åŠæ³•ã€‹ç¬¬28æ¡å’Œç¬¬32æ¡ï¼Œå•ä½å¡ç”¨äºè´­ä¹°é«˜ä»·å€¼è‰ºæœ¯å“æ˜¯å¦æ„æˆå¤§é¢äº¤æ˜“æˆ–å¯ç–‘äº¤æ˜“ï¼Ÿæ˜¯å¦å­˜åœ¨æœªæŒ‰è§„å®šè¿›è¡Œäº¤æ˜“èƒŒæ™¯å®¡æŸ¥çš„æƒ…å½¢ï¼Ÿ\n",
      "2. ä¾æ®ã€Šé‡‘èæœºæ„å®¢æˆ·å°½èŒè°ƒæŸ¥å’Œå®¢æˆ·èº«ä»½èµ„æ–™åŠäº¤æ˜“è®°å½•ä¿å­˜ç®¡ç†åŠæ³•ã€‹ç¬¬12æ¡å’Œç¬¬17æ¡ï¼Œå¤–å›½æ”¿è¦å…³è”å®¢æˆ·å•ç¬”11ä¸‡å…ƒäººæ°‘å¸çš„éæ—¥å¸¸äº¤æ˜“æ˜¯å¦è§¦å‘å¼ºåŒ–å°½èŒè°ƒæŸ¥ä¹‰åŠ¡ï¼Ÿæ˜¯å¦ç¬¦åˆâ€œå¤§é¢äº¤æ˜“â€æ ‡å‡†ï¼Ÿ\n",
      "3. ç»“åˆã€Šæ”¯ä»˜ç»“ç®—åŠæ³•ã€‹ç¬¬15æ¡åŠã€Šå®¢æˆ·å°½èŒè°ƒæŸ¥ç®¡ç†åŠæ³•ã€‹ç¬¬14æ¡ï¼Œä½¿ç”¨å•ä½å¡è¿›è¡Œéç»è¥æ€§å¤§é¢æ¶ˆè´¹æ˜¯å¦è¿åå•ä½å¡ä½¿ç”¨èŒƒå›´è§„å®šï¼Œä¸”æ˜¯å¦æ„æˆéœ€æŠ¥å‘Šçš„å¯ç–‘äº¤æ˜“ï¼Ÿ\n",
      "åŸå§‹èŠ‚ç‚¹æ•°: 2, åˆ†å‰²åå­èŠ‚ç‚¹æ•°: 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934675d13d10411b9555b7dc2b5c06eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ ¹æ®æä¾›çš„ã€Šæ”¯ä»˜ç»“ç®—åŠæ³•ã€‹å†…å®¹ï¼Œå¯¹ç¬¬ä¸€ç¬”äº¤æ˜“ï¼ˆä½¿ç”¨è…¾è®¯å‘è¡Œçš„å•ä½å¡è´­ä¹°ä¸€ä»¶ä»·å€¼11ä¸‡å…ƒäººæ°‘å¸çš„è‰ºæœ¯å“ï¼‰è¿›è¡Œåˆ†æå¦‚ä¸‹ï¼š\n",
      "\n",
      "1. **è¿åâ€œæ”¯ä»˜ç»“ç®—åŠæ³•â€ç¬¬-ç™¾å››åäºŒæ¡**ï¼š\n",
      "   - è¯¥æ¡æ¬¾æ˜ç¡®è§„å®šï¼šâ€œå•ä½å¡ä¸å¾—ç”¨äºï¼‘ï¼ä¸‡å…ƒä»¥ä¸Šçš„å•†å“äº¤æ˜“ã€åŠ³åŠ¡ä¾›åº”æ¬¾é¡¹çš„ç»“ç®—ã€‚â€\n",
      "   - æœ¬æ¬¡äº¤æ˜“é‡‘é¢ä¸º11ä¸‡å…ƒäººæ°‘å¸ï¼Œè¶…è¿‡10ä¸‡å…ƒé™é¢ï¼Œå› æ­¤**ç›´æ¥è¿å**è¯¥æ¡æ¬¾ã€‚\n",
      "\n",
      "2. **è§¦å‘â€œå®¢æˆ·å°½èŒè°ƒæŸ¥å’Œå®¢æˆ·èº«ä»½èµ„æ–™åŠäº¤æ˜“è®°å½•ä¿å­˜ç®¡ç†åŠæ³•â€çš„å¼ºåˆ¶æªæ–½é—¨æ§›**ï¼š\n",
      "   - æ–‡ä»¶ä¸­æœªæåŠã€Šå¤šå®¶å®¢æˆ·å°½èŒè°ƒæŸ¥å’Œå®¢æˆ·èº«ä»½èµ„æ–™åŠäº¤æ˜“è®°å½•ä¿å­˜ç®¡ç†åŠæ³•ã€‹çš„å…·ä½“å†…å®¹æˆ–ç›¸å…³æ¡æ¬¾ã€‚\n",
      "   - å› æ­¤ï¼Œ**æ— æ³•æ ¹æ®ç°æœ‰æ–‡ä»¶åˆ¤æ–­æ˜¯å¦è§¦å‘è¯¥åŠæ³•ä¸­çš„å¼ºåˆ¶æªæ–½é—¨æ§›**ã€‚\n",
      "\n",
      "ç»¼ä¸Šï¼Œä»…ä¾æ®ã€Šæ”¯ä»˜ç»“ç®—åŠæ³•ã€‹ï¼š\n",
      "- **å­˜åœ¨æ˜ç¡®è¿åè¡Œä¸º**ï¼šè¿åç¬¬-ç™¾å››åäºŒæ¡å…³äºå•ä½å¡ç»“ç®—é™é¢çš„è§„å®šã€‚\n",
      "- **æ— ä¾æ®æ”¯æŒè§¦å‘å®¢æˆ·å°½èŒè°ƒæŸ¥ç›¸å…³å¼ºåˆ¶æªæ–½**ã€‚\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "æ‚¨æ˜¯ä¸€å®¶å¤§å‹å•†ä¸šé“¶è¡Œçš„é¦–å¸­åˆè§„å®˜ã€‚æ‚¨çš„ä¸€ä½å®¢æˆ·æ˜¯è…¾è®¯çš„ä¸€ä½é«˜ç®¡ï¼Œç”±äºå…¶å®¶åº­å…³ç³»ï¼Œä»–ä¹Ÿè¢«åˆ—ä¸ºâ€œå¤–å›½æ”¿è¦â€ã€‚åœ¨è…¾è®¯2025å¹´ç¬¬äºŒå­£åº¦è´¢æŠ¥å‘å¸ƒåçš„ä¸€å‘¨å†…ï¼Œä»–é€šè¿‡è´µè¡Œè¿›è¡Œäº†ä»¥ä¸‹äº¤æ˜“ï¼š\n",
    "\n",
    "ä»–ä½¿ç”¨è…¾è®¯å‘è¡Œçš„å•ä½å¡è´­ä¹°äº†ä¸€ä»¶ä»·å€¼11ä¸‡å…ƒäººæ°‘å¸çš„è‰ºæœ¯å“ï¼Œæ‘†æ”¾åœ¨åŠå…¬å®¤ã€‚\n",
    "\n",
    "ä»–å°†8ä¸‡å…ƒäººæ°‘å¸ç°é‡‘å­˜å…¥ä¸ªäººè´¦æˆ·ï¼Œå¹¶æ³¨æ˜è¿™ç¬”èµ„é‡‘æ¥è‡ªä¸ªäººè‚¡æ¯ã€‚\n",
    "\n",
    "ä»–ä½œä¸ºä»˜æ¬¾äººç­¾ç½²äº†ä¸€å¼ é‡‘é¢ä¸º600ä¸‡å…ƒäººæ°‘å¸çš„å•†ä¸šæ‰¿å…‘æ±‡ç¥¨ï¼Œä»˜æ¬¾æœŸé™ä¸º90å¤©ã€‚è¯¥è‰æ¡ˆæ—¨åœ¨ä¸ºä¸€å®¶3Dæ‰“å°å…¬å¸æä¾›æ–°çš„èèµ„ï¼Œè¯¥å…¬å¸å°†ä½¿ç”¨è…¾è®¯çš„â€œæ··å…ƒ3Dæ¨¡å‹â€äººå·¥æ™ºèƒ½æœåŠ¡ï¼Œè¯¥æœåŠ¡åœ¨æœ€è¿‘çš„è´¢æŠ¥ä¸­è¢«é‡ç‚¹æåŠã€‚\n",
    "\n",
    "æ‚¨çš„ä»»åŠ¡ï¼š\n",
    "\n",
    "ä»…æ ¹æ®æä¾›çš„æ–‡ä»¶ï¼Œå›ç­”ä»¥ä¸‹é—®é¢˜ã€‚\n",
    "\n",
    "å¯¹äºè¿™ç¬¬ä¸€ç¬”äº¤æ˜“ï¼Œè¯·åˆ†åˆ«æ‰¾å‡ºä»»ä½•å¯èƒ½è¿åâ€œæ”¯ä»˜ç»“ç®—åŠæ³•â€çš„æƒ…å†µï¼Œæˆ–ä»»ä½•è§¦å‘â€œå¤šå®¶å®¢æˆ·å°½èŒè°ƒæŸ¥å’Œå®¢æˆ·èº«ä»½èµ„æ–™åŠäº¤æ˜“è®°å½•ä¿å­˜ç®¡ç†åŠæ³•â€å¼ºåˆ¶æªæ–½çš„ç‰¹å®šé—¨æ§›ã€‚è¯·å¼•ç”¨æ–‡ä»¶ä¸­çš„å…·ä½“æ¡æ¬¾ç¼–å·æ¥æ”¯æŒæ‚¨çš„å‘ç°ã€‚\"\"\"\n",
    "response = dynamic_query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad005fea-9d8a-48ce-aa8d-7bc4fbc79331",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

# -*- coding: utf-8 -*-
"""rag2025.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kwd-2SI7n-LiuL1mGF8c1yrlR7i8efMv
"""

!rm -r ./output_chunks/

"""# Task
Extract text content and outline information from PDF and DOCX files in the `./data` folder using `python-docx`, `PyMuPDF`, and `pdfplumber` libraries, and save the extracted information as text files in a new folder.

## Install necessary libraries

### Subtask:
Install `python-docx`, `PyMuPDF`, and `pdfplumber` using pip.

**Reasoning**:
Install the required libraries using pip.
"""

!pip install python-docx PyMuPDF pdfplumber lxml langchain
!pip install markdown beautifulsoup4 openpyxl

import re
import os
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Any
import fitz
import pdfplumber
from lxml import etree
import zipfile
import markdown
from bs4 import BeautifulSoup

def extract_outline_from_markdown(md_path):
    """
    解析 Markdown 文件，提取大綱信息和內容
    """
    try:
        with open(md_path, 'r', encoding='utf-8') as f:
            md_content = f.read()

        # 將 Markdown 轉換為 HTML
        html_content = markdown.markdown(md_content, extensions=['extra'])

        # 使用 BeautifulSoup 解析 HTML
        soup = BeautifulSoup(html_content, 'html.parser')

        extracted_content = "Outline and Text:\n"

        # 提取標題結構
        for heading in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
            level = int(heading.name[1])  # 獲取標題級別
            title = heading.get_text().strip()
            indent = "  " * (level - 1)
            extracted_content += f"{indent}Heading {level}: {title}\n"

        # 提取段落內容
        for element in soup.find_all(['p', 'ul', 'ol', 'blockquote']):
            if element.name == 'p':
                text = element.get_text().strip()
                if text:
                    extracted_content += f"Paragraph: {text}\n"
            elif element.name in ['ul', 'ol']:
                for li in element.find_all('li'):
                    text = li.get_text().strip()
                    if text:
                        extracted_content += f"Paragraph: • {text}\n"
            elif element.name == 'blockquote':
                text = element.get_text().strip()
                if text:
                    extracted_content += f"Paragraph: > {text}\n"

        # 提取代碼塊
        for code in soup.find_all('code'):
            text = code.get_text().strip()
            if text and len(text) > 10:  # 過濾太短的代碼片段
                extracted_content += f"Paragraph: ```{text}```\n"

        return extracted_content

    except Exception as e:
        raise Exception(f"Error parsing Markdown file: {e}")

from openpyxl import load_workbook

def extract_outline_from_excel(xlsx_path):
    """
    解析 Excel 文件，提取工作表和單元格內容
    """
    try:
        workbook = load_workbook(xlsx_path)
        extracted_content = "Outline and Text:\n"

        # 處理每個工作表
        for sheet_name in workbook.sheetnames:
            sheet = workbook[sheet_name]

            # 添加工作表作為一級標題
            extracted_content += f"Heading 1: sheet - {sheet_name}\n"

            # 提取單元格內容
            for row in sheet.iter_rows(values_only=True):
                row_data = []
                for cell in row:
                    if cell is not None:
                        row_data.append(str(cell))

                if row_data:
                    # 檢查是否可能是表頭
                    if is_table_header(row_data):
                        extracted_content += f"Heading 2: line\n"

                    # 添加行內容
                    row_text = " | ".join(row_data)
                    extracted_content += f"Paragraph: {row_text}\n"

        return extracted_content

    except Exception as e:
        raise Exception(f"Error parsing Excel file: {e}")

try:
    from langchain.text_splitter import RecursiveCharacterTextSplitter
except ImportError:
    print("Please install langchain: pip install langchain")
    raise

def extract_outline_from_pdf(pdf_path):
    """
    解析 PDF 文件，提取大綱信息並格式化為與 Word 一致的格式
    """
    try:
        # 使用 PyMuPDF 提取大綱
        doc = fitz.open(pdf_path)
        outline = doc.get_toc()

        # 使用 pdfplumber 提取文本內容
        with pdfplumber.open(pdf_path) as pdf:
            extracted_content = "Outline and Text:\n"

            # 如果有大綱，建立頁面到標題的映射
            page_to_headings = {}
            if outline:
                for item in outline:
                    level, title, page_num = item
                    if page_num not in page_to_headings:
                        page_to_headings[page_num] = []
                    page_to_headings[page_num].append((level, title))

            # 逐頁處理內容
            for page_num, page in enumerate(pdf.pages):
                current_page = page_num + 1

                # 如果當前頁有標題，先輸出標題
                if current_page in page_to_headings:
                    for level, title in page_to_headings[current_page]:
                        indent = "  " * (level - 1)
                        extracted_content += f"{indent}Heading {level}: {title}\n"

                # 提取頁面文本
                text = page.extract_text()
                if text:
                    # 將文本按段落分割
                    paragraphs = text.split('\n\n')
                    for paragraph in paragraphs:
                        # 清理段落文本
                        cleaned_paragraph = ' '.join(paragraph.split()).strip()
                        if cleaned_paragraph and len(cleaned_paragraph) > 10:  # 過濾掉太短的內容
                            # 檢查是否可能是標題（簡單的啟發式判斷）
                            if is_likely_heading(cleaned_paragraph):
                                # 嘗試推斷標題級別
                                level = infer_heading_level(cleaned_paragraph)
                                indent = "  " * (level - 1)
                                extracted_content += f"{indent}Heading {level}: {cleaned_paragraph}\n"
                            else:
                                extracted_content += f"Paragraph: {cleaned_paragraph}\n"

                # 提取表格
                tables = page.extract_tables()
                if tables:
                    for table_idx, table in enumerate(tables):
                        extracted_content += f"Table {table_idx + 1} (Page {current_page}):\n"
                        for row in table:
                            if row and any(cell for cell in row if cell):  # 確保行不為空
                                row_text = " | ".join(str(cell) if cell else "" for cell in row)
                                extracted_content += f"Paragraph: {row_text}\n"

        doc.close()
        return extracted_content

    except Exception as e:
        raise Exception(f"Error parsing PDF file: {e}")

def is_likely_heading(text):
    """
    簡單的啟發式判斷文本是否可能是標題
    """
    # 標題通常較短、首字母大寫、可能包含數字編號
    if len(text) > 100:  # 太長不太可能是標題
        return False

    # 檢查是否有編號格式（如 1.1, 第一章 等）
    heading_patterns = [
        r'^\d+\.?\d*\.?\d*\s+',  # 1. 1.1 1.1.1 格式
        r'^第[一二三四五六七八九十\d]+[章節条款部分]\s*',  # 第一章、第二節等
        r'^[一二三四五六七八九十]+[、\.]\s*',  # 一、二、三、格式
        r'^[A-Z][A-Z\s]*$',  # 全大寫
        r'^[A-Z][a-z\s]+$'   # 首字母大寫的短句
    ]

    for pattern in heading_patterns:
        if re.match(pattern, text):
            return True

    # 檢查是否全部大寫或首字母大寫且較短
    if text.isupper() or (text[0].isupper() and len(text) < 50):
        return True

    return False

def infer_heading_level(text):
    """
    推斷標題級別
    """
    # 根據編號格式推斷級別
    if re.match(r'^\d+\.\d+\.\d+', text):  # 1.1.1 格式
        return 3
    elif re.match(r'^\d+\.\d+', text):     # 1.1 格式
        return 2
    elif re.match(r'^\d+\.?', text):       # 1. 格式
        return 1
    elif re.match(r'^第[一二三四五六七八九十\d]+章', text):  # 第一章
        return 1
    elif re.match(r'^第[一二三四五六七八九十\d]+節', text):  # 第一節
        return 2
    elif re.match(r'^[一二三四五六七八九十]+[、\.]', text):  # 一、
        return 2
    elif text.isupper():  # 全大寫可能是高級標題
        return 1
    else:
        return 2  # 默認為二級標題

def optimize_chunks_after_splitting(original_chunks, min_chunk_size=500, max_overlap=200):
    """
    在现有分割器基础上进行后处理优化：
    1. 合并过短的分块（小于min_chunk_size）
    2. 添加智能重叠
    """
    if not original_chunks:
        return []

    # 首先将所有分块转换为统一格式
    processed_chunks = []
    for chunk in original_chunks:
        if isinstance(chunk, str):
            processed_chunks.append({
                'text': chunk,
                'original_chunk': chunk,
                'is_textchunk': False
            })
        elif hasattr(chunk, 'content'):
            processed_chunks.append({
                'text': chunk.content,
                'original_chunk': chunk,
                'is_textchunk': True,
                'metadata': chunk.metadata,
                'path_titles': chunk.path_titles,
                'chunk_id': chunk.chunk_id
            })
        else:
            processed_chunks.append({
                'text': str(chunk),
                'original_chunk': chunk,
                'is_textchunk': False
            })

    optimized_chunks = []
    pending_merge = []  # 待合并的短分块
    current_merged_length = 0

    for i, chunk_info in enumerate(processed_chunks):
        chunk_text = chunk_info['text']
        chunk_len = len(chunk_text)

        # 如果分块太短，先暂存等待合并
        if chunk_len < min_chunk_size:
            pending_merge.append(chunk_info)
            current_merged_length += chunk_len
        else:
            # 先处理待合并的分块
            if pending_merge:
                merged_chunk = merge_pending_chunks(pending_merge, optimized_chunks, min_chunk_size)
                if merged_chunk:
                    optimized_chunks.append(merged_chunk)
                pending_merge = []
                current_merged_length = 0

            # 添加当前正常分块
            optimized_chunks.append(chunk_info['original_chunk'])

    # 处理剩余的待合并分块
    if pending_merge:
        merged_chunk = merge_pending_chunks(pending_merge, optimized_chunks, min_chunk_size)
        if merged_chunk:
            optimized_chunks.append(merged_chunk)

    # 添加分块间重叠（只在真正需要时）
    if len(optimized_chunks) > 1:
        final_chunks = add_intelligent_overlap(optimized_chunks, max_overlap)
    else:
        final_chunks = optimized_chunks

    return final_chunks

def merge_pending_chunks(pending_chunks, existing_chunks, min_chunk_size):
    """合并过短的分块"""
    if not pending_chunks:
        return None

    # 计算合并后的总长度
    total_length = sum(len(chunk['text']) for chunk in pending_chunks)

    # 如果合并后仍然很短，尝试与上一个分块合并
    if total_length < min_chunk_size and existing_chunks:
        last_chunk = existing_chunks[-1]

        # 如果是TextChunk对象
        if hasattr(last_chunk, 'content'):
            # 不能直接修改，需要创建新的TextChunk
            merged_text = "\n".join([chunk['text'] for chunk in pending_chunks])
            new_content = last_chunk.content + "\n" + merged_text

            new_chunk = TextChunk(
                chunk_id=f"merged_{last_chunk.chunk_id}",
                content=new_content,
                path_titles=last_chunk.path_titles,
                source_node_index=last_chunk.source_node_index,
                chunk_index=last_chunk.chunk_index,
                metadata={
                    **last_chunk.metadata,
                    'merged_from': [last_chunk.chunk_id] + [chunk.get('chunk_id', 'unknown') for chunk in pending_chunks if chunk.get('chunk_id')],
                    'is_merged': True,
                    'original_text_length': last_chunk.metadata.get('original_text_length', 0) + total_length
                }
            )
            # 替换最后一个分块
            existing_chunks[-1] = new_chunk
            return None
        else:
            # 普通字符串，直接合并
            merged_text = "\n".join([chunk['text'] for chunk in pending_chunks])
            existing_chunks[-1] = last_chunk + "\n" + merged_text
            return None

    # 创建新的合并分块
    return create_merged_chunk(pending_chunks)

def create_merged_chunk(pending_chunks):
    """创建合并后的分块"""
    if not pending_chunks:
        return None

    merged_text = "\n".join([chunk['text'] for chunk in pending_chunks])

    # 如果是TextChunk对象，需要保持原有结构
    if pending_chunks[0]['is_textchunk']:
        first_chunk = pending_chunks[0]['original_chunk']

        # 提取标题部分（如果有）
        content_parts = first_chunk.content.split('\n\n', 1)
        if len(content_parts) > 1:
            title_part = content_parts[0]
            new_content = title_part + "\n\n" + merged_text
        else:
            new_content = merged_text

        return TextChunk(
            chunk_id=f"merged_{first_chunk.chunk_id}",
            content=new_content,
            path_titles=first_chunk.path_titles,
            source_node_index=first_chunk.source_node_index,
            chunk_index=first_chunk.chunk_index,
            metadata={
                **first_chunk.metadata,
                'merged_from': [chunk.get('chunk_id', 'unknown') for chunk in pending_chunks if chunk.get('chunk_id')],
                'is_merged': True,
                'original_text_length': sum(len(chunk['text']) for chunk in pending_chunks)
            }
        )
    else:
        return merged_text

def add_intelligent_overlap(chunks, max_overlap):
    """在分块之间添加智能重叠"""
    if len(chunks) <= 1:
        return chunks

    enhanced_chunks = [chunks[0]]  # 第一个分块保持不变

    for i in range(1, len(chunks)):
        current_chunk = chunks[i]
        previous_chunk = chunks[i-1]

        # 只有当前一个分块足够长时才添加重叠
        previous_text = get_chunk_text(previous_chunk)
        if len(previous_text) > max_overlap * 2:  # 只有当前一个分块足够长时才添加重叠
            overlap_text = extract_overlap_text(previous_chunk, max_overlap)

            if overlap_text and len(overlap_text) > 50:  # 确保重叠文本有足够长度
                if hasattr(current_chunk, 'content'):
                    # TextChunk对象
                    enhanced_chunk = TextChunk(
                        chunk_id=current_chunk.chunk_id + "_overlap",
                        content=overlap_text + "\n\n" + current_chunk.content,
                        path_titles=current_chunk.path_titles,
                        source_node_index=current_chunk.source_node_index,
                        chunk_index=current_chunk.chunk_index,
                        metadata={
                            **current_chunk.metadata,
                            'overlap_added': True,
                            'overlap_length': len(overlap_text)
                        }
                    )
                    enhanced_chunks.append(enhanced_chunk)
                else:
                    enhanced_chunks.append(overlap_text + "\n\n" + current_chunk)
                continue

        enhanced_chunks.append(current_chunk)

    return enhanced_chunks

def get_chunk_text(chunk):
    """获取分块的文本内容"""
    if isinstance(chunk, str):
        return chunk
    elif hasattr(chunk, 'content'):
        # 只获取正文部分（去掉标题）
        content = chunk.content
        if '\n\n' in content:
            return content.split('\n\n', 1)[1]
        return content
    else:
        return str(chunk)

def extract_overlap_text(chunk, max_overlap):
    """从分块末尾提取合适的重叠文本"""
    text = get_chunk_text(chunk)

    if len(text) <= max_overlap:
        return text

    # 从末尾开始查找合适的截断点
    punctuation_marks = ["。", "！", "？", "\n", ".", "!", "?", ";", "；", ":", "：", "，", ","]

    # 查找最近的标点符号（从max_overlap位置开始向前找）
    start_pos = max(0, len(text) - max_overlap - 100)  # 多找一些范围
    overlap_candidate = text[start_pos:]

    # 在候选文本中查找第一个标点
    for i, char in enumerate(overlap_candidate):
        if char in punctuation_marks:
            overlap_start = start_pos + i + 1  # 从标点后开始
            return text[overlap_start:]

    # 如果没有找到合适的标点，使用固定长度的重叠
    return text[-max_overlap:]

def merge_chunks_by_path(chunks: List[TextChunk], max_chunk_size: int = 4096, max_overlap: int = 400) -> List[TextChunk]:
    """
    根据路径合并相同路径的分块，并移除重复的路径标题
    """
    if not chunks:
        return []

    # 按路径分组
    chunks_by_path = {}
    for chunk in chunks:
        path_key = tuple(chunk.path_titles)  # 使用元组作为键
        if path_key not in chunks_by_path:
            chunks_by_path[path_key] = []
        chunks_by_path[path_key].append(chunk)

    merged_chunks = []

    # 对每个路径组进行合并
    for path_key, path_chunks in chunks_by_path.items():
        if len(path_chunks) == 1:
            # 单个分块，无需合并
            merged_chunks.append(path_chunks[0])
            continue

        # 按源节点索引和分块索引排序，确保顺序正确
        path_chunks.sort(key=lambda x: (x.source_node_index, x.chunk_index))

        current_merged_content = ""
        current_metadata = path_chunks[0].metadata.copy()
        chunks_to_merge = []

        for i, chunk in enumerate(path_chunks):
            # 移除分块内容中的路径标题部分（只保留正文）
            chunk_content = remove_path_title_from_content(chunk.content, chunk.path_titles)

            # 检查当前分块是否与已合并内容有重叠（避免重复）
            if current_merged_content and is_content_overlapping(current_merged_content, chunk_content):
                # 移除重叠部分
                chunk_content = remove_overlapping_content(current_merged_content, chunk_content)

            # 检查合并后是否超过最大长度
            if len(current_merged_content) + len(chunk_content) > max_chunk_size:
                # 创建合并后的分块（只在开头添加一次路径标题）
                if chunks_to_merge:
                    final_content = add_path_title_once(path_key, current_merged_content)
                    merged_chunk = create_merged_chunk_from_list(chunks_to_merge, final_content, current_metadata)
                    merged_chunks.append(merged_chunk)

                # 重置状态
                current_merged_content = chunk_content
                current_metadata = chunk.metadata.copy()
                chunks_to_merge = [chunk]
            else:
                # 添加分隔符（如果是第一个分块则不添加）
                if current_merged_content:
                    current_merged_content += "\n\n"
                current_merged_content += chunk_content
                chunks_to_merge.append(chunk)

                # 更新元数据
                current_metadata['original_text_length'] = current_metadata.get('original_text_length', 0) + chunk.metadata.get('chunk_text_length', 0)
                current_metadata['full_content_length'] = len(current_merged_content)

        # 处理最后一批分块
        if chunks_to_merge:
            final_content = add_path_title_once(path_key, current_merged_content)
            merged_chunk = create_merged_chunk_from_list(chunks_to_merge, final_content, current_metadata)
            merged_chunks.append(merged_chunk)

    # 在合并后的分块间添加智能重叠
    if len(merged_chunks) > 1:
        merged_chunks = add_intelligent_overlap_to_merged(merged_chunks, max_overlap)

    return merged_chunks

def is_content_overlapping(existing_content: str, new_content: str) -> bool:
    """检查新内容是否与现有内容有重叠"""
    if not existing_content or not new_content:
        return False

    # 检查新内容开头是否与现有内容结尾重复
    overlap_check_length = min(200, len(existing_content), len(new_content))
    if overlap_check_length == 0:
        return False

    existing_end = existing_content[-overlap_check_length:]
    new_start = new_content[:overlap_check_length]

    return existing_end == new_start

def remove_overlapping_content(existing_content: str, new_content: str) -> str:
    """移除新内容中与现有内容重叠的部分"""
    if not existing_content or not new_content:
        return new_content

    # 查找最大重叠长度
    max_overlap = min(200, len(existing_content), len(new_content))
    for overlap_length in range(max_overlap, 0, -1):
        if existing_content.endswith(new_content[:overlap_length]):
            return new_content[overlap_length:]

    return new_content


def remove_path_title_from_content(content: str, path_titles: List[str]) -> str:
    """
    从分块内容中移除路径标题部分，只保留正文
    """
    if not path_titles or not content:
        return content

    # 构建路径标题模式（可能出现在内容开头的各种形式）
    path_patterns = []

    # 1. 完整的路径标题格式：标题1 > 标题2 > 标题3
    full_path = " > ".join(path_titles)
    path_patterns.append(full_path + "\n\n")
    path_patterns.append(full_path + "\n")

    # 2. 单独的标题（逐级检查）
    for i in range(len(path_titles)):
        partial_path = " > ".join(path_titles[i:])
        path_patterns.append(partial_path + "\n\n")
        path_patterns.append(partial_path + "\n")

    # 3. 每个单独的标题
    for title in path_titles:
        path_patterns.append(title + "\n\n")
        path_patterns.append(title + "\n")

    # 移除内容开头的路径标题
    cleaned_content = content
    for pattern in path_patterns:
        if cleaned_content.startswith(pattern):
            cleaned_content = cleaned_content[len(pattern):]
            break

    # 移除内容中间可能出现的重复路径标题
    for pattern in path_patterns:
        # 避免过度移除，只在明显的分隔位置移除
        pattern_with_newlines = "\n\n" + pattern
        while pattern_with_newlines in cleaned_content:
            cleaned_content = cleaned_content.replace(pattern_with_newlines, "\n\n")

    return cleaned_content.strip()

def add_path_title_once(path_key: tuple, content: str) -> str:
    """
    只在合并后的内容开头添加一次路径标题
    """
    path_titles = list(path_key)
    if not path_titles:
        return content

    full_path = " > ".join(path_titles)
    return f"{full_path}\n\n{content}"

def create_merged_chunk_from_list(chunks: List[TextChunk], merged_content: str, metadata: Dict[str, Any]) -> TextChunk:
    """从多个分块创建合并后的分块"""
    if not chunks:
        return None

    first_chunk = chunks[0]
    last_chunk = chunks[-1]

    return TextChunk(
        chunk_id=f"path_merged_{first_chunk.chunk_id}_to_{last_chunk.chunk_id}",
        content=merged_content,
        path_titles=first_chunk.path_titles,
        source_node_index=first_chunk.source_node_index,
        chunk_index=first_chunk.chunk_index,
        metadata={
            **metadata,
            'merged_from': [chunk.chunk_id for chunk in chunks],
            'is_path_merged': True,
            'merged_chunk_count': len(chunks),
            'first_source_node': first_chunk.source_node_index,
            'last_source_node': last_chunk.source_node_index,
            'path_titles_preserved': True  # 标记路径标题已优化处理
        }
    )

def add_intelligent_overlap_to_merged(chunks: List[TextChunk], max_overlap: int) -> List[TextChunk]:
    """为合并后的分块添加智能重叠，避免重复内容"""
    if len(chunks) <= 1:
        return chunks

    enhanced_chunks = [chunks[0]]

    for i in range(1, len(chunks)):
        current_chunk = chunks[i]
        previous_chunk = chunks[i-1]

        # 提取前一个分块末尾的重叠文本（移除路径标题部分）
        previous_text = remove_path_title_from_content(previous_chunk.content, previous_chunk.path_titles)
        current_text = remove_path_title_from_content(current_chunk.content, current_chunk.path_titles)

        # 检查当前分块开头是否已经包含了前一个分块的内容（避免重复）
        if len(previous_text) > max_overlap and not is_content_already_included(previous_text, current_text, max_overlap):
            # 查找合适的截断点
            overlap_text = extract_overlap_from_text(previous_text, max_overlap)

            if overlap_text and len(overlap_text) > 50:
                # 确保重叠文本不会与当前分块开头重复
                if not current_text.startswith(overlap_text):
                    # 创建带有重叠的新分块
                    enhanced_chunk = TextChunk(
                        chunk_id=current_chunk.chunk_id + "_overlap",
                        content=overlap_text + "\n\n" + current_text,
                        path_titles=current_chunk.path_titles,
                        source_node_index=current_chunk.source_node_index,
                        chunk_index=current_chunk.chunk_index,
                        metadata={
                            **current_chunk.metadata,
                            'overlap_added': True,
                            'overlap_length': len(overlap_text),
                            'overlap_source': previous_chunk.chunk_id
                        }
                    )
                    enhanced_chunks.append(enhanced_chunk)
                    continue

        enhanced_chunks.append(current_chunk)

    return enhanced_chunks

def is_content_already_included(previous_text: str, current_text: str, max_overlap: int) -> bool:
    """
    检查当前分块是否已经包含了前一个分块的内容
    避免添加重复的重叠
    """
    if not previous_text or not current_text:
        return False

    # 检查当前分块开头是否与前一个分块结尾有重复
    previous_end = previous_text[-min(max_overlap * 2, len(previous_text)):]  # 检查前一个分块的结尾部分
    current_start = current_text[:min(max_overlap * 2, len(current_text))]    # 检查当前分块的开头部分

    # 如果当前分块开头已经包含了前一个分块结尾的内容，就不需要再添加重叠
    for overlap_length in range(50, min(len(previous_end), len(current_start)) + 1):
        if previous_end[-overlap_length:] == current_start[:overlap_length]:
            return True

    return False

def extract_overlap_from_text(text: str, max_overlap: int) -> str:
    """从文本末尾提取重叠内容，确保不会重复"""
    if len(text) <= max_overlap:
        return text

    # 从末尾开始查找合适的截断点
    punctuation_marks = ["。", "！", "？", "\n", ".", "!", "?", ";", "；", ":", "：", "，", ","]

    # 在最后max_overlap + 200字符范围内查找标点（扩大搜索范围）
    search_start = max(0, len(text) - max_overlap - 200)
    search_text = text[search_start:]

    # 优先查找段落分隔符
    if "\n\n" in search_text:
        last_double_newline = search_text.rfind("\n\n")
        if last_double_newline != -1:
            overlap_start = search_start + last_double_newline + 2
            return text[overlap_start:]

    # 其次查找句子结束标点
    for i in range(len(search_text) - 1, -1, -1):
        if search_text[i] in punctuation_marks:
            overlap_start = search_start + i + 1
            # 确保重叠文本有足够长度
            if len(text) - overlap_start >= 50:
                return text[overlap_start:]

    # 如果没有找到合适的标点，使用固定长度的重叠（但确保不会太短）
    return text[-min(max_overlap, len(text)):]

@dataclass
class DocumentNode:
    """文檔樹節點"""
    text: str = ""  # 段落文本
    level: int = 0  # 段落級別（數字越小級別越高）
    parent_index: Optional[int] = None  # 父節點索引
    children_indices: List[int] = field(default_factory=list)  # 子節點索引列表
    node_type: str = "paragraph"  # 節點類型：root, heading, paragraph
    tables: List[Dict[str, Any]] = field(default_factory=list)  # 該節點關聯的表格

@dataclass
class TextChunk:
    """文本分块"""
    chunk_id: str  # 分块ID
    content: str  # 分块内容
    path_titles: List[str]  # 回溯路径上的标题
    source_node_index: int  # 源叶子节点索引
    chunk_index: int  # 在该叶子节点中的分块索引
    metadata: Dict[str, Any] = field(default_factory=dict)  # 额外元数据

class DocumentTree:
    """文檔樹"""
    def __init__(self):
        self.nodes: List[DocumentNode] = []
        self.current_index: int = 0
        self._init_root()

    def _init_root(self):
        """初始化根節點"""
        root = DocumentNode(
            text="ROOT",
            level=0,  # 根節點級別為0
            parent_index=None,
            node_type="root"
        )
        self.nodes.append(root)
        self.current_index = 0

    def add_paragraph(self, text: str, level: int, node_type: str = "paragraph"):
        """添加段落到文檔樹"""
        # print(f"Adding: {node_type} level={level}, text='{text[:30]}...', current_index={self.current_index}, current_level={self.nodes[self.current_index].level}")

        if node_type == "heading":
            # 處理標題：找到合適的父節點
            parent_index = self._find_parent_for_heading(level)

            # 創建新的標題節點
            new_node = DocumentNode(
                text=text,
                level=level,
                parent_index=parent_index,
                node_type=node_type
            )

            new_index = len(self.nodes)
            self.nodes.append(new_node)

            # 更新父節點的子節點列表
            self.nodes[parent_index].children_indices.append(new_index)

            # 更新當前節點為新創建的標題節點
            self.current_index = new_index

            # print(f"  Created heading node {new_index}, parent={parent_index}")

        else:  # paragraph
            # 處理段落：添加到當前節點（通常是最近的標題）
            if self.nodes[self.current_index].text and self.nodes[self.current_index].node_type == "paragraph":
                # 如果當前節點已經是段落且有內容，合併文本
                self.nodes[self.current_index].text += "\n" + text
            elif self.nodes[self.current_index].node_type == "heading":
                # 如果當前節點是標題，創建新的段落節點作為其子節點
                new_node = DocumentNode(
                    text=text,
                    level=999,  # 段落級別設為最大
                    parent_index=self.current_index,
                    node_type="paragraph"
                )

                new_index = len(self.nodes)
                self.nodes.append(new_node)

                # 更新父節點的子節點列表
                self.nodes[self.current_index].children_indices.append(new_index)

                # print(f"  Created paragraph node {new_index} under heading {self.current_index}")
            else:
                # 其他情況，更新當前節點的文本
                if self.nodes[self.current_index].text:
                    self.nodes[self.current_index].text += "\n" + text
                else:
                    self.nodes[self.current_index].text = text

    def _find_parent_for_heading(self, level: int) -> int:
        """為標題找到合適的父節點"""
        current_idx = self.current_index

        # 從當前節點開始向上查找
        while current_idx is not None:
            current_node = self.nodes[current_idx]

            # 如果當前節點是根節點，直接返回
            if current_node.node_type == "root":
                return current_idx

            # 如果當前節點是標題且級別比新標題高（數字更小），則作為父節點
            if current_node.node_type == "heading" and current_node.level < level:
                return current_idx

            # 繼續向上查找
            current_idx = current_node.parent_index

        # 如果沒找到合適的父節點，返回根節點
        return 0

    def add_table_to_current_node(self, table_data: Dict[str, Any]):
        """將表格添加到當前節點"""
        self.nodes[self.current_index].tables.append(table_data)


    def get_leaf_nodes(self) -> List[int]:
        """獲取所有葉子節點的索引"""
        leaf_nodes = []
        for i, node in enumerate(self.nodes):
            if not node.children_indices and node.node_type != "root":
                leaf_nodes.append(i)
        return leaf_nodes


    def get_path_to_root(self, node_index: int) -> List[int]:
        """獲取從指定節點到根節點的路徑"""
        path = []
        current_idx = node_index

        while current_idx is not None:
            path.append(current_idx)
            current_idx = self.nodes[current_idx].parent_index

        return path  # 从叶子节点到根节点的路径


    def get_path_titles(self, node_index: int) -> List[str]:
        """獲取從根節點到指定節點路徑上的所有標題"""
        path = self.get_path_to_root(node_index)
        path.reverse()  # 反转，从根节点到叶子节点

        titles = []
        for idx in path:
            node = self.nodes[idx]
            if node.node_type == "heading" and node.text.strip():
                titles.append(node.text.strip())

        return titles

    def create_chunks_for_leaf_node(self, leaf_node_index: int,
                              max_chunk_size: int = 4096,
                              min_overlap: int = 200,
                              max_overlap: int = 400) -> List[TextChunk]:
      """为叶子节点创建文本分块（使用后处理优化）"""
      leaf_node = self.nodes[leaf_node_index]

      if not leaf_node.text or not leaf_node.text.strip():
          return []

      # 获取路径上的标题
      path_titles = self.get_path_titles(leaf_node_index)

      # 使用原有的分割器
      text_splitter = RecursiveCharacterTextSplitter(
          chunk_size=max_chunk_size,
          chunk_overlap=min_overlap,
          length_function=len,
          separators=["\n\n", "\n", "。", "！", "？", "；", ".", "!", "?", ";", " ", ""]
      )

      # 分割文本
      raw_chunks = text_splitter.split_text(leaf_node.text)

      # 后处理优化
      optimized_chunks = optimize_chunks_after_splitting(
          raw_chunks,
          min_chunk_size=1000,  # 小于100字符的分块需要合并
          max_overlap=max_overlap      # 最大重叠长度
      )

      # 创建TextChunk对象
      chunks = []
      for i, chunk_text in enumerate(optimized_chunks):
          if hasattr(chunk_text, 'content'):
              # 已经是优化后的TextChunk对象
              chunks.append(chunk_text)
          else:
              # 构建完整内容
              if path_titles:
                  title_text = " > ".join(path_titles)
                  full_content = f"{title_text}\n\n{chunk_text}"
              else:
                  full_content = chunk_text

              chunk_id = f"node_{leaf_node_index}_chunk_{i}"

              chunk = TextChunk(
                  chunk_id=chunk_id,
                  content=full_content,
                  path_titles=path_titles,
                  source_node_index=leaf_node_index,
                  chunk_index=i,
                  metadata={
                      'original_text_length': len(leaf_node.text),
                      'chunk_text_length': len(chunk_text),
                      'full_content_length': len(full_content),
                      'node_type': leaf_node.node_type,
                      'has_tables': len(leaf_node.tables) > 0,
                      'table_count': len(leaf_node.tables),
                      'is_root_content': leaf_node_index == 0
                  }
              )
              chunks.append(chunk)

      return chunks





    def create_all_chunks(self, max_chunk_size: int = 4096,
                        min_overlap: int = 100,
                        max_overlap: int = 400) -> List[TextChunk]:
        """為所有葉子節點創建文本分塊"""
        all_chunks = []
        leaf_nodes = self.get_leaf_nodes()

        print(f"Found {len(leaf_nodes)} leaf nodes")

        # 如果沒有葉子節點，檢查所有可能包含內容的節點
        if not leaf_nodes:
            print("No leaf nodes found, checking all nodes for content...")

            content_nodes = []
            for i, node in enumerate(self.nodes):
                # 檢查節點是否有足夠長的文本內容
                if node.text and node.text.strip() and len(node.text.strip()) > 10:
                    content_nodes.append(i)
                    print(f"  Node {i}: {len(node.text)} chars - {node.text[:50]}...")

            print(f"Found {len(content_nodes)} content nodes")

            # 特別處理根節點（索引0）
            if 0 in content_nodes and len(content_nodes) == 1:
                print("Only root node has content, treating as single content document")
                chunks = self.create_chunks_for_leaf_node(
                    0, max_chunk_size, min_overlap, max_overlap
                )
                all_chunks.extend(chunks)
                print(f"  Created {len(chunks)} chunks from root node")
            else:
                # 處理其他內容節點
                for node_idx in content_nodes:
                    if node_idx == 0:  # 跳過根節點，除非它是唯一的內容節點
                        continue
                    print(f"Processing content node {node_idx}: {self.nodes[node_idx].text[:50]}...")
                    chunks = self.create_chunks_for_leaf_node(
                        node_idx, max_chunk_size, min_overlap, max_overlap
                    )
                    all_chunks.extend(chunks)
                    print(f"  Created {len(chunks)} chunks")

        else:
            # 正常處理葉子節點
            for leaf_idx in leaf_nodes:
                print(f"Processing leaf node {leaf_idx}: {self.nodes[leaf_idx].text[:50]}...")
                chunks = self.create_chunks_for_leaf_node(
                    leaf_idx, max_chunk_size, min_overlap, max_overlap
                )
                all_chunks.extend(chunks)
                print(f"  Created {len(chunks)} chunks")

        return all_chunks






    def get_tree_structure(self, node_index: int = 0, indent: int = 0) -> str:
        """獲取樹結構的字符串表示"""
        if node_index >= len(self.nodes):
            return ""

        node = self.nodes[node_index]

        if node.node_type == "root":
            result = f"ROOT (children: {len(node.children_indices)})\n"
        else:
            result = "  " * indent + f"[{node.node_type}] Level {node.level}: {node.text[:50]}"
            if len(node.text) > 50:
                result += "..."
            result += f" (children: {len(node.children_indices)})\n"

        if node.tables:
            result += "  " * (indent + 1) + f"Tables: {len(node.tables)}\n"

        for child_index in node.children_indices:
            result += self.get_tree_structure(child_index, indent + 1)

        return result

def parse_extracted_text(text_content: str) -> DocumentTree:
    """解析提取的文本內容，構建文檔樹"""
    tree = DocumentTree()
    lines = text_content.split('\n')


    # 檢查是否有任何標題結構
    has_headings = any(line.strip().startswith('Heading') for line in lines)


    # 如果完全沒有標題結構，將所有內容作為單一段落處理
    if not has_headings:
        print("No headings found, treating as single paragraph document")
        all_text = '\n'.join([line for line in lines if line.strip() and not line.startswith('Outline')])

        # 直接添加到根節點
        if tree.nodes[0].text:
            tree.nodes[0].text += "\n" + all_text
        else:
            tree.nodes[0].text = all_text

        return tree


    current_tables = []  # 暫存表格數據

    for line in lines:
        line = line.strip()
        if not line:
            continue

        # print(f"Processing line: {line}")

        # 解析標題
        heading_match = re.match(r'^(\s*)Heading (\d+): (.+)$', line)
        if heading_match:
            indent, level_str, title = heading_match.groups()
            level = int(level_str)

            # 如果有暫存的表格，添加到上一個節點
            if current_tables:
                for table in current_tables:
                    tree.add_table_to_current_node(table)
                current_tables = []

            tree.add_paragraph(title, level, "heading")
            continue

        # 解析段落
        paragraph_match = re.match(r'^Paragraph: (.+)$', line)
        if paragraph_match:
            content = paragraph_match.group(1)

            # 檢查是否是表格行
            if '|' in content and content.count('|') >= 2:
                # 這是表格數據，暫存起來
                table_row = [cell.strip() for cell in content.split('|')[1:-1]]

                # 如果是新表格的開始
                if not current_tables:
                    current_tables.append({
                        'rows': [table_row],
                        'has_header': is_table_header(table_row),
                        'position': 'after_current_heading'
                    })
                else:
                    # 添加到當前表格
                    current_tables[-1]['rows'].append(table_row)
            else:
                # 如果有暫存的表格，先添加到當前節點
                if current_tables:
                    for table in current_tables:
                        tree.add_table_to_current_node(table)
                    current_tables = []

                # 正文段落
                tree.add_paragraph(content, 999, "paragraph")
            continue

        # 解析表格標題
        table_match = re.match(r'^Table \d+ \(Page \d+\):$', line)
        if table_match:
            # 表格開始標記，準備收集表格數據
            continue

    # 處理剩餘的表格
    if current_tables:
        for table in current_tables:
            tree.add_table_to_current_node(table)

    return tree

def is_table_header(row: List[str]) -> bool:
    """簡單判斷表格行是否為表頭（基於文本特徵）"""
    if not row:
        return False

    # 檢查是否包含常見的表頭關鍵詞
    header_keywords = [
        # 基本信息類
        '名称', '姓名', '编号', '序号', '代码', '标识', '标号', 'ID', 'Code', 'Name',

        # 分類描述類
        '类型', '分类', '种类', '类别', '性质', '属性', '特征', 'Type', 'Category',
        '项目', '内容', '描述', '说明', '备注', '注释', '详情', 'Item', 'Description',

        # 數量金額類
        '数量', '金额', '价格', '费用', '成本', '总计', '小计', '合计',
        'Amount', 'Price', 'Cost', 'Total', 'Sum', 'Count', 'Quantity',

        # 時間日期類
        '日期', '时间', '年份', '月份', '期间', '开始', '结束', '截止',
        'Date', 'Time', 'Year', 'Month', 'Period', 'Start', 'End',

        # 狀態結果類
        '状态', '结果', '等级', '级别', '评分', '得分', '排名', '排序',
        'Status', 'Result', 'Level', 'Grade', 'Score', 'Rank',

        # 地址聯系類
        '地址', '位置', '区域', '部门', '单位', '公司', '机构', '组织',
        'Address', 'Location', 'Department', 'Company', 'Organization',
        '电话', '邮箱', '联系', '手机', 'Phone', 'Email', 'Contact',

        # 人員相關類
        '负责人', '联系人', '经办人', '申请人', '审批人', '操作员',
        'Manager', 'Contact', 'Operator', 'Applicant', 'Approver',

        # 業務流程類
        '流程', '步骤', '阶段', '环节', '操作', '处理', '审核', '审批',
        'Process', 'Step', 'Stage', 'Operation', 'Review', 'Approval',

        # 技術參數類
        '参数', '配置', '规格', '型号', '版本', '尺寸', '重量', '容量',
        'Parameter', 'Config', 'Specification', 'Model', 'Version', 'Size',

        # 統計分析類
        '比例', '百分比', '占比', '增长率', '完成率', '达成率', '通过率','准确率'
        'Ratio', 'Percentage', 'Rate', 'Growth', 'Completion','accuracy'

        # 質量安全類
        '质量', '安全', '风险', '问题', '缺陷', '异常', '故障',
        'Quality', 'Safety', 'Risk', 'Issue', 'Defect', 'Exception',

        # 計劃目標類
        '计划', '目标', '任务', '指标', '要求', '标准', '规范',
        'Plan', 'Target', 'Task', 'Indicator', 'Requirement', 'Standard',

        # 資源材料類
        '资源', '材料', '设备', '工具', '软件', '系统', '平台',
        'Resource', 'Material', 'Equipment', 'Tool', 'Software', 'System',

        # 權限角色類
        '权限', '角色', '职位', '岗位', '职责', '权利', '义务',
        'Permission', 'Role', 'Position', 'Responsibility', 'Authority'
    ]

    header_score = 0
    total_cells = len([cell for cell in row if cell and cell.strip()])

    if total_cells == 0:
        return False

    for cell in row:
        if cell and cell.strip():
            cell_text = cell.strip()

            # 直接匹配關鍵詞
            for keyword in header_keywords:
                if keyword in cell_text:
                    header_score += 1
                    break
            else:
                # 檢查是否是簡短的描述性文字（可能是表頭）
                if len(cell_text) <= 10 and not cell_text.isdigit():
                    # 檢查是否包含中文字符或英文字母
                    if any('\u4e00' <= char <= '\u9fff' for char in cell_text) or \
                       any(char.isalpha() for char in cell_text):
                        header_score += 0.5

    # 如果超過60%的單元格符合表頭特徵，認為是表頭
    return header_score / total_cells > 0.6

def extract_outline_from_docx_with_tables(docx_path):
    """
    增強版 docx 解析，包含表格處理
    """
    try:
        with zipfile.ZipFile(docx_path, 'r') as docx_zip:
            document_xml = docx_zip.read('word/document.xml')

            # 嘗試讀取樣式文件，但有些文檔可能沒有
            styles_xml = None
            try:
                styles_xml = docx_zip.read('word/styles.xml')
            except KeyError:
                print(f"Warning: styles.xml not found in {docx_path}")
                pass

            doc_tree = etree.fromstring(document_xml)
            styles_tree = etree.fromstring(styles_xml) if styles_xml else None

            namespaces = {
                'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'
            }

            # 獲取樣式映射
            style_outline_map = {}
            if styles_tree is not None:
                styles = styles_tree.xpath('//w:style', namespaces=namespaces)
                for style in styles:
                    style_id = style.get('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}styleId')
                    outline_lvl = style.xpath('.//w:outlineLvl', namespaces=namespaces)
                    if outline_lvl:
                        level = int(outline_lvl[0].get('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}val'))
                        style_outline_map[style_id] = level + 1
                    # 額外檢查：如果樣式名包含Heading但沒有outlineLvl，推斷級別
                    elif style_id and style_id.startswith('Heading'):
                        try:
                            level = int(re.findall(r'\d+', style_id)[0])
                            style_outline_map[style_id] = level
                        except (ValueError, IndexError):
                            pass

            extracted_content = "Outline and Text:\n"

            # 獲取所有段落和表格，按文檔順序處理
            body_elements = doc_tree.xpath('//w:body/*', namespaces=namespaces)

            for element in body_elements:
                if element.tag.endswith('p'):  # 段落
                    # 處理段落
                    text_elements = element.xpath('.//w:t', namespaces=namespaces)
                    paragraph_text = ''.join([t.text or '' for t in text_elements]).strip()

                    if not paragraph_text:
                        continue

                    # 獲取段落級別 - 多種方式嘗試
                    outline_level = None

                    # 方式1: 檢查pStyle
                    pStyle = element.xpath('.//w:pStyle', namespaces=namespaces)
                    if pStyle:
                        style_id = pStyle[0].get('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}val')
                        if style_id in style_outline_map:
                            outline_level = style_outline_map[style_id]
                        elif style_id and (style_id.startswith('Heading') or style_id.startswith('heading')):
                            try:
                                outline_level = int(re.findall(r'\d+', style_id)[0])
                            except (ValueError, IndexError):
                                pass

                    # 方式2: 直接檢查outlineLvl
                    if outline_level is None:
                        outline_lvl_elem = element.xpath('.//w:outlineLvl', namespaces=namespaces)
                        if outline_lvl_elem:
                            outline_level = int(outline_lvl_elem[0].get('{http://schemas.openxmlformats.org/wordprocessingml/2006/main}val')) + 1

                    # 方式3: 啟發式判斷（如果前面幾種方式都失敗）
                    if outline_level is None and is_likely_heading(paragraph_text):
                        outline_level = infer_heading_level(paragraph_text)

                    # 格式化輸出
                    if outline_level:
                        indent = "  " * (outline_level - 1)
                        extracted_content += f"{indent}Heading {outline_level}: {paragraph_text}\n"
                    else:
                        extracted_content += f"Paragraph: {paragraph_text}\n"

                elif element.tag.endswith('tbl'):  # 表格
                    rows = element.xpath('.//w:tr', namespaces=namespaces)
                    if rows:
                        extracted_content += f"Table:\n"
                        for row_idx, row in enumerate(rows):
                            cells = row.xpath('.//w:tc', namespaces=namespaces)
                            row_data = []
                            for cell in cells:
                                cell_text = ''.join([t.text or '' for t in cell.xpath('.//w:t', namespaces=namespaces)])
                                row_data.append(cell_text.strip())

                            if row_data:
                                extracted_content += "Paragraph: " + " | ".join(row_data) + "\n"

            return extracted_content

    except Exception as e:
        raise Exception(f"Error parsing docx file: {e}")

def save_chunks_to_files(chunks: List[TextChunk], output_folder: str, base_filename: str, filename_prefix: str = ""):
    """將分塊保存到文件"""
    chunks_folder = os.path.join(output_folder, f"{base_filename}_chunks")
    os.makedirs(chunks_folder, exist_ok=True)

    # 保存每個分塊到單獨的文件
    for chunk in chunks:
        # 在此處加入前綴
        chunk_filename = f"{filename_prefix}{base_filename}_{chunk.chunk_id}.txt"
        chunk_path = os.path.join(chunks_folder, chunk_filename)

        with open(chunk_path, 'w', encoding='utf-8') as f:
            f.write(f"Chunk path: {chunk_path}\n")
            f.write(f"Chunk name: {chunk_filename}\n")
            f.write("=" * 50 + "\n")
            f.write(f"Chunk ID: {chunk.chunk_id}\n")
            f.write(f"Source Node: {chunk.source_node_index}\n")
            f.write(f"Chunk Index: {chunk.chunk_index}\n")
            f.write(f"Path Titles: {' > '.join(chunk.path_titles)}\n")
            if chunk.metadata.get('is_path_merged'):
                f.write(f"Merged From: {', '.join(chunk.metadata.get('merged_from', []))}\n")
                f.write(f"Merged Count: {chunk.metadata.get('merged_chunk_count', 0)}\n")
            f.write("=" * 50 + "\n")
            f.write(chunk.content)
            f.write("\n" + "=" * 50 + "\n")
            f.write(f"Metadata: {chunk.metadata}\n")

    # 保存分塊摘要
    summary_path = os.path.join(chunks_folder, "chunks_summary.txt")
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write(f"Total Chunks: {len(chunks)}\n")
        f.write(f"Path Merging Applied: {any(chunk.metadata.get('is_path_merged') for chunk in chunks)}\n")
        f.write("=" * 50 + "\n")

        for chunk in chunks:
            f.write(f"Chunk ID: {chunk.chunk_id}\n")
            f.write(f"  Path: {' > '.join(chunk.path_titles)}\n")
            f.write(f"  Content Length: {len(chunk.content)}\n")
            f.write(f"  Source Node: {chunk.source_node_index}\n")
            if chunk.metadata.get('is_path_merged'):
                f.write(f"  Merged Count: {chunk.metadata.get('merged_chunk_count')}\n")
            f.write(f"  Preview: {chunk.content[:100]}...\n")
            f.write("-" * 30 + "\n")

    print(f"Saved {len(chunks)} chunks to {chunks_folder}")
    return chunks_folder



def analyze_document_structure(doc_tree: DocumentTree):
    """分析文檔結構的輔助函數"""
    print("Document Structure Analysis:")
    print("=" * 50)

    total_nodes = len(doc_tree.nodes)
    heading_nodes = sum(1 for node in doc_tree.nodes if node.node_type == "heading")
    paragraph_nodes = sum(1 for node in doc_tree.nodes if node.node_type == "paragraph")
    leaf_nodes = len(doc_tree.get_leaf_nodes())

    print(f"Total nodes: {total_nodes}")
    print(f"Heading nodes: {heading_nodes}")
    print(f"Paragraph nodes: {paragraph_nodes}")
    print(f"Leaf nodes: {leaf_nodes}")
    print()

    # 分析層級分佈
    level_distribution = {}
    for node in doc_tree.nodes:
        if node.node_type == "heading":
            level = node.level
            if level not in level_distribution:
                level_distribution[level] = 0
            level_distribution[level] += 1

    print("Heading level distribution:")
    for level in sorted(level_distribution.keys()):
        print(f"  Level {level}: {level_distribution[level]} headings")
    print()

    # 分析文本長度
    text_lengths = [len(node.text) for node in doc_tree.nodes if node.text.strip()]
    if text_lengths:
        avg_length = sum(text_lengths) / len(text_lengths)
        max_length = max(text_lengths)
        min_length = min(text_lengths)

        print(f"Text length statistics:")
        print(f"  Average: {avg_length:.1f} characters")
        print(f"  Maximum: {max_length} characters")
        print(f"  Minimum: {min_length} characters")
    print()

def export_chunks_to_json(chunks: List[TextChunk], output_path: str):
    """將分塊導出為JSON格式"""
    import json

    chunks_data = []
    for chunk in chunks:
        chunk_data = {
            'chunk_id': chunk.chunk_id,
            'content': chunk.content,
            'path_titles': chunk.path_titles,
            'source_node_index': chunk.source_node_index,
            'chunk_index': chunk.chunk_index,
            'metadata': chunk.metadata
        }
        chunks_data.append(chunk_data)

    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(chunks_data, f, ensure_ascii=False, indent=2)

    print(f"Exported {len(chunks)} chunks to JSON: {output_path}")

def create_chunk_index(chunks: List[TextChunk]) -> Dict[str, Any]:
    """創建分塊索引，便於查找和管理"""
    index = {
        'total_chunks': len(chunks),
        'chunks_by_node': {},
        'chunks_by_path': {},
        'chunk_metadata': {}
    }

    for chunk in chunks:
        # 按源節點索引
        node_idx = chunk.source_node_index
        if node_idx not in index['chunks_by_node']:
            index['chunks_by_node'][node_idx] = []
        index['chunks_by_node'][node_idx].append(chunk.chunk_id)

        # 按路徑索引
        path_key = ' > '.join(chunk.path_titles)
        if path_key not in index['chunks_by_path']:
            index['chunks_by_path'][path_key] = []
        index['chunks_by_path'][path_key].append(chunk.chunk_id)

        # 元數據索引
        index['chunk_metadata'][chunk.chunk_id] = {
            'content_length': len(chunk.content),
            'path_titles': chunk.path_titles,
            'source_node': chunk.source_node_index,
            'chunk_index': chunk.chunk_index,
            'metadata': chunk.metadata
        }

    return index


def process_documents_with_fallback(target_files, data_folder, output_folder,
                                  enable_chunking=True,
                                  max_chunk_size=4096,
                                  min_overlap=100,
                                  max_overlap=400,
                                  export_json=False,
                                  create_index=False,
                                  analyze_structure=True):
    """處理文檔並構建文檔樹，包含回退機制"""

    # 確保輸出文件夾存在
    os.makedirs(output_folder, exist_ok=True)

    for file_name in target_files:
        file_path = os.path.join(data_folder, file_name)

        if not os.path.exists(file_path):
            print(f"File not found: {file_path}")
            continue

        try:
            print(f"Processing: {file_name}")

            # 根據文件類型提取內容
            if file_name.lower().endswith('.docx'):
                extracted_content = extract_outline_from_docx_with_tables(file_path)
            elif file_name.lower().endswith('.pdf'):
                extracted_content = extract_outline_from_pdf(file_path)
            elif file_name.lower().endswith('.md'):
                extracted_content = extract_outline_from_markdown(file_path)
            elif file_name.lower().endswith(('.xlsx', '.xls')):
                extracted_content = extract_outline_from_excel(file_path)
            else:
                print(f"Unsupported file type: {file_name}")
                continue

            # 構建文檔樹
            doc_tree = parse_extracted_text(extracted_content)
            base_filename = file_name.rsplit('.', 1)[0]

            # 分析文檔結構
            # analyze_document_structure(doc_tree)

            # 保存文件
            output_filename = base_filename + '.txt'
            output_path = os.path.join(output_folder, output_filename)
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(extracted_content)

            tree_filename = base_filename + '_tree.txt'
            tree_path = os.path.join(output_folder, tree_filename)
            with open(tree_path, 'w', encoding='utf-8') as f:
                f.write("Document Tree Structure:\n")
                f.write("=" * 50 + "\n")
                f.write(doc_tree.get_tree_structure())

            # 分塊處理
            if enable_chunking:
                print("Creating chunks with fallback mechanism...")
                chunks = doc_tree.create_all_chunks(
                    max_chunk_size=max_chunk_size,
                    min_overlap=min_overlap,
                    max_overlap=max_overlap
                )

                if chunks:
                    chunks_folder = save_chunks_to_files(chunks, output_folder, base_filename)
                    print(f"Successfully created {len(chunks)} chunks")
                    # 導出JSON
                    if export_json:
                        json_path = os.path.join(output_folder, f"{base_filename}_chunks.json")
                        export_chunks_to_json(chunks, json_path)

                    # 創建索引
                    if create_index:
                        chunk_index = create_chunk_index(chunks)
                        index_path = os.path.join(output_folder, f"{base_filename}_chunk_index.json")
                        import json
                        with open(index_path, 'w', encoding='utf-8') as f:
                            json.dump(chunk_index, f, ensure_ascii=False, indent=2)
                        print(f"Created chunk index: {index_path}")

                    print(f"Successfully processed {file_name} with {len(chunks)} chunks")
                else:
                    print("No chunks created")

            print()

        except Exception as e:
            print(f"Error processing {file_name}: {e}")
            import traceback
            traceback.print_exc()

def process_documents_with_advanced_options(target_files, data_folder, output_folder,
                                           # 新增選項
                                           save_full_text=True,
                                           save_tree_structure=True,
                                           save_chunk_summary=True,
                                           filename_prefix="",
                                           # 現有選項
                                           enable_chunking=True,
                                           max_chunk_size=4096,
                                           min_overlap=100,
                                           max_overlap=400,
                                           enable_path_merging=False,
                                           export_json=True,
                                           create_index=True,
                                           analyze_structure=True):
    """增強版文檔處理函數，包含超细粒度合并和選擇性保存選項"""

    os.makedirs(output_folder, exist_ok=True)

    for file_name in target_files:
        file_path = os.path.join(data_folder, file_name)

        if not os.path.exists(file_path):
            print(f"File not found: {file_path}")
            continue

        try:
            print(f"Processing: {file_name}")

            if file_name.lower().endswith('.docx'):
                extracted_content = extract_outline_from_docx_with_tables(file_path)
            elif file_name.lower().endswith('.pdf'):
                extracted_content = extract_outline_from_pdf(file_path)
            elif file_name.lower().endswith('.md'):
                extracted_content = extract_outline_from_markdown(file_path)
            elif file_name.lower().endswith(('.xlsx', '.xls')):
                extracted_content = extract_outline_from_excel(file_path)
            else:
                print(f"Unsupported file type: {file_name}")
                continue

            doc_tree = parse_extracted_text(extracted_content)
            base_filename = file_name.rsplit('.', 1)[0]

            if analyze_structure:
                analyze_document_structure(doc_tree)

            # 根據選項保存基礎文件
            if save_full_text:
                output_filename = base_filename + '.txt'
                output_path = os.path.join(output_folder, output_filename)
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(extracted_content)
                print(f"Saved full text to {output_path}")

            if save_tree_structure:
                tree_filename = base_filename + '_tree.txt'
                tree_path = os.path.join(output_folder, tree_filename)
                with open(tree_path, 'w', encoding='utf-8') as f:
                    f.write("Document Tree Structure:\n")
                    f.write("=" * 50 + "\n")
                    f.write(doc_tree.get_tree_structure())
                print(f"Saved tree structure to {tree_path}")

            if enable_chunking:
                print("Creating chunks...")
                chunks = doc_tree.create_all_chunks(
                    max_chunk_size=max_chunk_size,
                    min_overlap=min_overlap,
                    max_overlap=max_overlap
                )

                if enable_path_merging and chunks:
                    print("Applying path-based merging...")
                    original_count = len(chunks)
                    chunks = merge_chunks_by_path(chunks, max_chunk_size, max_overlap)
                    print(f"Path merging reduced chunks from {original_count} to {len(chunks)}")

                if chunks:
                    # 傳入前綴以保存分塊文件
                    chunks_folder = save_chunks_to_files(chunks, output_folder, base_filename, filename_prefix)

                    # 選擇性保存 chunk summary
                    if not save_chunk_summary:
                        summary_path = os.path.join(chunks_folder, "chunks_summary.txt")
                        if os.path.exists(summary_path):
                            os.remove(summary_path)
                            print("Chunk summary file removed as per settings.")

                    if export_json:
                        json_path = os.path.join(output_folder, f"{base_filename}_chunks.json")
                        export_chunks_to_json(chunks, json_path)

                    if create_index:
                        chunk_index = create_chunk_index(chunks)
                        index_path = os.path.join(output_folder, f"{base_filename}_chunk_index.json")
                        import json
                        with open(index_path, 'w', encoding='utf-8') as f:
                            json.dump(chunk_index, f, ensure_ascii=False, indent=2)
                        print(f"Created chunk index: {index_path}")

                    print(f"Successfully processed {file_name} with {len(chunks)} chunks")
                else:
                    print("No chunks created")

            print()

        except Exception as e:
            print(f"Error processing {file_name}: {e}")
            import traceback
            traceback.print_exc()

# 主函數
if __name__ == "__main__":
    # 測試分塊功能

    file_list = os.listdir('/content/data')
    target_files = [f for f in file_list if f.lower().endswith(('.pdf', '.docx', '.md', '.xlsx', '.xls'))]

    print(target_files)

    # 使用增強版本，開啟超细粒度合并
    process_documents_with_advanced_options(
        target_files=target_files,
        data_folder='./data',
        output_folder='./output_chunks',

        # --- 新的控制選項 ---
        save_full_text=False,          # 設置為 False，不保存原始提取的 .txt 檔案
        save_tree_structure=False,     # 設置為 False，不保存樹狀結構檔案
        save_chunk_summary=True,       # 保持 True，保存 chunk 的摘要檔案
        filename_prefix="project-test_",  # 為每個 chunk 檔案名稱加上 "project_A_" 前綴

        # --- 原有的處理選項 ---
        enable_chunking=True,
        max_chunk_size=4096,
        min_overlap=200,
        max_overlap=500,
        enable_path_merging=True,
        export_json=False,
        create_index=False,
        analyze_structure=False
    )

"""# 新增區段 test"""
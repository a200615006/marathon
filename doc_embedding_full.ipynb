{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd904dc2-b816-432f-bf16-6790cb770384",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (VectorStoreIndex, SimpleDirectoryReader, load_index_from_storage\n",
    "    , Document, Settings, StorageContext, PromptTemplate)\n",
    "from llama_index.vector_stores.milvus import MilvusVectorStore\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core.extractors import KeywordExtractor, SummaryExtractor\n",
    "from llama_index.core.schema import MetadataMode\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.llms.dashscope import DashScope\n",
    "\n",
    "from llama_index.extractors.entity import EntityExtractor\n",
    "from llama_index.readers.file import UnstructuredReader,PyMuPDFReader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "import os, re, asyncio\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8c75a3f-3416-499a-9ec9-edc255829118",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 15:45:06,059 - INFO - Load pretrained SentenceTransformer: ./Qwen3-Embedding-0.6B\n",
      "2025-10-09 15:45:07,089 - INFO - 1 prompt is loaded, with the key: query\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型嵌入维度: 1024\n"
     ]
    }
   ],
   "source": [
    "embedding_model = \"./Qwen3-Embedding-0.6B\"\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=embedding_model,\n",
    "    cache_folder=None,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained(embedding_model, trust_remote_code=True, local_files_only=True)\n",
    "dimension = config.hidden_size\n",
    "print(f\"模型嵌入维度: {dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77249092-c390-4c43-a08d-44631dff2612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_model = \"./Qwen3-4B-Thinking-2507\"\n",
    "# Settings.llm = HuggingFaceLLM(\n",
    "#     model_name=main_model,\n",
    "#     tokenizer_name=main_model,\n",
    "#     generate_kwargs={\"temperature\": 0.1, \"top_p\": 0.7},\n",
    "#     device_map=\"cuda\",\n",
    "#     max_new_tokens=512\n",
    "# )\n",
    "\n",
    "Settings.llm = DashScope(\n",
    "    api_key=\"sk-7afc6caf37b64d069ef3be129e68753a\",\n",
    "    model=\"qwen3-max\",\n",
    "    generate_kwargs={\"temperature\": 0.1, \"top_p\": 0.7},\n",
    "    max_new_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e20ec199-841c-4eef-b612-7cc4422d8688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "绝对数据库路径: /root/marathon/milvus_test/milvus_lite.db\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/milvus_lite/__init__.py:15: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import DistributionNotFound, get_distribution\n"
     ]
    }
   ],
   "source": [
    "milvus_dir = \"./milvus_test\"\n",
    "milvus_db_path = os.path.join(milvus_dir, \"milvus_lite.db\")\n",
    "abs_db_path = os.path.abspath(milvus_db_path)\n",
    "print(f\"绝对数据库路径: {abs_db_path}\")\n",
    "\n",
    "if not os.path.exists(milvus_dir):\n",
    "    os.makedirs(milvus_dir)\n",
    "    print(\"已创建 ./milvus 目录\")\n",
    "\n",
    "milvus_vector_store = MilvusVectorStore(\n",
    "    uri=f\"{abs_db_path}\",\n",
    "    collection_name=\"rag_collection\",\n",
    "    dim=1024,\n",
    "    overwrite=False\n",
    ")\n",
    "storage_context = StorageContext.from_defaults(vector_store=milvus_vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "567914c9-37a4-4739-bf70-da546fa297e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text).strip()\n",
    "    # text = re.sub(r'(\\w+\\s*){3,}\\n', '', text)\n",
    "    # text = re.sub(r'[^a-zA-Z0-9\\u4e00-\\u9fa5\\s\\.,!?]', '', text)  # 去除特殊字符，保留中英文\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b345e0de-d5ab-4f42-8a2f-da4bbb4bc79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_summary_async(text, max_words=20):\n",
    "    prompt = f\"总结以下文本，不超过{max_words}字，直接回复结果：{text}\"\n",
    "    response = await Settings.llm.acomplete(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "def generate_summary(text, max_words=20):\n",
    "    prompt = f\"总结以下文本，不超过{max_words}字，直接回复结果：{text}\"\n",
    "    response = Settings.llm.complete(prompt)\n",
    "    return response.text.strip()\n",
    "\n",
    "async def add_summaries_to_nodes_async(nodes_list):\n",
    "    tasks = [generate_summary_async(node.text) for node in nodes_list]\n",
    "\n",
    "    summaries = []\n",
    "    for future in tqdm_asyncio.as_completed(tasks, total=len(tasks), desc=\"生成节点摘要进度\"):\n",
    "        summary = await future\n",
    "        summaries.append(summary)\n",
    "\n",
    "    for node, summary in zip(nodes_list, summaries):\n",
    "        node.metadata[\"node_summary\"] = summary\n",
    "        \n",
    "def add_summaries_to_nodes(nodes_list):\n",
    "    for node in tqdm(nodes_list, desc=\"生成摘要\"):\n",
    "        summary = generate_summary(node.text)\n",
    "        node.metadata[\"node_summary\"] = summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "619e3fe0-1330-4417-be61-b13e6867ee68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件大小:326\n"
     ]
    }
   ],
   "source": [
    "qwen_tokenizer = AutoTokenizer.from_pretrained(\"./Qwen3-Embedding-0.6B\", trust_remote_code=True)\n",
    "documents_dir = \"./docs\"\n",
    "\n",
    "file_extractor = {\n",
    "    \".pdf\": PyMuPDFReader(), \n",
    "    \".docx\": UnstructuredReader()\n",
    "}\n",
    "reader = SimpleDirectoryReader(input_dir=documents_dir, recursive=True, file_extractor=file_extractor)\n",
    "documents = reader.load_data()\n",
    "\n",
    "cleaned_documents = [Document(text=clean_text(doc.text), metadata=doc.metadata) for doc in documents]\n",
    "documents = cleaned_documents\n",
    "\n",
    "print(f\"文件大小:{len(documents)}\")\n",
    "\n",
    "node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=100, tokenizer=qwen_tokenizer.tokenize)\n",
    "#nodes = node_parser.get_nodes_from_documents(documents)\n",
    "#print(f\"节点数量:{len(nodes)}\")\n",
    "\n",
    "#asyncio.run(add_summaries_to_nodes_async(nodes))\n",
    "#add_summaries_to_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbc61554-078f-4042-83c7-720d344f0f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_summaries_to_json(nodes_list, file_path=\"nodes_summaries_temp.json\"):\n",
    "    summaries_dict = {}\n",
    "    for idx, node in enumerate(nodes_list):\n",
    "        summaries_dict[str(idx)] = node.metadata.get(\"node_summary\", \"\")  # 获取摘要，若无则为空\n",
    "    \n",
    "    # 保存到 JSON\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summaries_dict, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"节点摘要已保存到 {file_path}\")\n",
    "\n",
    "def load_summaries_to_nodes(nodes_list, file_path=\"nodes_summaries.json\"):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        summaries_dict = json.load(f)\n",
    "    sorted_keys = sorted(summaries_dict.keys(), key=int)\n",
    "\n",
    "    for key in sorted_keys:\n",
    "        idx = int(key)\n",
    "        if idx < len(nodes_list):\n",
    "            nodes_list[idx].metadata[\"node_summary\"] = summaries_dict[key]\n",
    "        else:\n",
    "            print(f\"警告：索引 {idx} 超出节点列表长度，跳过。\")\n",
    "    \n",
    "    return nodes_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0f7ab110-62f0-4da3-bce4-cd8775eb24d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "节点摘要已保存到 nodes_summaries.json\n"
     ]
    }
   ],
   "source": [
    "#save_summaries_to_json(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fff6d82-bcf3-4272-bfdf-5c6d8209f04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyword_extractor = KeywordExtractor(\n",
    "#     llm=Settings.llm,  # 使用 LLM 提取关键词作为主题\n",
    "#     keywords=5,  # 提取前 5 个关键词\n",
    "#     prompt_template_str=\"\"\"\n",
    "#     从以下文本中提取 2 个主要关键词。\n",
    "#     输出格式：仅用逗号分隔的关键词列表，不要添加任何解释或额外文本。\n",
    "#     示例输出：关键词1,关键词2\n",
    "\n",
    "#     文本：{text}\n",
    "#     \"\"\"\n",
    "# )\n",
    "# max_words = 20\n",
    "# summary_extractor = SummaryExtractor(\n",
    "#     llm=Settings.llm,\n",
    "#     summaries=[\"self\"],\n",
    "#     prompt_template_str=\"\"\"\n",
    "#     总结以下文本，不超过20字，直接回复结果：\\n{text}\n",
    "#         \"\"\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "775b208d-7576-4475-89ad-5cafb8a9cee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = VectorStoreIndex.from_documents(\n",
    "#     documents,\n",
    "#     storage_context=storage_context,\n",
    "#     embed_model=Settings.embed_model,\n",
    "#     node_parser=node_parser,\n",
    "#     store_nodes_override=True\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "transformations = [node_parser]\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents,\n",
    "    storage_context=storage_context,\n",
    "    embed_model=Settings.embed_model,\n",
    "    # node_parser=node_parser,\n",
    "    transformations=transformations,\n",
    "    store_nodes_override=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e313de7b-3c72-4774-b09b-495c9f47fddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n",
      "节点数量: 353\n"
     ]
    }
   ],
   "source": [
    "nodes = list(index.docstore.docs.values())\n",
    "print(len(documents[0].text))\n",
    "print(f\"节点数量: {len(nodes)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94296e03-1489-43a4-aa3a-58af278065ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_nodes = load_summaries_to_nodes(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6257d0e6-daf0-454c-aae0-cdd242a23236",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = VectorStoreIndex(\n",
    "    loaded_nodes, \n",
    "    storage_context=storage_context, \n",
    "    embed_model=Settings.embed_model, \n",
    "    store_nodes_override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6866ae85-1668-4cb9-96d4-739d5c78126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_with_sums = list(index.docstore.docs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac47622a-bff0-40c5-8268-4a64e6d2f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for node in nodes_with_sums:\n",
    "    print(node.metadata[\"node_summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86095679-0990-4a3b-a46f-9cec92bfd155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "#以下是Reranker步骤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac1217ce-40e8-4722-9d07-eeb0e5e4b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.packs.fusion_retriever import HybridFusionRetrieverPack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3268b969-dfb8-435f-b4e0-759a33c094a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Qwen3ForSequenceClassification were not initialized from the model checkpoint at ./Qwen3-Reranker-0.6B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad token: [PAD]\n",
      "Pad token ID: 151669\n",
      "Model config pad_token_id: 151669\n"
     ]
    }
   ],
   "source": [
    "reranker_model_path = \"./Qwen3-Reranker-0.6B\"\n",
    "\n",
    "reranker = SentenceTransformerRerank(\n",
    "    model=reranker_model_path,\n",
    "    top_n=5,\n",
    "    device=\"cuda\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "cross_encoder = reranker._model\n",
    "reranker_tokenizer = cross_encoder.tokenizer\n",
    "reranker_model = cross_encoder.model\n",
    "\n",
    "special_tokens = {'pad_token': '[PAD]'}\n",
    "num_added_tokens = reranker_tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "reranker_model.resize_token_embeddings(len(reranker_tokenizer))\n",
    "\n",
    "reranker_tokenizer.pad_token = '[PAD]'\n",
    "reranker_tokenizer.pad_token_id = reranker_tokenizer.convert_tokens_to_ids('[PAD]')\n",
    "reranker_model.config.pad_token_id = reranker_tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Pad token: {reranker_tokenizer.pad_token}\")\n",
    "print(f\"Pad token ID: {reranker_tokenizer.pad_token_id}\")\n",
    "print(f\"Model config pad_token_id: {reranker_model.config.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "679e14bd-a633-42c8-b148-ae94e9ff7d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reranker_tokenize(text):\n",
    "    rerank_tokenizer = AutoTokenizer.from_pretrained(\"./Qwen3-Reranker-0.6B\", padding_side='left')\n",
    "    if not text.strip():\n",
    "        return []\n",
    "    tokens = rerank_tokenizer.tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bd42524-f6c5-48e3-b4cb-c1a420db9dec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 15:48:19,600 - WARNING - The tokenizer parameter is deprecated and will be removed in a future release. Use a stemmer from PyStemmer instead.\n",
      "2025-10-09 15:48:19,772 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    nodes=nodes_with_sums, \n",
    "    similarity_top_k=10,\n",
    "    tokenizer=reranker_tokenize)\n",
    "vector_retriever = VectorIndexRetriever(index=index, similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6163be4-9d0d-4a7f-acf3-ef0bfbb28e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-09 15:48:39,813 - DEBUG - Building index from IDs objects\n"
     ]
    }
   ],
   "source": [
    "hybrid_pack = HybridFusionRetrieverPack(\n",
    "    nodes=nodes_with_sums,\n",
    "    bm25_retriever=bm25_retriever,\n",
    "    vector_retriever=vector_retriever,\n",
    "    mode=\"reciprocal_rerank\",\n",
    "    similarity_top_k=10\n",
    ")\n",
    "hybrid_retriever = hybrid_pack.fusion_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac381211-4274-4167-8fb1-0943eb15cec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_qa_template_str = (\n",
    "    \"上下文信息如下：\\n\"\n",
    "    \"{context_str}\\n\"\n",
    "    \"基于提供的上下文，用中文直接回答查询，答案只能从上下文知识中获取，不要自己发挥。\\n\"\n",
    "    \"查询：{query_str}\\n\"\n",
    "    \"回答：\"\n",
    ")\n",
    "text_qa_template = PromptTemplate(text_qa_template_str)\n",
    "\n",
    "refine_template_str = (\n",
    "    \"原始查询是：{query_str}\\n\"\n",
    "    \"我们已有回答：{existing_answer}\\n\"\n",
    "    \"基于以下新上下文，用中文精炼现有回答，确保完整性和准确性：\\n\"\n",
    "    \"{context_msg}\\n\"\n",
    "    \"精炼后的回答：\"\n",
    ")\n",
    "refine_template = PromptTemplate(refine_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55d19d9b-9767-4616-9aed-860010a1bbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_synthesizer = get_response_synthesizer(\n",
    "    text_qa_template=text_qa_template,\n",
    "    refine_template=refine_template,\n",
    "    response_mode=\"compact\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "338d9f96-6f8f-4caa-8dfb-080eab0a808e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = RetrieverQueryEngine(\n",
    "    retriever=hybrid_retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    node_postprocessors=[reranker]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c440156a-70aa-4d53-a410-f2ceda5ecf0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated queries:\n",
      "Queries:\n",
      "1. GPT的定义和工作原理\n",
      "2. GPT模型的发展历程及其应用领域\n",
      "3. 如何使用GPT进行文本生成与处理\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e699cfd2644591bb861afabe6b6d4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT是一种通过大量网络数据学习的模型，旨在提升文本生成与问答能力。最初版本的GPT（Generative Pre-trained Transformer）在2018年出现，拥有约1亿1千7百万个参数，并基于相对较小的数据集进行训练。随着时间的发展，GPT系列模型逐渐增大，比如GPT-2的参数量达到了15亿，而GPT-3更是达到了GPT-2的100倍大，且训练数据量也显著增加至570GB。这些模型能够根据给定的输入生成连贯的文本输出，甚至可以回答问题或生成文章摘要等。简而言之，GPT是利用互联网上的海量信息来训练的一种深度学习模型，特别擅长处理自然语言相关的任务。\n"
     ]
    }
   ],
   "source": [
    "query = \"什么是GPT？\"\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e56a636-29bf-4723-98a6-032f2f34d0d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

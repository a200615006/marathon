{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d95783b-a4ed-42ce-a7ce-dd9a9fb3ec38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAGæª¢ç´¢å™¨ä½¿ç”¨è¨­å‚™: cuda\n",
      "RAGæª¢ç´¢å™¨åˆå§‹åŒ–å®Œæˆï¼Œé€£æ¥åˆ°é›†åˆ: testchunks\n",
      "æª¢ç´¢å™¨çµ±è¨ˆ: {'collection_name': 'testchunks', 'stats': {'row_count': 567}, 'device': 'cuda', 'model_dimension': 768}\n",
      "\n",
      "============================================================\n",
      "æ¸¬è©¦æŸ¥è©¢: äººå·¥æ™ºèƒ½çš„ç™¼å±•è¶¨å‹¢\n",
      "æ­£åœ¨æª¢ç´¢æŸ¥è©¢: 'äººå·¥æ™ºèƒ½çš„ç™¼å±•è¶¨å‹¢...' (top_k=10)\n",
      "æª¢ç´¢å®Œæˆï¼Œæ‰¾åˆ° 10 å€‹ç›¸é—œçµæœ\n",
      "æ‰¾åˆ° 10 å€‹ç›¸é—œçµæœ:\n",
      "1. åˆ†æ•¸: 0.8554\n",
      "   ä¾†æº: documents_dup_part_1_part_1_chunks/prod_documents_dup_part_1_part_1_path_merged_node_0_chunk_92_to_node_0_chunk_92.txt\n",
      "   å…§å®¹: Chunk path: ./chunks_output/documents_dup_part_1_part_1_chunks/prod_documents_dup_part_1_part_1_path...\n",
      "\n",
      "2. åˆ†æ•¸: 0.9175\n",
      "   ä¾†æº: documents_dup_part_1_part_1_chunks/prod_documents_dup_part_1_part_1_path_merged_node_0_chunk_8_to_node_0_chunk_8.txt\n",
      "   å…§å®¹: Chunk path: ./chunks_output/documents_dup_part_1_part_1_chunks/prod_documents_dup_part_1_part_1_path...\n",
      "\n",
      "3. åˆ†æ•¸: 0.9766\n",
      "   ä¾†æº: documents_dup_part_1_part_1_chunks/prod_documents_dup_part_1_part_1_path_merged_node_0_chunk_38_to_node_0_chunk_38.txt\n",
      "   å…§å®¹: Chunk path: ./chunks_output/documents_dup_part_1_part_1_chunks/prod_documents_dup_part_1_part_1_path...\n",
      "\n",
      "4. åˆ†æ•¸: 0.9884\n",
      "   ä¾†æº: test_ml_chunks/prod_test_ml_path_merged_node_76_chunk_0_to_node_79_chunk_0.txt\n",
      "   å…§å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_path_merged_node_76_chunk_0_to_node_79_chunk...\n",
      "\n",
      "5. åˆ†æ•¸: 1.0046\n",
      "   ä¾†æº: test_ml_chunks/prod_test_ml_node_80_chunk_0.txt\n",
      "   å…§å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_node_80_chunk_0.txt\n",
      "Chunk name: prod_test_ml...\n",
      "\n",
      "\n",
      "============================================================\n",
      "æ¸¬è©¦æŸ¥è©¢: æ©Ÿå™¨å­¸ç¿’ç®—æ³•\n",
      "æ­£åœ¨æª¢ç´¢æŸ¥è©¢: 'æ©Ÿå™¨å­¸ç¿’ç®—æ³•...' (top_k=10)\n",
      "æª¢ç´¢å®Œæˆï¼Œæ‰¾åˆ° 10 å€‹ç›¸é—œçµæœ\n",
      "æ‰¾åˆ° 10 å€‹ç›¸é—œçµæœ:\n",
      "1. åˆ†æ•¸: 0.7015\n",
      "   ä¾†æº: test_ml_chunks/prod_test_ml_path_merged_node_38_chunk_0_to_node_40_chunk_0.txt\n",
      "   å…§å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_path_merged_node_38_chunk_0_to_node_40_chunk...\n",
      "\n",
      "2. åˆ†æ•¸: 0.7234\n",
      "   ä¾†æº: test_ml_chunks/prod_test_ml_path_merged_node_11_chunk_0_to_node_15_chunk_0.txt\n",
      "   å…§å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_path_merged_node_11_chunk_0_to_node_15_chunk...\n",
      "\n",
      "3. åˆ†æ•¸: 0.7279\n",
      "   ä¾†æº: test_ml_chunks/prod_test_ml_node_36_chunk_0.txt\n",
      "   å…§å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_node_36_chunk_0.txt\n",
      "Chunk name: prod_test_ml...\n",
      "\n",
      "4. åˆ†æ•¸: 0.7512\n",
      "   ä¾†æº: test_ml_chunks/prod_test_ml_node_26_chunk_0.txt\n",
      "   å…§å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_node_26_chunk_0.txt\n",
      "Chunk name: prod_test_ml...\n",
      "\n",
      "5. åˆ†æ•¸: 0.7529\n",
      "   ä¾†æº: test_ml_chunks/prod_test_ml_path_merged_node_48_chunk_0_to_node_49_chunk_0.txt\n",
      "   å…§å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_path_merged_node_48_chunk_0_to_node_49_chunk...\n",
      "\n",
      "\n",
      "============================================================\n",
      "æ¸¬è©¦æŸ¥è©¢: æ·±åº¦å­¸ç¿’æ‡‰ç”¨\n",
      "æ­£åœ¨æª¢ç´¢æŸ¥è©¢: 'æ·±åº¦å­¸ç¿’æ‡‰ç”¨...' (top_k=10)\n",
      "æª¢ç´¢å®Œæˆï¼Œæ‰¾åˆ° 10 å€‹ç›¸é—œçµæœ\n",
      "æ‰¾åˆ° 10 å€‹ç›¸é—œçµæœ:\n",
      "1. åˆ†æ•¸: 0.8076\n",
      "   ä¾†æº: test_ml_chunks/prod_test_ml_path_merged_node_18_chunk_0_to_node_21_chunk_0.txt\n",
      "   å…§å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_path_merged_node_18_chunk_0_to_node_21_chunk...\n",
      "\n",
      "2. åˆ†æ•¸: 0.8128\n",
      "   ä¾†æº: test_ml_chunks/prod_test_ml_path_merged_node_38_chunk_0_to_node_40_chunk_0.txt\n",
      "   å…§å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_path_merged_node_38_chunk_0_to_node_40_chunk...\n",
      "\n",
      "3. åˆ†æ•¸: 0.8266\n",
      "   ä¾†æº: test_ml_chunks/prod_test_ml_path_merged_node_48_chunk_0_to_node_49_chunk_0.txt\n",
      "   å…§å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_path_merged_node_48_chunk_0_to_node_49_chunk...\n",
      "\n",
      "4. åˆ†æ•¸: 0.8443\n",
      "   ä¾†æº: test_ml_chunks/prod_test_ml_path_merged_node_53_chunk_0_to_node_56_chunk_0.txt\n",
      "   å…§å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_path_merged_node_53_chunk_0_to_node_56_chunk...\n",
      "\n",
      "5. åˆ†æ•¸: 0.8772\n",
      "   ä¾†æº: test_ml_chunks/prod_test_ml_node_66_chunk_0.txt\n",
      "   å…§å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_node_66_chunk_0.txt\n",
      "Chunk name: prod_test_ml...\n",
      "\n",
      "\n",
      "============================================================\n",
      "æ¸¬è©¦æŸ¥è©¢: è‡ªç„¶èªè¨€è™•ç†æŠ€è¡“\n",
      "æ­£åœ¨æª¢ç´¢æŸ¥è©¢: 'è‡ªç„¶èªè¨€è™•ç†æŠ€è¡“...' (top_k=10)\n",
      "æª¢ç´¢å®Œæˆï¼Œæ‰¾åˆ° 10 å€‹ç›¸é—œçµæœ\n",
      "æ‰¾åˆ° 10 å€‹ç›¸é—œçµæœ:\n",
      "1. åˆ†æ•¸: 0.9469\n",
      "   ä¾†æº: test_ml_chunks/prod_test_ml_path_merged_node_58_chunk_0_to_node_60_chunk_0.txt\n",
      "   å…§å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_path_merged_node_58_chunk_0_to_node_60_chunk...\n",
      "\n",
      "2. åˆ†æ•¸: 1.0545\n",
      "   ä¾†æº: test_ml_chunks/prod_test_ml_path_merged_node_38_chunk_0_to_node_40_chunk_0.txt\n",
      "   å…§å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_path_merged_node_38_chunk_0_to_node_40_chunk...\n",
      "\n",
      "3. åˆ†æ•¸: 1.0747\n",
      "   ä¾†æº: documents_dup_part_1_part_1_chunks/prod_documents_dup_part_1_part_1_path_merged_node_0_chunk_24_to_node_0_chunk_24.txt\n",
      "   å…§å®¹: Chunk path: ./chunks_output/documents_dup_part_1_part_1_chunks/prod_documents_dup_part_1_part_1_path...\n",
      "\n",
      "4. åˆ†æ•¸: 1.1020\n",
      "   ä¾†æº: test_ml_chunks/prod_test_ml_node_27_chunk_0.txt\n",
      "   å…§å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_node_27_chunk_0.txt\n",
      "Chunk name: prod_test_ml...\n",
      "\n",
      "5. åˆ†æ•¸: 1.1128\n",
      "   ä¾†æº: test_ml_chunks/prod_test_ml_node_66_chunk_0.txt\n",
      "   å…§å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_node_66_chunk_0.txt\n",
      "Chunk name: prod_test_ml...\n",
      "\n",
      "\n",
      "============================================================\n",
      "æ¸¬è©¦æ‰¹é‡æª¢ç´¢:\n",
      "è™•ç†æŸ¥è©¢ 1/2\n",
      "æ­£åœ¨æª¢ç´¢æŸ¥è©¢: 'äººå·¥æ™ºèƒ½çš„ç™¼å±•è¶¨å‹¢...' (top_k=5)\n",
      "æª¢ç´¢å®Œæˆï¼Œæ‰¾åˆ° 5 å€‹ç›¸é—œçµæœ\n",
      "è™•ç†æŸ¥è©¢ 2/2\n",
      "æ­£åœ¨æª¢ç´¢æŸ¥è©¢: 'æ©Ÿå™¨å­¸ç¿’ç®—æ³•...' (top_k=5)\n",
      "æª¢ç´¢å®Œæˆï¼Œæ‰¾åˆ° 5 å€‹ç›¸é—œçµæœ\n",
      "æŸ¥è©¢ 1: æ‰¾åˆ° 5 å€‹çµæœ\n",
      "æŸ¥è©¢ 2: æ‰¾åˆ° 5 å€‹çµæœ\n",
      "RAGæª¢ç´¢å™¨å·²é—œé–‰\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pymilvus import MilvusClient, DataType, CollectionSchema, FieldSchema\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "\n",
    "class MilvusLiteRAGRetriever:\n",
    "    def __init__(self, db_path=\"./milvus_data.db\", model_path=\"./embedding-model\"):\n",
    "        self.db_path = db_path\n",
    "        \n",
    "        # åŠ è¼‰æ¨¡å‹åˆ°å°æ‡‰è¨­å‚™\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(f\"RAGæª¢ç´¢å™¨ä½¿ç”¨è¨­å‚™: {self.device}\")\n",
    "        self.model = SentenceTransformer(model_path).to(self.device)\n",
    "        self.dimension = 768\n",
    "        \n",
    "        self.client = MilvusClient(db_path)\n",
    "        self.collection_name = \"testchunks\"\n",
    "        self.vector_field_name = \"vector\"\n",
    "        \n",
    "        # æª¢æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨\n",
    "        collections = self.client.list_collections()\n",
    "        if self.collection_name not in collections:\n",
    "            raise ValueError(f\"é›†åˆ '{self.collection_name}' ä¸å­˜åœ¨ï¼è«‹å…ˆé‹è¡Œæ•¸æ“šæ’å…¥ç¨‹åºã€‚\")\n",
    "        \n",
    "        print(f\"RAGæª¢ç´¢å™¨åˆå§‹åŒ–å®Œæˆï¼Œé€£æ¥åˆ°é›†åˆ: {self.collection_name}\")\n",
    "    \n",
    "    def retrieve_for_rag(self, query_text, top_k=10, score_threshold=None, filter_condition=None):\n",
    "        \"\"\"\n",
    "        ç‚º RAG ç³»çµ±æª¢ç´¢ç›¸é—œæ–‡æª”\n",
    "        \n",
    "        Args:\n",
    "            query_text (str): æŸ¥è©¢æ–‡æœ¬\n",
    "            top_k (int): è¿”å›çš„çµæœæ•¸é‡ï¼Œé»˜èª10\n",
    "            score_threshold (float, optional): åˆ†æ•¸é–¾å€¼ï¼Œä½æ–¼æ­¤åˆ†æ•¸çš„çµæœå°‡è¢«éæ¿¾\n",
    "            filter_condition (str, optional): éæ¿¾æ¢ä»¶ï¼Œä¾‹å¦‚ 'folder == \"æŸå€‹æ–‡ä»¶å¤¾\"'\n",
    "        \n",
    "        Returns:\n",
    "            dict: åŒ…å«æª¢ç´¢çµæœçš„å­—å…¸\n",
    "                - hits: æª¢ç´¢åˆ°çš„æ–‡æª”åˆ—è¡¨\n",
    "                - scores: å°æ‡‰çš„ç›¸ä¼¼åº¦åˆ†æ•¸åˆ—è¡¨\n",
    "                - query: åŸå§‹æŸ¥è©¢æ–‡æœ¬\n",
    "                - total_hits: ç¸½å‘½ä¸­æ•¸\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"æ­£åœ¨æª¢ç´¢æŸ¥è©¢: '{query_text[:50]}...' (top_k={top_k})\")\n",
    "            \n",
    "            # ç”ŸæˆæŸ¥è©¢åµŒå…¥å‘é‡\n",
    "            with torch.no_grad():\n",
    "                query_embedding = self.model.encode(\n",
    "                    [query_text], \n",
    "                    device=self.device,\n",
    "                    convert_to_numpy=True,\n",
    "                    show_progress_bar=False\n",
    "                )[0].tolist()\n",
    "            \n",
    "            # FLAT ç´¢å¼•æœå°‹åƒæ•¸\n",
    "            search_params = {\n",
    "                \"params\": {}\n",
    "            }\n",
    "            \n",
    "            # åŸ·è¡Œå‘é‡æœç´¢\n",
    "            search_results = self.client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                anns_field=self.vector_field_name,\n",
    "                data=[query_embedding],\n",
    "                filter=filter_condition,\n",
    "                limit=top_k,\n",
    "                output_fields=[\"text\", \"folder\", \"file\", \"timestamp\"],\n",
    "                search_params=search_params\n",
    "            )\n",
    "            \n",
    "            # è™•ç†æœç´¢çµæœ\n",
    "            hits = []\n",
    "            scores = []\n",
    "            \n",
    "            for hit in search_results[0]:\n",
    "                score = hit[\"distance\"]  # L2è·é›¢ï¼Œè¶Šå°è¶Šç›¸ä¼¼\n",
    "                \n",
    "                # å¦‚æœè¨­ç½®äº†åˆ†æ•¸é–¾å€¼ï¼Œéæ¿¾ä½åˆ†çµæœ\n",
    "                if score_threshold is not None and score > score_threshold:\n",
    "                    continue\n",
    "                \n",
    "                # æ§‹å»ºçµæœé …\n",
    "                hit_item = {\n",
    "                    \"id\": hit[\"id\"],\n",
    "                    \"text\": hit[\"entity\"][\"text\"],\n",
    "                    \"source\": f\"{hit['entity']['folder']}/{hit['entity']['file']}\",\n",
    "                    \"folder\": hit[\"entity\"][\"folder\"],\n",
    "                    \"file\": hit[\"entity\"][\"file\"],\n",
    "                    \"timestamp\": hit[\"entity\"][\"timestamp\"],\n",
    "                    \"score\": score\n",
    "                }\n",
    "                \n",
    "                hits.append(hit_item)\n",
    "                scores.append(score)\n",
    "            \n",
    "            # æ§‹å»ºè¿”å›çµæœ\n",
    "            result = {\n",
    "                \"hits\": hits,\n",
    "                \"scores\": scores,\n",
    "                \"query\": query_text,\n",
    "                \"total_hits\": len(hits),\n",
    "                \"top_k_requested\": top_k\n",
    "            }\n",
    "            \n",
    "            print(f\"æª¢ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(hits)} å€‹ç›¸é—œçµæœ\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"æª¢ç´¢éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "            return {\n",
    "                \"hits\": [],\n",
    "                \"scores\": [],\n",
    "                \"query\": query_text,\n",
    "                \"total_hits\": 0,\n",
    "                \"top_k_requested\": top_k,\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "    \n",
    "    def batch_retrieve(self, queries, top_k=10, score_threshold=None):\n",
    "        \"\"\"\n",
    "        æ‰¹é‡æª¢ç´¢å¤šå€‹æŸ¥è©¢\n",
    "        \n",
    "        Args:\n",
    "            queries (list): æŸ¥è©¢æ–‡æœ¬åˆ—è¡¨\n",
    "            top_k (int): æ¯å€‹æŸ¥è©¢è¿”å›çš„çµæœæ•¸é‡\n",
    "            score_threshold (float, optional): åˆ†æ•¸é–¾å€¼\n",
    "        \n",
    "        Returns:\n",
    "            list: æ¯å€‹æŸ¥è©¢çš„æª¢ç´¢çµæœåˆ—è¡¨\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for i, query in enumerate(queries):\n",
    "            print(f\"è™•ç†æŸ¥è©¢ {i+1}/{len(queries)}\")\n",
    "            result = self.retrieve_for_rag(query, top_k, score_threshold)\n",
    "            results.append(result)\n",
    "        return results\n",
    "    \n",
    "    def get_retriever_stats(self):\n",
    "        \"\"\"ç²å–æª¢ç´¢å™¨çµ±è¨ˆä¿¡æ¯\"\"\"\n",
    "        try:\n",
    "            stats = self.client.get_collection_stats(self.collection_name)\n",
    "            return {\n",
    "                \"collection_name\": self.collection_name,\n",
    "                \"stats\": stats,\n",
    "                \"device\": self.device,\n",
    "                \"model_dimension\": self.dimension\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"é—œé–‰æª¢ç´¢å™¨\"\"\"\n",
    "        self.client.close()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        print(\"RAGæª¢ç´¢å™¨å·²é—œé–‰\")\n",
    "\n",
    "\n",
    "# ä½¿ç”¨ç¤ºä¾‹å’Œæ¸¬è©¦å‡½æ•¸\n",
    "def test_rag_retriever():\n",
    "    \"\"\"æ¸¬è©¦ RAG æª¢ç´¢å™¨\"\"\"\n",
    "    try:\n",
    "        # åˆå§‹åŒ–æª¢ç´¢å™¨\n",
    "        retriever = MilvusLiteRAGRetriever(\"./milvus_data.db\", \"./embedding-model\")\n",
    "        \n",
    "        # ç²å–çµ±è¨ˆä¿¡æ¯\n",
    "        stats = retriever.get_retriever_stats()\n",
    "        print(f\"æª¢ç´¢å™¨çµ±è¨ˆ: {stats}\")\n",
    "        \n",
    "        # æ¸¬è©¦å–®å€‹æŸ¥è©¢\n",
    "        test_queries = [\n",
    "            \"äººå·¥æ™ºèƒ½çš„ç™¼å±•è¶¨å‹¢\",\n",
    "            \"æ©Ÿå™¨å­¸ç¿’ç®—æ³•\",\n",
    "            \"æ·±åº¦å­¸ç¿’æ‡‰ç”¨\",\n",
    "            \"è‡ªç„¶èªè¨€è™•ç†æŠ€è¡“\"\n",
    "        ]\n",
    "        \n",
    "        for query in test_queries:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"æ¸¬è©¦æŸ¥è©¢: {query}\")\n",
    "            \n",
    "            # åŸ·è¡Œæª¢ç´¢\n",
    "            result = retriever.retrieve_for_rag(query, top_k=10)\n",
    "            \n",
    "            if result[\"total_hits\"] > 0:\n",
    "                print(f\"æ‰¾åˆ° {result['total_hits']} å€‹ç›¸é—œçµæœ:\")\n",
    "                for i, hit in enumerate(result[\"hits\"][:5], 1):  # åªé¡¯ç¤ºå‰5å€‹\n",
    "                    print(f\"{i}. åˆ†æ•¸: {hit['score']:.4f}\")\n",
    "                    print(f\"   ä¾†æº: {hit['source']}\")\n",
    "                    print(f\"   å…§å®¹: {hit['text'][:100]}...\")\n",
    "                    print()\n",
    "            else:\n",
    "                print(\"æœªæ‰¾åˆ°ç›¸é—œçµæœ\")\n",
    "        \n",
    "        # æ¸¬è©¦æ‰¹é‡æª¢ç´¢\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"æ¸¬è©¦æ‰¹é‡æª¢ç´¢:\")\n",
    "        batch_results = retriever.batch_retrieve(test_queries[:2], top_k=5)\n",
    "        for i, result in enumerate(batch_results):\n",
    "            print(f\"æŸ¥è©¢ {i+1}: æ‰¾åˆ° {result['total_hits']} å€‹çµæœ\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"æ¸¬è©¦éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        if 'retriever' in locals():\n",
    "            retriever.close()\n",
    "\n",
    "\n",
    "# ç°¡åŒ–çš„ RAG æª¢ç´¢å‡½æ•¸ï¼ˆå¯ç›´æ¥å°å…¥ä½¿ç”¨ï¼‰\n",
    "def rag_retrieve(query_text, top_k=10, db_path=\"./milvus_data.db\", model_path=\"./embedding-model\"):\n",
    "    \"\"\"\n",
    "    ç°¡åŒ–çš„ RAG æª¢ç´¢å‡½æ•¸ï¼Œå¯ä»¥ç›´æ¥å°å…¥ä½¿ç”¨\n",
    "    \n",
    "    Args:\n",
    "        query_text (str): æŸ¥è©¢æ–‡æœ¬\n",
    "        top_k (int): è¿”å›çµæœæ•¸é‡\n",
    "        db_path (str): Milvus æ•¸æ“šåº«è·¯å¾‘\n",
    "        model_path (str): åµŒå…¥æ¨¡å‹è·¯å¾‘\n",
    "    \n",
    "    Returns:\n",
    "        dict: æª¢ç´¢çµæœ\n",
    "    \"\"\"\n",
    "    retriever = None\n",
    "    try:\n",
    "        retriever = MilvusLiteRAGRetriever(db_path, model_path)\n",
    "        result = retriever.retrieve_for_rag(query_text, top_k)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"hits\": [],\n",
    "            \"scores\": [],\n",
    "            \"query\": query_text,\n",
    "            \"total_hits\": 0,\n",
    "            \"error\": str(e)\n",
    "        }\n",
    "    finally:\n",
    "        if retriever:\n",
    "            retriever.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # é‹è¡Œæ¸¬è©¦\n",
    "    test_rag_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38944cd-07e8-4668-a22a-8842cf083517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.exceptions import RequestError, ConnectionError\n",
    "\n",
    "class ElasticsearchSearcher:\n",
    "    def __init__(self, \n",
    "                 es_url=\"https://localhost:9200\", \n",
    "                 username=\"elastic\", \n",
    "                 password=\"\", \n",
    "                 index_name=\"chunk_documents\"):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–æœç´¢å™¨\n",
    "        :param es_url: Elasticsearch åœ°å€\n",
    "        :param username: ç”¨æˆ·åï¼ˆé»˜è®¤ elasticï¼‰\n",
    "        :param password: å¯†ç \n",
    "        :param index_name: è¦æœç´¢çš„ç´¢å¼•åï¼ˆé»˜è®¤ä¸å¯¼å…¥æ—¶ä¸€è‡´ï¼‰\n",
    "        \"\"\"\n",
    "        self.es_url = es_url\n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.index_name = index_name\n",
    "        self.es = self._connect()\n",
    "\n",
    "    def _connect(self):\n",
    "        \"\"\"å»ºç«‹ä¸ Elasticsearch çš„è¿æ¥\"\"\"\n",
    "        try:\n",
    "            es = Elasticsearch(\n",
    "                [self.es_url],\n",
    "                basic_auth=(self.username, self.password),\n",
    "                verify_certs=False,  # ä¿æŒä¸ä¹‹å‰ä¸€è‡´çš„ SSL é…ç½®\n",
    "                ssl_show_warn=False\n",
    "            )\n",
    "            # éªŒè¯è¿æ¥\n",
    "            if es.ping():\n",
    "                print(f\"âœ… å·²è¿æ¥åˆ° Elasticsearchï¼š{self.es_url}\")\n",
    "                return es\n",
    "            else:\n",
    "                print(\"âŒ è¿æ¥å¤±è´¥ï¼šElasticsearch æœªå“åº”\")\n",
    "                return None\n",
    "        except ConnectionError:\n",
    "            print(f\"âŒ æ— æ³•è¿æ¥åˆ° {self.es_url}ï¼Œè¯·æ£€æŸ¥æœåŠ¡æ˜¯å¦å¯åŠ¨\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ è¿æ¥é”™è¯¯ï¼š{e}\")\n",
    "            return None\n",
    "\n",
    "    def search(self, keyword, size=10, preview_length=200):\n",
    "        \"\"\"\n",
    "        æœç´¢åŒ…å«å…³é”®è¯çš„æ–‡æ¡£\n",
    "        :param keyword: æœç´¢å…³é”®è¯ï¼ˆå­—ç¬¦ä¸²ï¼‰\n",
    "        :param size: è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤10æ¡ï¼‰\n",
    "        :param preview_length: å†…å®¹é¢„è§ˆé•¿åº¦ï¼ˆé»˜è®¤200å­—ç¬¦ï¼‰\n",
    "        :return: æ ¼å¼åŒ–çš„æœç´¢ç»“æœåˆ—è¡¨\n",
    "        \"\"\"\n",
    "        if not self.es:\n",
    "            print(\"âš ï¸ æœªå»ºç«‹æœ‰æ•ˆè¿æ¥ï¼Œè¯·æ£€æŸ¥é…ç½®\")\n",
    "            return []\n",
    "\n",
    "        try:\n",
    "            # æ„é€ æœç´¢æŸ¥è¯¢ï¼ˆåœ¨ content å­—æ®µä¸­æœç´¢å…³é”®è¯ï¼‰\n",
    "            query = {\n",
    "                \"query\": {\n",
    "                    \"match\": {\n",
    "                        \"content\": keyword  # æœç´¢å†…å®¹å­—æ®µ\n",
    "                    }\n",
    "                },\n",
    "                \"size\": size\n",
    "            }\n",
    "\n",
    "            # æ‰§è¡Œæœç´¢\n",
    "            response = self.es.search(index=self.index_name, body=query)\n",
    "            hits = response[\"hits\"][\"hits\"]\n",
    "            total = response[\"hits\"][\"total\"][\"value\"]\n",
    "\n",
    "            print(f\"\\nğŸ” æœç´¢å…³é”®è¯ï¼š'{keyword}'ï¼Œæ‰¾åˆ° {total} æ¡åŒ¹é…ç»“æœï¼ˆæ˜¾ç¤ºå‰ {len(hits)} æ¡ï¼‰\")\n",
    "\n",
    "            # æ ¼å¼åŒ–ç»“æœ\n",
    "            results = []\n",
    "            for hit in hits:\n",
    "                source = hit[\"_source\"]\n",
    "                results.append({\n",
    "                    \"id\": hit[\"_id\"],\n",
    "                    \"filename\": source[\"filename\"],\n",
    "                    \"folder\": source[\"folder\"],\n",
    "                    \"score\": hit[\"_score\"],  # åŒ¹é…å¾—åˆ†ï¼ˆè¶Šé«˜è¶Šç›¸å…³ï¼‰\n",
    "                    \"content_preview\": source[\"content\"][:preview_length] + \"...\" if len(source[\"content\"]) > preview_length else source[\"content\"],\n",
    "                    \"import_time\": source[\"import_time\"]\n",
    "                })\n",
    "\n",
    "            return results\n",
    "\n",
    "        except RequestError as e:\n",
    "            print(f\"âŒ æœç´¢è¯·æ±‚é”™è¯¯ï¼š{e}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ æœç´¢å¤±è´¥ï¼š{e}\")\n",
    "            return []\n",
    "\n",
    "    def print_results(self, results):\n",
    "        \"\"\"æ ¼å¼åŒ–æ‰“å°æœç´¢ç»“æœ\"\"\"\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\n--- ç»“æœ {i} ---\")\n",
    "            print(f\"æ–‡ä»¶åï¼š{result['filename']}\")\n",
    "            print(f\"æ–‡ä»¶å¤¹ï¼š{result['folder']}\")\n",
    "            print(f\"ç›¸å…³åº¦ï¼š{result['score']:.2f}\")\n",
    "            print(f\"å†…å®¹é¢„è§ˆï¼š{result['content_preview']}\")\n",
    "            print(f\"å¯¼å…¥æ—¶é—´ï¼š{result['import_time']}\")\n",
    "\n",
    "\n",
    "# ç¤ºä¾‹ç”¨æ³•\n",
    "if __name__ == \"__main__\":\n",
    "    # åˆå§‹åŒ–æœç´¢å™¨ï¼ˆæ›¿æ¢ä¸ºä½ çš„å¯†ç ï¼‰\n",
    "    searcher = ElasticsearchSearcher(\n",
    "        password=\"vSCQnhBXoox0sRo7-U1x\",  # ä½ çš„ Elasticsearch å¯†ç \n",
    "        index_name=\"chunk_documents\"       # ä¸å¯¼å…¥æ—¶çš„ç´¢å¼•åä¸€è‡´\n",
    "    )\n",
    "\n",
    "    # æœç´¢ç¤ºä¾‹ï¼ˆæ›¿æ¢ä¸ºä½ æƒ³æœç´¢çš„å…³é”®è¯ï¼‰\n",
    "    keyword = \"äººå·¥æ™ºèƒ½\"  # ä¾‹å¦‚ \"äººå·¥æ™ºèƒ½\"ã€\"æ•°æ®åˆ†æ\" ç­‰\n",
    "    search_results = searcher.search(keyword, size=5)  # æœç´¢å¹¶è¿”å›å‰5æ¡ç»“æœ\n",
    "\n",
    "    # æ‰“å°ç»“æœ\n",
    "    searcher.print_results(search_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1577f12a-744b-4a55-8296-8a2955b6d70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:ä½¿ç”¨è®¾å¤‡: cuda | æœ€å¤§åºåˆ—é•¿åº¦: 4096\n",
      "INFO:__main__:BCEåµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: ./embedding-model\n",
      "INFO:__main__:BCEé‡æ’åºæ¨¡å‹åŠ è½½æˆåŠŸ: ./reranker-model\n",
      "INFO:__main__:Milvus è¿æ¥æˆåŠŸ: testchunks\n",
      "INFO:elastic_transport.transport:HEAD https://localhost:9200/ [status:200 duration:0.015s]\n",
      "INFO:__main__:Elasticsearch è¿æ¥æˆåŠŸ: https://localhost:9200\n",
      "INFO:__main__:æ··åˆæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\n",
      "INFO:__main__:å¼€å§‹æ··åˆæ£€ç´¢: 'äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿...'\n",
      "INFO:__main__:å‘é‡æœç´¢æ‰¾åˆ° 50 ä¸ªç»“æœ\n",
      "INFO:elastic_transport.transport:POST https://localhost:9200/chunk_documents/_search [status:200 duration:0.018s]\n",
      "INFO:__main__:å…³é”®è¯æœç´¢æ‰¾åˆ° 50 ä¸ªç»“æœ\n",
      "INFO:__main__:åˆå¹¶åå…±æœ‰ 6 ä¸ªå”¯ä¸€ç»“æœ\n",
      "ERROR:__main__:é‡æ’åºå¤±è´¥: The expanded size of the tensor (2962) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [6, 2962].  Tensor sizes: [1, 514]\n",
      "INFO:__main__:æ··åˆæ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° 5 ä¸ªç»“æœï¼Œè€—æ—¶ 0.07 ç§’\n",
      "INFO:__main__:å¼€å§‹æ··åˆæ£€ç´¢: 'äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿...'\n",
      "INFO:__main__:å‘é‡æœç´¢æ‰¾åˆ° 50 ä¸ªç»“æœ\n",
      "INFO:elastic_transport.transport:POST https://localhost:9200/chunk_documents/_search [status:200 duration:0.012s]\n",
      "INFO:__main__:å…³é”®è¯æœç´¢æ‰¾åˆ° 50 ä¸ªç»“æœ\n",
      "INFO:__main__:åˆå¹¶åå…±æœ‰ 6 ä¸ªå”¯ä¸€ç»“æœ\n",
      "INFO:__main__:æ··åˆæ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° 3 ä¸ªç»“æœï¼Œè€—æ—¶ 0.04 ç§’\n",
      "INFO:__main__:å¼€å§‹æ··åˆæ£€ç´¢: 'æœºå™¨å­¦ä¹ ç®—æ³•åŸç†...'\n",
      "INFO:__main__:å‘é‡æœç´¢æ‰¾åˆ° 50 ä¸ªç»“æœ\n",
      "INFO:elastic_transport.transport:POST https://localhost:9200/chunk_documents/_search [status:200 duration:0.010s]\n",
      "INFO:__main__:å…³é”®è¯æœç´¢æ‰¾åˆ° 50 ä¸ªç»“æœ\n",
      "INFO:__main__:åˆå¹¶åå…±æœ‰ 28 ä¸ªå”¯ä¸€ç»“æœ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================================================================================================\n",
      "æµ‹è¯•æŸ¥è¯¢: äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿\n",
      "\n",
      "ã€å¯ç”¨é‡æ’åºã€‘\n",
      "\n",
      "================================================================================\n",
      "æŸ¥è¯¢: äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿\n",
      "æ‰§è¡Œæ—¶é—´: 0.07 ç§’\n",
      "å‘é‡æœç´¢ç»“æœ: 50 ä¸ª\n",
      "å…³é”®è¯æœç´¢ç»“æœ: 50 ä¸ª\n",
      "åˆå¹¶åç»“æœ: 6 ä¸ª\n",
      "æœ€ç»ˆè¿”å›: 5 ä¸ª\n",
      "é‡æ’åº: å·²å¯ç”¨\n",
      "================================================================================\n",
      "\n",
      "--- ç»“æœ 1 ---\n",
      "æ¥æº: documents_dup_part_1_part_1_chunks/prod_documents_dup_part_1_part_1_path_merged_node_0_chunk_92_to_node_0_chunk_92.txt\n",
      "æœç´¢ç±»å‹: vector\n",
      "åˆ†æ•°: 0.5393\n",
      "å†…å®¹: Chunk path: ./chunks_output/documents_dup_part_1_part_1_chunks/prod_documents_dup_part_1_part_1_path_merged_node_0_chunk_92_to_node_0_chunk_92.txt\n",
      "Chunk name: prod_documents_dup_part_1_part_1_path_merged_node_0_chunk_92_to_node_0_chunk_92.txt\n",
      "==================================================\n",
      "Chunk ...\n",
      "\n",
      "--- ç»“æœ 2 ---\n",
      "æ¥æº: test_ml_chunks/prod_test_ml_node_80_chunk_0.txt\n",
      "æœç´¢ç±»å‹: vector\n",
      "åˆ†æ•°: 0.4966\n",
      "å†…å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_node_80_chunk_0.txt\n",
      "Chunk name: prod_test_ml_node_80_chunk_0.txt\n",
      "==================================================\n",
      "Chunk ID: node_80_chunk_0\n",
      "Source Node: 80\n",
      "Chunk Index: 0\n",
      "Path Titles: AI ä¸å…¶ä»–å­¦ç§‘èåˆï¼šä¸ç‰©ç†å­¦ï¼ˆå¦‚é‡å­æœºå™¨å­¦ä¹ ï¼Œæå‡è®¡ç®—é€Ÿåº¦ï¼‰ã€ç”Ÿç‰©å­¦ï¼ˆå¦‚ç”Ÿç‰©ä¿¡æ¯å­¦ä¸­çš„åŸºå› åºåˆ—åˆ†æï¼‰ã€ç¤¾ä¼šå­¦ï¼ˆå¦‚ç¤¾...\n",
      "\n",
      "--- ç»“æœ 3 ---\n",
      "æ¥æº: test_ml_chunks/prod_test_ml_path_merged_node_76_chunk_0_to_node_79_chunk_0.txt\n",
      "æœç´¢ç±»å‹: vector\n",
      "åˆ†æ•°: 0.4914\n",
      "å†…å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_path_merged_node_76_chunk_0_to_node_79_chunk_0.txt\n",
      "Chunk name: prod_test_ml_path_merged_node_76_chunk_0_to_node_79_chunk_0.txt\n",
      "==================================================\n",
      "Chunk ID: path_merged_node_76_chunk_0_to_node_79_chunk_0\n",
      "Source No...\n",
      "\n",
      "--- ç»“æœ 4 ---\n",
      "æ¥æº: test_ml_chunks/prod_test_ml_node_82_chunk_0.txt\n",
      "æœç´¢ç±»å‹: vector\n",
      "åˆ†æ•°: 0.4876\n",
      "å†…å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_node_82_chunk_0.txt\n",
      "Chunk name: prod_test_ml_node_82_chunk_0.txt\n",
      "==================================================\n",
      "Chunk ID: node_82_chunk_0\n",
      "Source Node: 82\n",
      "Chunk Index: 0\n",
      "Path Titles: 6. æ€»ç»“\n",
      "==================================================\n",
      "6...\n",
      "\n",
      "--- ç»“æœ 5 ---\n",
      "æ¥æº: test_ml_chunks/prod_test_ml_path_merged_node_71_chunk_0_to_node_74_chunk_0.txt\n",
      "æœç´¢ç±»å‹: vector\n",
      "åˆ†æ•°: 0.4846\n",
      "å†…å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_path_merged_node_71_chunk_0_to_node_74_chunk_0.txt\n",
      "Chunk name: prod_test_ml_path_merged_node_71_chunk_0_to_node_74_chunk_0.txt\n",
      "==================================================\n",
      "Chunk ID: path_merged_node_71_chunk_0_to_node_74_chunk_0\n",
      "Source No...\n",
      "\n",
      "ã€ç¦ç”¨é‡æ’åºã€‘\n",
      "1. æ··åˆåˆ†æ•°: 0.3236 | æ¥æº: documents_dup_part_1_part_1_chunks/prod_documents_dup_part_1_part_1_path_merged_node_0_chunk_92_to_node_0_chunk_92.txt\n",
      "2. æ··åˆåˆ†æ•°: 0.2979 | æ¥æº: test_ml_chunks/prod_test_ml_node_80_chunk_0.txt\n",
      "3. æ··åˆåˆ†æ•°: 0.2948 | æ¥æº: test_ml_chunks/prod_test_ml_path_merged_node_76_chunk_0_to_node_79_chunk_0.txt\n",
      "\n",
      "====================================================================================================\n",
      "æµ‹è¯•æŸ¥è¯¢: æœºå™¨å­¦ä¹ ç®—æ³•åŸç†\n",
      "\n",
      "ã€å¯ç”¨é‡æ’åºã€‘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:__main__:é‡æ’åºå¤±è´¥: The expanded size of the tensor (3380) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [28, 3380].  Tensor sizes: [1, 514]\n",
      "INFO:__main__:æ··åˆæ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° 5 ä¸ªç»“æœï¼Œè€—æ—¶ 0.11 ç§’\n",
      "INFO:__main__:å¼€å§‹æ··åˆæ£€ç´¢: 'æœºå™¨å­¦ä¹ ç®—æ³•åŸç†...'\n",
      "INFO:__main__:å‘é‡æœç´¢æ‰¾åˆ° 50 ä¸ªç»“æœ\n",
      "INFO:elastic_transport.transport:POST https://localhost:9200/chunk_documents/_search [status:200 duration:0.016s]\n",
      "INFO:__main__:å…³é”®è¯æœç´¢æ‰¾åˆ° 50 ä¸ªç»“æœ\n",
      "INFO:__main__:åˆå¹¶åå…±æœ‰ 28 ä¸ªå”¯ä¸€ç»“æœ\n",
      "INFO:__main__:æ··åˆæ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° 3 ä¸ªç»“æœï¼Œè€—æ—¶ 0.04 ç§’\n",
      "INFO:__main__:ä½¿ç”¨è®¾å¤‡: cuda | æœ€å¤§åºåˆ—é•¿åº¦: 4096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "æŸ¥è¯¢: æœºå™¨å­¦ä¹ ç®—æ³•åŸç†\n",
      "æ‰§è¡Œæ—¶é—´: 0.11 ç§’\n",
      "å‘é‡æœç´¢ç»“æœ: 50 ä¸ª\n",
      "å…³é”®è¯æœç´¢ç»“æœ: 50 ä¸ª\n",
      "åˆå¹¶åç»“æœ: 28 ä¸ª\n",
      "æœ€ç»ˆè¿”å›: 5 ä¸ª\n",
      "é‡æ’åº: å·²å¯ç”¨\n",
      "================================================================================\n",
      "\n",
      "--- ç»“æœ 1 ---\n",
      "æ¥æº: test_ml_chunks/prod_test_ml_path_merged_node_11_chunk_0_to_node_15_chunk_0.txt\n",
      "æœç´¢ç±»å‹: vector\n",
      "åˆ†æ•°: 0.6455\n",
      "å†…å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_path_merged_node_11_chunk_0_to_node_15_chunk_0.txt\n",
      "Chunk name: prod_test_ml_path_merged_node_11_chunk_0_to_node_15_chunk_0.txt\n",
      "==================================================\n",
      "Chunk ID: path_merged_node_11_chunk_0_to_node_15_chunk_0\n",
      "Source No...\n",
      "\n",
      "--- ç»“æœ 2 ---\n",
      "æ¥æº: test_ml_chunks/prod_test_ml_node_26_chunk_0.txt\n",
      "æœç´¢ç±»å‹: vector\n",
      "åˆ†æ•°: 0.5994\n",
      "å†…å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_node_26_chunk_0.txt\n",
      "Chunk name: prod_test_ml_node_26_chunk_0.txt\n",
      "==================================================\n",
      "Chunk ID: node_26_chunk_0\n",
      "Source Node: 26\n",
      "Chunk Index: 0\n",
      "Path Titles: 3. æœºå™¨å­¦ä¹ çš„ä¸»è¦åˆ†ç±» > 3.1 ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰ > 3.1.1 å®šä¹‰ä¸ç‰¹ç‚¹...\n",
      "\n",
      "--- ç»“æœ 3 ---\n",
      "æ¥æº: test_ml_chunks/prod_test_ml_node_27_chunk_0.txt\n",
      "æœç´¢ç±»å‹: vector\n",
      "åˆ†æ•°: 0.5955\n",
      "å†…å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_node_27_chunk_0.txt\n",
      "Chunk name: prod_test_ml_node_27_chunk_0.txt\n",
      "==================================================\n",
      "Chunk ID: node_27_chunk_0\n",
      "Source Node: 27\n",
      "Chunk Index: 0\n",
      "Path Titles: 3. æœºå™¨å­¦ä¹ çš„ä¸»è¦åˆ†ç±» > 3.1 ç›‘ç£å­¦ä¹ ï¼ˆSupervised Learningï¼‰ > 3.1.2 å¸¸è§ä»»åŠ¡ä¸...\n",
      "\n",
      "--- ç»“æœ 4 ---\n",
      "æ¥æº: test_ml_chunks/prod_test_ml_node_23_chunk_0.txt\n",
      "æœç´¢ç±»å‹: vector\n",
      "åˆ†æ•°: 0.5904\n",
      "å†…å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_node_23_chunk_0.txt\n",
      "Chunk name: prod_test_ml_node_23_chunk_0.txt\n",
      "==================================================\n",
      "Chunk ID: node_23_chunk_0\n",
      "Source Node: 23\n",
      "Chunk Index: 0\n",
      "Path Titles: 3. æœºå™¨å­¦ä¹ çš„ä¸»è¦åˆ†ç±»\n",
      "=============================================...\n",
      "\n",
      "--- ç»“æœ 5 ---\n",
      "æ¥æº: test_ml_chunks/prod_test_ml_node_3_chunk_0.txt\n",
      "æœç´¢ç±»å‹: vector\n",
      "åˆ†æ•°: 0.5871\n",
      "å†…å®¹: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_node_3_chunk_0.txt\n",
      "Chunk name: prod_test_ml_node_3_chunk_0.txt\n",
      "==================================================\n",
      "Chunk ID: node_3_chunk_0\n",
      "Source Node: 3\n",
      "Chunk Index: 0\n",
      "Path Titles: 1. å¼•è¨€ > 1.1 æœºå™¨å­¦ä¹ çš„å®šä¹‰\n",
      "==========================================...\n",
      "\n",
      "ã€ç¦ç”¨é‡æ’åºã€‘\n",
      "1. æ··åˆåˆ†æ•°: 0.3873 | æ¥æº: test_ml_chunks/prod_test_ml_path_merged_node_11_chunk_0_to_node_15_chunk_0.txt\n",
      "2. æ··åˆåˆ†æ•°: 0.3596 | æ¥æº: test_ml_chunks/prod_test_ml_node_26_chunk_0.txt\n",
      "3. æ··åˆåˆ†æ•°: 0.3573 | æ¥æº: test_ml_chunks/prod_test_ml_node_27_chunk_0.txt\n",
      "\n",
      "====================================================================================================\n",
      "æµ‹è¯•ç‹¬ç«‹æœç´¢å‡½æ•°:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:BCEåµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: ./embedding-model\n",
      "INFO:__main__:BCEé‡æ’åºæ¨¡å‹åŠ è½½æˆåŠŸ: ./reranker-model\n",
      "INFO:__main__:Milvus è¿æ¥æˆåŠŸ: testchunks\n",
      "INFO:elastic_transport.transport:HEAD https://localhost:9200/ [status:200 duration:0.015s]\n",
      "INFO:__main__:Elasticsearch è¿æ¥æˆåŠŸ: https://localhost:9200\n",
      "INFO:__main__:æ··åˆæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\n",
      "INFO:__main__:å¼€å§‹æ··åˆæ£€ç´¢: 'äººå·¥æ™ºèƒ½æŠ€æœ¯åº”ç”¨åœºæ™¯...'\n",
      "INFO:__main__:å‘é‡æœç´¢æ‰¾åˆ° 50 ä¸ªç»“æœ\n",
      "INFO:elastic_transport.transport:POST https://localhost:9200/chunk_documents/_search [status:200 duration:0.017s]\n",
      "INFO:__main__:å…³é”®è¯æœç´¢æ‰¾åˆ° 50 ä¸ªç»“æœ\n",
      "INFO:__main__:åˆå¹¶åå…±æœ‰ 13 ä¸ªå”¯ä¸€ç»“æœ\n",
      "ERROR:__main__:é‡æ’åºå¤±è´¥: The expanded size of the tensor (2963) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [13, 2963].  Tensor sizes: [1, 514]\n",
      "INFO:__main__:æ··åˆæ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° 3 ä¸ªç»“æœï¼Œè€—æ—¶ 0.07 ç§’\n",
      "INFO:__main__:GPUç¼“å­˜å·²æ¸…ç†\n",
      "INFO:__main__:æ··åˆæ£€ç´¢ç³»ç»Ÿå·²å…³é—­\n",
      "INFO:__main__:GPUç¼“å­˜å·²æ¸…ç†\n",
      "INFO:__main__:æ··åˆæ£€ç´¢ç³»ç»Ÿå·²å…³é—­\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç‹¬ç«‹å‡½æ•°æœç´¢ç»“æœ: æ‰¾åˆ° 3 ä¸ªç»“æœ\n",
      "1. åˆ†æ•°: 0.5325 | æ¥æº: documents_dup_part_1_part_1_chunks/prod_documents_dup_part_1_part_1_path_merged_node_0_chunk_92_to_node_0_chunk_92.txt\n",
      "2. åˆ†æ•°: 0.5137 | æ¥æº: test_ml_chunks/prod_test_ml_node_66_chunk_0.txt\n",
      "3. åˆ†æ•°: 0.5115 | æ¥æº: test_ml_chunks/prod_test_ml_node_80_chunk_0.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from pymilvus import MilvusClient\n",
    "from elasticsearch import Elasticsearch\n",
    "import json\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# é…ç½®æ—¥å¿—\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class HybridRAGSearcher:\n",
    "    \"\"\"\n",
    "    æ··åˆæ£€ç´¢ä¸é‡æ’åºç³»ç»Ÿ\n",
    "    ç»“åˆ Milvus å‘é‡æœç´¢ã€Elasticsearch å…³é”®è¯æœç´¢ï¼ŒåŸºäº transformers åŠ è½½ BCE ç³»åˆ—æ¨¡å‹\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 milvus_db_path=\"./milvus_data.db\",\n",
    "                 embedding_model_name_or_path=\"maidalun1020/bce-embedding-base_v1\",\n",
    "                 reranker_model_name_or_path=\"maidalun1020/bce-reranker-base_v1\",\n",
    "                 es_url=\"https://localhost:9200\",\n",
    "                 es_username=\"elastic\",\n",
    "                 es_password=\"\",\n",
    "                 es_index_name=\"chunk_documents\",\n",
    "                 milvus_collection_name=\"testchunks\",\n",
    "                 max_seq_length: int = 512):\n",
    "        \"\"\"åˆå§‹åŒ–æ··åˆæ£€ç´¢ç³»ç»Ÿ\"\"\"\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.max_seq_length = max_seq_length\n",
    "        logger.info(f\"ä½¿ç”¨è®¾å¤‡: {self.device} | æœ€å¤§åºåˆ—é•¿åº¦: {self.max_seq_length}\")\n",
    "        \n",
    "        # åˆå§‹åŒ–æ¨¡å‹\n",
    "        self.embedding_tokenizer, self.embedding_model = self._init_embedding_model(embedding_model_name_or_path)\n",
    "        self.reranker_tokenizer, self.reranker_model = self._init_reranker_model(reranker_model_name_or_path)\n",
    "        \n",
    "        # åˆå§‹åŒ– Milvus\n",
    "        self._init_milvus(milvus_db_path, milvus_collection_name)\n",
    "        \n",
    "        # åˆå§‹åŒ– Elasticsearch\n",
    "        self._init_elasticsearch(es_url, es_username, es_password, es_index_name)\n",
    "        \n",
    "        logger.info(\"æ··åˆæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\")\n",
    "    \n",
    "    def _init_embedding_model(self, model_name_or_path: str):\n",
    "        \"\"\"åˆå§‹åŒ– BCE åµŒå…¥æ¨¡å‹\"\"\"\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "            model = AutoModel.from_pretrained(model_name_or_path)\n",
    "            model = model.to(self.device)\n",
    "            model.eval()\n",
    "            logger.info(f\"BCEåµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ: {model_name_or_path}\")\n",
    "            return tokenizer, model\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _init_reranker_model(self, model_name_or_path: str):\n",
    "        \"\"\"åˆå§‹åŒ– BCE é‡æ’åºæ¨¡å‹\"\"\"\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\n",
    "            model = model.to(self.device)\n",
    "            model.eval()\n",
    "            logger.info(f\"BCEé‡æ’åºæ¨¡å‹åŠ è½½æˆåŠŸ: {model_name_or_path}\")\n",
    "            return tokenizer, model\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"é‡æ’åºæ¨¡å‹åŠ è½½å¤±è´¥: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _init_milvus(self, db_path, collection_name):\n",
    "        \"\"\"åˆå§‹åŒ– Milvus è¿æ¥\"\"\"\n",
    "        try:\n",
    "            self.milvus_client = MilvusClient(db_path)\n",
    "            self.milvus_collection_name = collection_name\n",
    "            \n",
    "            collections = self.milvus_client.list_collections()\n",
    "            if collection_name not in collections:\n",
    "                raise ValueError(f\"Milvus é›†åˆ '{collection_name}' ä¸å­˜åœ¨\")\n",
    "            \n",
    "            logger.info(f\"Milvus è¿æ¥æˆåŠŸ: {collection_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Milvus åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "            self.milvus_client = None\n",
    "    \n",
    "    def _init_elasticsearch(self, es_url, username, password, index_name):\n",
    "        \"\"\"åˆå§‹åŒ– Elasticsearch è¿æ¥\"\"\"\n",
    "        try:\n",
    "            self.es_client = Elasticsearch(\n",
    "                [es_url],\n",
    "                basic_auth=(username, password),\n",
    "                verify_certs=False,\n",
    "                ssl_show_warn=False\n",
    "            )\n",
    "            \n",
    "            self.es_index_name = index_name\n",
    "            \n",
    "            if self.es_client.ping():\n",
    "                logger.info(f\"Elasticsearch è¿æ¥æˆåŠŸ: {es_url}\")\n",
    "            else:\n",
    "                logger.warning(\"Elasticsearch è¿æ¥å¤±è´¥\")\n",
    "                self.es_client = None\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Elasticsearch åˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "            self.es_client = None\n",
    "    \n",
    "    def _generate_embedding(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"ç”Ÿæˆæ–‡æœ¬åµŒå…¥\"\"\"\n",
    "        with torch.no_grad():\n",
    "            inputs = self.embedding_tokenizer(\n",
    "                texts,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_seq_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            outputs = self.embedding_model(** inputs)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "            normalized_embeddings = cls_embeddings / cls_embeddings.norm(dim=1, keepdim=True)\n",
    "            \n",
    "            return normalized_embeddings.cpu().numpy()\n",
    "    \n",
    "    def _vector_search(self, query_text: str, top_k: int = 20) -> List[Dict]:\n",
    "        \"\"\"æ‰§è¡Œå‘é‡æœç´¢\"\"\"\n",
    "        if not self.milvus_client:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            query_embedding = self._generate_embedding([query_text])[0].tolist()\n",
    "            \n",
    "            search_results = self.milvus_client.search(\n",
    "                collection_name=self.milvus_collection_name,\n",
    "                anns_field=\"vector\",\n",
    "                data=[query_embedding],\n",
    "                limit=top_k,\n",
    "                output_fields=[\"text\", \"folder\", \"file\", \"timestamp\"],\n",
    "                search_params={\"params\": {}}\n",
    "            )\n",
    "            \n",
    "            results = []\n",
    "            for hit in search_results[0]:\n",
    "                results.append({\n",
    "                    \"id\": f\"milvus_{hit['id']}\",\n",
    "                    \"text\": hit[\"entity\"][\"text\"],\n",
    "                    \"source\": f\"{hit['entity']['folder']}/{hit['entity']['file']}\",\n",
    "                    \"folder\": hit[\"entity\"][\"folder\"],\n",
    "                    \"file\": hit[\"entity\"][\"file\"],\n",
    "                    \"timestamp\": hit[\"entity\"][\"timestamp\"],\n",
    "                    \"score\": 1.0 / (1.0 + hit[\"distance\"]),\n",
    "                    \"search_type\": \"vector\"\n",
    "                })\n",
    "            \n",
    "            logger.info(f\"å‘é‡æœç´¢æ‰¾åˆ° {len(results)} ä¸ªç»“æœ\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"å‘é‡æœç´¢å¤±è´¥: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _keyword_search(self, query_text: str, top_k: int = 20) -> List[Dict]:\n",
    "        \"\"\"æ‰§è¡Œå…³é”®è¯æœç´¢\"\"\"\n",
    "        if not self.es_client:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            query = {\n",
    "                \"query\": {\n",
    "                    \"match\": {\n",
    "                        \"content\": query_text\n",
    "                    }\n",
    "                },\n",
    "                \"size\": top_k\n",
    "            }\n",
    "            \n",
    "            response = self.es_client.search(index=self.es_index_name, body=query)\n",
    "            hits = response[\"hits\"][\"hits\"]\n",
    "            \n",
    "            results = []\n",
    "            for hit in hits:\n",
    "                source = hit[\"_source\"]\n",
    "                results.append({\n",
    "                    \"id\": f\"es_{hit['_id']}\",\n",
    "                    \"text\": source[\"content\"],\n",
    "                    \"source\": f\"{source['folder']}/{source['filename']}\",\n",
    "                    \"folder\": source[\"folder\"],\n",
    "                    \"file\": source[\"filename\"],\n",
    "                    \"timestamp\": source.get(\"import_time\", \"\"),\n",
    "                    \"score\": hit[\"_score\"] / 100.0,\n",
    "                    \"search_type\": \"keyword\"\n",
    "                })\n",
    "            \n",
    "            logger.info(f\"å…³é”®è¯æœç´¢æ‰¾åˆ° {len(results)} ä¸ªç»“æœ\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"å…³é”®è¯æœç´¢å¤±è´¥: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _merge_and_deduplicate(self, vector_results: List[Dict], keyword_results: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"åˆå¹¶å¹¶å»é‡æœç´¢ç»“æœ\"\"\"\n",
    "        seen_texts = set()\n",
    "        merged_results = []\n",
    "        \n",
    "        for result in vector_results:\n",
    "            text_key = result[\"text\"][:100]\n",
    "            if text_key not in seen_texts:\n",
    "                seen_texts.add(text_key)\n",
    "                merged_results.append(result)\n",
    "        \n",
    "        for result in keyword_results:\n",
    "            text_key = result[\"text\"][:100]\n",
    "            if text_key not in seen_texts:\n",
    "                seen_texts.add(text_key)\n",
    "                merged_results.append(result)\n",
    "        \n",
    "        logger.info(f\"åˆå¹¶åå…±æœ‰ {len(merged_results)} ä¸ªå”¯ä¸€ç»“æœ\")\n",
    "        return merged_results\n",
    "    \n",
    "    def _calculate_rerank_scores(self, query_text: str, texts: List[str]) -> List[float]:\n",
    "        \"\"\"è®¡ç®—é‡æ’åºåˆ†æ•° - ä¿®å¤ç»´åº¦ä¸åŒ¹é…é—®é¢˜\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # ç¡®ä¿æ–‡æœ¬é•¿åº¦ä¸ä¼šè¶…è¿‡æ¨¡å‹é™åˆ¶\n",
    "            max_text_length = self.max_seq_length - len(self.reranker_tokenizer.tokenize(query_text)) - 3  # é¢„ç•™ç©ºé—´ç»™ç‰¹æ®Šæ ‡è®°\n",
    "            if max_text_length < 10:  # ç¡®ä¿æœ‰è¶³å¤Ÿç©ºé—´\n",
    "                max_text_length = 10\n",
    "                \n",
    "            # å¤„ç†æ–‡æœ¬ï¼Œç¡®ä¿ä¸ä¼šè¶…è¿‡æœ€å¤§é•¿åº¦\n",
    "            processed_texts = []\n",
    "            for text in texts:\n",
    "                # æˆªæ–­æ–‡æœ¬ä»¥é€‚åº”æ¨¡å‹é•¿åº¦é™åˆ¶\n",
    "                tokens = self.reranker_tokenizer.tokenize(text)\n",
    "                if len(tokens) > max_text_length:\n",
    "                    tokens = tokens[:max_text_length]\n",
    "                    text = self.reranker_tokenizer.convert_tokens_to_string(tokens)\n",
    "                processed_texts.append(text)\n",
    "            \n",
    "            # æ„é€  (query, doc) å¯¹\n",
    "            sentence_pairs = [[query_text, doc_text] for doc_text in processed_texts]\n",
    "            \n",
    "            # Tokenizeå¤„ç†\n",
    "            inputs = self.reranker_tokenizer(\n",
    "                sentence_pairs,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=self.max_seq_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # ç§»åŠ¨åˆ°è®¾å¤‡\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # æ¨¡å‹å‰å‘ä¼ æ’­\n",
    "            outputs = self.reranker_model(** inputs)\n",
    "            logits = outputs.logits.view(-1,).float()\n",
    "            scores = torch.sigmoid(logits).cpu().tolist()\n",
    "            \n",
    "            return scores\n",
    "    \n",
    "    def _rerank_results(self, query_text: str, results: List[Dict], top_k: int = 10) -> List[Dict]:\n",
    "        \"\"\"ä½¿ç”¨é‡æ’åºæ¨¡å‹å¯¹ç»“æœè¿›è¡Œé‡æ’åº\"\"\"\n",
    "        if not results or not self.reranker_model:\n",
    "            return results[:top_k]\n",
    "        \n",
    "        try:\n",
    "            texts_to_rerank = [result[\"text\"] for result in results]\n",
    "            rerank_scores = self._calculate_rerank_scores(query_text, texts_to_rerank)\n",
    "            \n",
    "            for i, result in enumerate(results):\n",
    "                result[\"rerank_score\"] = rerank_scores[i]\n",
    "                result[\"original_score\"] = result[\"score\"]\n",
    "            \n",
    "            reranked_results = sorted(results, key=lambda x: x[\"rerank_score\"], reverse=True)\n",
    "            logger.info(f\"é‡æ’åºå®Œæˆï¼Œè¿”å›å‰ {min(top_k, len(reranked_results))} ä¸ªç»“æœ\")\n",
    "            return reranked_results[:top_k]\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"é‡æ’åºå¤±è´¥: {e}\")\n",
    "            # å¤±è´¥æ—¶å°è¯•è¿”å›åŸå§‹åˆ†æ•°æ’åºçš„ç»“æœ\n",
    "            return sorted(results, key=lambda x: x[\"score\"], reverse=True)[:top_k]\n",
    "    \n",
    "    def hybrid_search(self, \n",
    "                     query_text: str, \n",
    "                     top_k: int = 10,\n",
    "                     vector_weight: float = 0.6,\n",
    "                     keyword_weight: float = 0.4,\n",
    "                     enable_rerank: bool = True,\n",
    "                     retrieval_size: int = 50) -> Dict[str, Any]:\n",
    "        \"\"\"æ‰§è¡Œæ··åˆæ£€ç´¢\"\"\"\n",
    "        logger.info(f\"å¼€å§‹æ··åˆæ£€ç´¢: '{query_text[:50]}...'\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        vector_results = self._vector_search(query_text, retrieval_size)\n",
    "        keyword_results = self._keyword_search(query_text, retrieval_size)\n",
    "        merged_results = self._merge_and_deduplicate(vector_results, keyword_results)\n",
    "        \n",
    "        if not enable_rerank:\n",
    "            for result in merged_results:\n",
    "                if result[\"search_type\"] == \"vector\":\n",
    "                    result[\"hybrid_score\"] = result[\"score\"] * vector_weight\n",
    "                else:\n",
    "                    result[\"hybrid_score\"] = result[\"score\"] * keyword_weight\n",
    "            \n",
    "            merged_results = sorted(merged_results, key=lambda x: x[\"hybrid_score\"], reverse=True)\n",
    "            final_results = merged_results[:top_k]\n",
    "        else:\n",
    "            final_results = self._rerank_results(query_text, merged_results, top_k)\n",
    "        \n",
    "        execution_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        search_result = {\n",
    "            \"query\": query_text,\n",
    "            \"results\": final_results,\n",
    "            \"total_found\": len(final_results),\n",
    "            \"vector_results_count\": len(vector_results),\n",
    "            \"keyword_results_count\": len(keyword_results),\n",
    "            \"merged_results_count\": len(merged_results),\n",
    "            \"execution_time_seconds\": execution_time,\n",
    "            \"rerank_enabled\": enable_rerank,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"error\": None  # å§‹ç»ˆåŒ…å«erroré”®ï¼Œé»˜è®¤ä¸ºNone\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"æ··åˆæ£€ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(final_results)} ä¸ªç»“æœï¼Œè€—æ—¶ {execution_time:.2f} ç§’\")\n",
    "        return search_result\n",
    "    \n",
    "    def print_search_results(self, search_result: Dict[str, Any], show_full_text: bool = False):\n",
    "        \"\"\"æ ¼å¼åŒ–æ‰“å°æœç´¢ç»“æœ\"\"\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"æŸ¥è¯¢: {search_result['query']}\")\n",
    "        print(f\"æ‰§è¡Œæ—¶é—´: {search_result['execution_time_seconds']:.2f} ç§’\")\n",
    "        print(f\"å‘é‡æœç´¢ç»“æœ: {search_result['vector_results_count']} ä¸ª\")\n",
    "        print(f\"å…³é”®è¯æœç´¢ç»“æœ: {search_result['keyword_results_count']} ä¸ª\")\n",
    "        print(f\"åˆå¹¶åç»“æœ: {search_result['merged_results_count']} ä¸ª\")\n",
    "        print(f\"æœ€ç»ˆè¿”å›: {search_result['total_found']} ä¸ª\")\n",
    "        print(f\"é‡æ’åº: {'å·²å¯ç”¨' if search_result['rerank_enabled'] else 'æœªå¯ç”¨'}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        for i, result in enumerate(search_result['results'], 1):\n",
    "            print(f\"\\n--- ç»“æœ {i} ---\")\n",
    "            print(f\"æ¥æº: {result['source']}\")\n",
    "            print(f\"æœç´¢ç±»å‹: {result['search_type']}\")\n",
    "            \n",
    "            if 'rerank_score' in result:\n",
    "                print(f\"é‡æ’åºåˆ†æ•°: {result['rerank_score']:.4f}\")\n",
    "                print(f\"åŸå§‹åˆ†æ•°: {result['original_score']:.4f}\")\n",
    "            elif 'hybrid_score' in result:\n",
    "                print(f\"æ··åˆåˆ†æ•°: {result['hybrid_score']:.4f}\")\n",
    "            else:\n",
    "                print(f\"åˆ†æ•°: {result['score']:.4f}\")\n",
    "            \n",
    "            text_preview = result['text'][:300] if not show_full_text else result['text']\n",
    "            if len(result['text']) > 300 and not show_full_text:\n",
    "                text_preview += \"...\"\n",
    "            print(f\"å†…å®¹: {text_preview}\")\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"å…³é—­æ‰€æœ‰è¿æ¥\"\"\"\n",
    "        if self.milvus_client:\n",
    "            self.milvus_client.close()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "            logger.info(\"GPUç¼“å­˜å·²æ¸…ç†\")\n",
    "        \n",
    "        logger.info(\"æ··åˆæ£€ç´¢ç³»ç»Ÿå·²å…³é—­\")\n",
    "\n",
    "\n",
    "def hybrid_rag_search(query_text: str, \n",
    "                     top_k: int = 10,\n",
    "                     milvus_db_path: str = \"./milvus_data.db\",\n",
    "                     embedding_model_name_or_path: str = \"maidalun1020/bce-embedding-base_v1\",\n",
    "                     reranker_model_name_or_path: str = \"maidalun1020/bce-reranker-base_v1\",\n",
    "                     es_url: str = \"https://localhost:9200\",\n",
    "                     es_username: str = \"elastic\",\n",
    "                     es_password: str = \"\",\n",
    "                     es_index_name: str = \"chunk_documents\",\n",
    "                     enable_rerank: bool = True,\n",
    "                     max_seq_length: int = 512) -> Dict[str, Any]:\n",
    "    \"\"\"ç‹¬ç«‹çš„æ··åˆæ£€ç´¢æœåŠ¡å‡½æ•°\"\"\"\n",
    "    searcher = None\n",
    "    try:\n",
    "        searcher = HybridRAGSearcher(\n",
    "            milvus_db_path=milvus_db_path,\n",
    "            embedding_model_name_or_path=embedding_model_name_or_path,\n",
    "            reranker_model_name_or_path=reranker_model_name_or_path,\n",
    "            es_url=es_url,\n",
    "            es_username=es_username,\n",
    "            es_password=es_password,\n",
    "            es_index_name=es_index_name,\n",
    "            max_seq_length=max_seq_length\n",
    "        )\n",
    "        \n",
    "        result = searcher.hybrid_search(\n",
    "            query_text=query_text,\n",
    "            top_k=top_k,\n",
    "            enable_rerank=enable_rerank\n",
    "        )\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"æœç´¢æœåŠ¡é”™è¯¯: {e}\")\n",
    "        return {\n",
    "            \"query\": query_text,\n",
    "            \"results\": [],\n",
    "            \"total_found\": 0,\n",
    "            \"error\": str(e),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    finally:\n",
    "        if searcher:\n",
    "            searcher.close()\n",
    "\n",
    "\n",
    "def test_hybrid_search():\n",
    "    \"\"\"æµ‹è¯•æ··åˆæ£€ç´¢ç³»ç»Ÿ\"\"\"\n",
    "    \n",
    "    config = {\n",
    "        \"milvus_db_path\": \"./milvus_data.db\",\n",
    "        \"embedding_model_name_or_path\": \"./embedding-model\",\n",
    "        \"reranker_model_name_or_path\": \"./reranker-model\",\n",
    "        \"es_url\": \"https://localhost:9200\",\n",
    "        \"es_username\": \"elastic\",\n",
    "        \"es_password\": \"vSCQnhBXoox0sRo7-U1x\",\n",
    "        \"es_index_name\": \"chunk_documents\",\n",
    "        \"max_seq_length\": 4096\n",
    "    }\n",
    "    \n",
    "    test_queries = [\n",
    "        \"äººå·¥æ™ºèƒ½çš„å‘å±•è¶‹åŠ¿\",\n",
    "        \"æœºå™¨å­¦ä¹ ç®—æ³•åŸç†\",\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        searcher = HybridRAGSearcher(** config)\n",
    "        \n",
    "        for query in test_queries:\n",
    "            print(f\"\\n{'='*100}\")\n",
    "            print(f\"æµ‹è¯•æŸ¥è¯¢: {query}\")\n",
    "            \n",
    "            print(f\"\\nã€å¯ç”¨é‡æ’åºã€‘\")\n",
    "            result_with_rerank = searcher.hybrid_search(\n",
    "                query_text=query,\n",
    "                top_k=5,\n",
    "                enable_rerank=True\n",
    "            )\n",
    "            searcher.print_search_results(result_with_rerank, show_full_text=False)\n",
    "            \n",
    "            print(f\"\\nã€ç¦ç”¨é‡æ’åºã€‘\")\n",
    "            result_no_rerank = searcher.hybrid_search(\n",
    "                query_text=query,\n",
    "                top_k=3,\n",
    "                enable_rerank=False,\n",
    "                vector_weight=0.6,\n",
    "                keyword_weight=0.4\n",
    "            )\n",
    "            for i, res in enumerate(result_no_rerank['results'], 1):\n",
    "                print(f\"{i}. æ··åˆåˆ†æ•°: {res['hybrid_score']:.4f} | æ¥æº: {res['source']}\")\n",
    "        \n",
    "        print(f\"\\n{'='*100}\")\n",
    "        print(\"æµ‹è¯•ç‹¬ç«‹æœç´¢å‡½æ•°:\")\n",
    "        standalone_result = hybrid_rag_search(\n",
    "            query_text=\"äººå·¥æ™ºèƒ½æŠ€æœ¯åº”ç”¨åœºæ™¯\",\n",
    "            top_k=3,\n",
    "            **config\n",
    "        )\n",
    "        \n",
    "        # ä¿®å¤KeyErroré—®é¢˜ï¼šæ£€æŸ¥erroré”®æ˜¯å¦å­˜åœ¨ä¸”ä¸ä¸ºNone\n",
    "        if \"error\" in standalone_result and standalone_result[\"error\"] is not None:\n",
    "            print(f\"ç‹¬ç«‹å‡½æ•°é”™è¯¯: {standalone_result['error']}\")\n",
    "        else:\n",
    "            print(f\"ç‹¬ç«‹å‡½æ•°æœç´¢ç»“æœ: æ‰¾åˆ° {standalone_result['total_found']} ä¸ªç»“æœ\")\n",
    "            for i, res in enumerate(standalone_result['results'], 1):\n",
    "                score = res.get('rerank_score', res.get('hybrid_score', res['score']))\n",
    "                print(f\"{i}. åˆ†æ•°: {score:.4f} | æ¥æº: {res['source']}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    finally:\n",
    "        if 'searcher' in locals():\n",
    "            searcher.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_hybrid_search()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f7edd04-1b47-4645-8a48-9eeb5f24ab8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 11:38:51,405 - INFO - RAGæ£€ç´¢ç³»ç»Ÿä½¿ç”¨è®¾å¤‡: cuda\n",
      "2025-09-19 11:38:52,584 - INFO - BCEåµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ\n",
      "2025-09-19 11:38:53,817 - INFO - BCEé‡æ’åºæ¨¡å‹åŠ è½½æˆåŠŸ\n",
      "2025-09-19 11:38:53,822 - INFO - Milvusè¿æ¥æˆåŠŸ\n",
      "2025-09-19 11:38:53,870 - INFO - HEAD https://localhost:9200/ [status:200 duration:0.027s]\n",
      "2025-09-19 11:38:53,871 - INFO - Elasticsearchè¿æ¥æˆåŠŸ\n",
      "2025-09-19 11:38:53,871 - INFO - æ··åˆRAGæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "===== å¸¦è¯¦ç»†æ–‡æ¡£å±•ç¤ºçš„RAGé—®ç­”ç³»ç»Ÿï¼ˆåŸºäºDeepSeek-V3.1ï¼‰ =====\n",
      "è¯´æ˜ï¼šè¾“å…¥é—®é¢˜åï¼Œå°†è¿”å›AIå›ç­”+æ£€ç´¢æ–‡æ¡£è¯¦æƒ…ï¼ˆå«300å­—å†…å®¹+æ–‡ä»¶è·¯å¾„ï¼‰\n",
      "è¾“å…¥'é€€å‡º'æˆ–'quit'å¯ç»“æŸç¨‹åº\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š æœºå™¨å­¦ä¹ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 11:38:57,686 - INFO - å¼€å§‹RAGæ£€ç´¢ï¼ŒæŸ¥è¯¢: æœºå™¨å­¦ä¹ ...\n",
      "2025-09-19 11:38:57,712 - INFO - POST https://localhost:9200/chunk_documents/_search [status:200 duration:0.008s]\n",
      "2025-09-19 11:38:57,747 - INFO - RAGæ£€ç´¢å®Œæˆï¼Œè·å– 3 æ¡ç›¸å…³ç»“æœï¼Œè€—æ—¶ 0.06 ç§’\n",
      "2025-09-19 11:38:57,748 - INFO - è°ƒç”¨DeepSeek-V3.1ç”Ÿæˆå›ç­”...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "æ­£åœ¨å¤„ç†é—®é¢˜ï¼šæœºå™¨å­¦ä¹ \n",
      "æ­¥éª¤1/2ï¼šRAGæ£€ç´¢ç›¸å…³æ–‡æ¡£...\n",
      "æ­¥éª¤2/2ï¼šè°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”...\n",
      "==================================================\n",
      "\n",
      "====================================================================================================\n",
      "æœ€ç»ˆç»“æœï¼š\n",
      "\n",
      "# RAGé—®ç­”ç»“æœ\n",
      "## ä¸€ã€ç”¨æˆ·é—®é¢˜\n",
      "æœºå™¨å­¦ä¹ \n",
      "\n",
      "## äºŒã€AIå›ç­”\n",
      "æ ¹æ®æä¾›çš„å‚è€ƒæ–‡æ¡£ï¼Œä»¥ä¸‹æ˜¯å…³äºæœºå™¨å­¦ä¹ çš„å›ç­”ï¼š\n",
      "\n",
      "### 1. **æœºå™¨å­¦ä¹ çš„å®šä¹‰**\n",
      "æœºå™¨å­¦ä¹ ï¼ˆMachine Learning, MLï¼‰æ˜¯äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰çš„æ ¸å¿ƒåˆ†æ”¯ä¹‹ä¸€ï¼Œé€šè¿‡è®¾è®¡ç®—æ³•ä½¿è®¡ç®—æœºä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ è§„å¾‹ã€ä¼˜åŒ–æ¨¡å‹ï¼Œå¹¶åœ¨æœªæ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å®Œæˆé¢„æµ‹ã€åˆ†ç±»æˆ–å†³ç­–ç­‰ä»»åŠ¡ã€‚å…¶æ ¸å¿ƒç›®æ ‡æ˜¯è®©è®¡ç®—æœºåƒäººç±»ä¸€æ ·ä»ç»éªŒä¸­å­¦ä¹ ï¼Œæ€§èƒ½éšæ•°æ®é‡å’Œè®­ç»ƒæ¬¡æ•°çš„å¢åŠ è€Œæå‡ã€å‚è€ƒ1ã€‘ã€‚\n",
      "\n",
      "### 2. **æœºå™¨å­¦ä¹ çš„ä¸»è¦åˆ†ç±»**\n",
      "æœºå™¨å­¦ä¹ æ ¹æ®æ•°æ®æ˜¯å¦åŒ…å«æ ‡ç­¾ä»¥åŠå­¦ä¹ æ–¹å¼çš„å·®å¼‚ï¼Œå¯åˆ†ä¸ºä¸‰å¤§ç±»ï¼ˆå…·ä½“ç±»åˆ«æœªåœ¨æ–‡æ¡£ä¸­è¯¦ç»†åˆ—å‡ºï¼Œä½†æåˆ°äº†åˆ†ç±»ä¾æ®ï¼‰ã€å‚è€ƒ3ã€‘ã€‚\n",
      "\n",
      "### 3. **åº”ç”¨ä¸æ„ä¹‰**\n",
      "æœºå™¨å­¦ä¹ ä»¥â€œæ•°æ®é©±åŠ¨â€çš„æ–¹å¼å®ç°äº†ä»â€œè§„åˆ™ç¼–ç¨‹â€åˆ°â€œè‡ªä¸»å­¦ä¹ â€çš„çªç ´ï¼Œåœ¨è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€é‡‘èã€åŒ»ç–—ç­‰é¢†åŸŸæœ‰æ·±è¿œå½±å“ã€‚å°½ç®¡é¢ä¸´æ•°æ®éšç§å’Œæ¨¡å‹å¯è§£é‡Šæ€§ç­‰æŒ‘æˆ˜ï¼Œå®ƒä»åœ¨æå‡æ™ºèƒ½åŒ–æ°´å¹³ã€è§£å†³å¤æ‚é—®é¢˜å’Œæ¨åŠ¨äº§ä¸šå‡çº§ä¸­å‘æŒ¥é‡è¦ä½œç”¨ï¼Œæ˜¯æœªæ¥æ•°å­—ç¤¾ä¼šçš„æ ¸å¿ƒåŸºç¡€è®¾æ–½ä¹‹ä¸€ã€å‚è€ƒ2ã€‘ã€‚\n",
      "\n",
      "æ³¨ï¼šä»¥ä¸Šå›ç­”ä¸¥æ ¼åŸºäºæä¾›çš„å‚è€ƒæ–‡æ¡£ï¼Œæœªè¡¥å……é¢å¤–ä¿¡æ¯ã€‚æ–‡æ¡£ä¸­æœªæ¶‰åŠæœºå™¨å­¦ä¹ çš„å…·ä½“ç®—æ³•æˆ–åˆ†ç±»ç»†èŠ‚ï¼Œå› æ­¤ä»…æ¦‚æ‹¬æ€§è¯´æ˜ã€‚å¦‚éœ€æ›´è¯¦ç»†å†…å®¹ï¼Œå»ºè®®æä¾›æ›´å¤šç›¸å…³æ–‡æ¡£ã€‚\n",
      "\n",
      "## ä¸‰ã€æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯\n",
      "- æ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£æ•°é‡ï¼š3 æ¡\n",
      "- æ€»æ£€ç´¢è€—æ—¶ï¼š0.06 ç§’ï¼ˆå«å‘é‡æ£€ç´¢ã€å…³é”®è¯æ£€ç´¢ã€é‡æ’åºï¼‰\n",
      "- æ£€ç´¢ç±»å‹ï¼šå‘é‡æ£€ç´¢ï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰+ å…³é”®è¯æ£€ç´¢ï¼ˆå­—é¢åŒ¹é…ï¼‰\n",
      "\n",
      "## å››ã€### è¯¦ç»†æ£€ç´¢æ–‡æ¡£ï¼ˆå…±3æ¡ï¼‰\n",
      "\n",
      "#### æ–‡æ¡£1\n",
      "- **æ¥æºè·¯å¾„ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰**: test_ml_chunks/prod_test_ml_node_3_chunk_0.txt\n",
      "- **å®Œæ•´è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰**: /root/doc_processor/test_ml_chunks/prod_test_ml_node_3_chunk_0.txt\n",
      "- **æ£€ç´¢ç±»å‹**: å‘é‡æ£€ç´¢\n",
      "- **ç›¸å…³æ€§åˆ†æ•°**: 0.5348ï¼ˆåˆ†æ•°è¶Šé«˜è¶Šç›¸å…³ï¼‰\n",
      "- **æ–‡æ¡£æ—¶é—´æˆ³**: 2025-09-19T09:26:50.183964\n",
      "- **300å­—å†…å®¹é¢„è§ˆ**: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_node_3_chunk_0.txt\n",
      "Chunk name: prod_test_ml_node_3_chunk_0.txt\n",
      "==================================================\n",
      "Chunk ID: node_3_chunk_0\n",
      "Source Node: 3\n",
      "Chunk Index: 0\n",
      "Path Titles: 1. å¼•è¨€ > 1.1 æœºå™¨å­¦ä¹ çš„å®šä¹‰\n",
      "==================================================\n",
      "1. å¼•è¨€ > 1.1 æœºå™¨å­¦ä¹ çš„å®šä¹‰\n",
      "\n",
      "æœºå™¨å­¦ä¹ ï¼ˆMachine Learning, MLï¼‰æ˜¯äººå·¥æ™ºèƒ½ï¼ˆArtificial Intelligence, AIï¼‰çš„æ ¸å¿ƒåˆ†æ”¯ä¹‹ä¸€ï¼Œå®ƒé€šè¿‡è®¾è®¡ç®—æ³•ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­è‡ªåŠ¨å­¦ä¹ è§„å¾‹ã€ä¼˜åŒ–æ¨¡å‹ï¼Œå¹¶åœ¨æœªæ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å®Œæˆé¢„æµ‹ã€åˆ†ç±»æˆ–å†³ç­–ç­‰ä»»åŠ¡ã€‚ç®€å•æ¥è¯´ï¼Œæœºå™¨å­¦ä¹ çš„æ ¸å¿ƒç›®æ ‡æ˜¯ â€œè®©è®¡ç®—æœºåƒäººç±»ä¸€æ ·ä»ç»éªŒä¸­å­¦ä¹ â€ï¼Œå…¶æ€§èƒ½ä¼šéšç€æ•°æ®é‡çš„å¢åŠ å’Œè®­ç»ƒæ¬¡æ•°çš„ç§¯ç´¯è€Œé€æ­¥æå‡ã€‚\n",
      "==================================================\n",
      "Metadata: {'original_text_length': 183, 'chunk_text_length': 183, 'full_content_length': 204, 'node_type': 'paragraph', 'has_tables': False, 'table_count': 0, 'is_root_content': False}\n",
      "\n",
      "--------------------------------------------------  # åˆ†éš”çº¿ï¼ŒåŒºåˆ†ä¸åŒæ–‡æ¡£\n",
      "\n",
      "#### æ–‡æ¡£2\n",
      "- **æ¥æºè·¯å¾„ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰**: test_ml_chunks/prod_test_ml_node_82_chunk_0.txt\n",
      "- **å®Œæ•´è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰**: /root/doc_processor/test_ml_chunks/prod_test_ml_node_82_chunk_0.txt\n",
      "- **æ£€ç´¢ç±»å‹**: å…³é”®è¯æ£€ç´¢\n",
      "- **ç›¸å…³æ€§åˆ†æ•°**: 0.5230ï¼ˆåˆ†æ•°è¶Šé«˜è¶Šç›¸å…³ï¼‰\n",
      "- **æ–‡æ¡£æ—¶é—´æˆ³**: 2025-09-19 10:23:35\n",
      "- **300å­—å†…å®¹é¢„è§ˆ**: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_node_82_chunk_0.txt\n",
      "Chunk name: prod_test_ml_node_82_chunk_0.txt\n",
      "==================================================\n",
      "Chunk ID: node_82_chunk_0\n",
      "Source Node: 82\n",
      "Chunk Index: 0\n",
      "Path Titles: 6. æ€»ç»“\n",
      "==================================================\n",
      "6. æ€»ç»“\n",
      "\n",
      "æœºå™¨å­¦ä¹ ä½œä¸ºäººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œé€šè¿‡ â€œæ•°æ®é©±åŠ¨â€ çš„æ–¹å¼ï¼Œå®ç°äº†ä» â€œè§„åˆ™ç¼–ç¨‹â€ åˆ° â€œè‡ªä¸»å­¦ä¹ â€ çš„çªç ´ï¼Œå·²åœ¨è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ã€é‡‘èã€åŒ»ç–—ç­‰é¢†åŸŸäº§ç”Ÿæ·±è¿œå½±å“ã€‚å°½ç®¡å½“å‰é¢ä¸´æ•°æ®éšç§ã€æ¨¡å‹å¯è§£é‡Šæ€§ç­‰æŒ‘æˆ˜ï¼Œä½†éšç€æŠ€æœ¯çš„æŒç»­è¿­ä»£ï¼Œæœºå™¨å­¦ä¹ å°†åœ¨ â€œæå‡æ™ºèƒ½åŒ–æ°´å¹³â€â€œè§£å†³å¤æ‚é—®é¢˜â€â€œæ¨åŠ¨äº§ä¸šå‡çº§â€ ä¸­å‘æŒ¥æ›´é‡è¦çš„ä½œç”¨ï¼Œæˆä¸ºæœªæ¥æ•°å­—ç¤¾ä¼šçš„æ ¸å¿ƒåŸºç¡€è®¾æ–½ä¹‹ä¸€ã€‚å¯¹äºåˆå­¦è€…è€Œè¨€ï¼Œç†è§£æœºå™¨å­¦ä¹ çš„åŸºæœ¬æµç¨‹ã€åˆ†ç±»ä¸æ ¸å¿ƒæ¦‚å¿µï¼Œæ˜¯è¿›ä¸€æ­¥æ·±å…¥å­¦ä¹ å’Œåº”ç”¨çš„å…³é”®åŸºç¡€ã€‚\n",
      "==================================================\n",
      "Metadata: {'original_text_length': 223, 'chunk_text_length': 223, 'full_content_length': 230, 'node_type': 'paragraph', 'has_tables': False, 'table_count': 0, 'is_root_content': False}\n",
      "\n",
      "--------------------------------------------------  # åˆ†éš”çº¿ï¼ŒåŒºåˆ†ä¸åŒæ–‡æ¡£\n",
      "\n",
      "#### æ–‡æ¡£3\n",
      "- **æ¥æºè·¯å¾„ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰**: test_ml_chunks/prod_test_ml_node_23_chunk_0.txt\n",
      "- **å®Œæ•´è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰**: /root/doc_processor/test_ml_chunks/prod_test_ml_node_23_chunk_0.txt\n",
      "- **æ£€ç´¢ç±»å‹**: å…³é”®è¯æ£€ç´¢\n",
      "- **ç›¸å…³æ€§åˆ†æ•°**: 0.5020ï¼ˆåˆ†æ•°è¶Šé«˜è¶Šç›¸å…³ï¼‰\n",
      "- **æ–‡æ¡£æ—¶é—´æˆ³**: 2025-09-19 10:23:35\n",
      "- **300å­—å†…å®¹é¢„è§ˆ**: Chunk path: ./chunks_output/test_ml_chunks/prod_test_ml_node_23_chunk_0.txt\n",
      "Chunk name: prod_test_ml_node_23_chunk_0.txt\n",
      "==================================================\n",
      "Chunk ID: node_23_chunk_0\n",
      "Source Node: 23\n",
      "Chunk Index: 0\n",
      "Path Titles: 3. æœºå™¨å­¦ä¹ çš„ä¸»è¦åˆ†ç±»\n",
      "==================================================\n",
      "3. æœºå™¨å­¦ä¹ çš„ä¸»è¦åˆ†ç±»\n",
      "\n",
      "æ ¹æ®æ•°æ®æ˜¯å¦åŒ…å«æ ‡ç­¾ä»¥åŠå­¦ä¹ æ–¹å¼çš„å·®å¼‚ï¼Œæœºå™¨å­¦ä¹ å¯åˆ†ä¸ºä¸‰å¤§ç±»ï¼Œå„ç±»åˆ«é€‚ç”¨åœºæ™¯ä¸æ ¸å¿ƒç®—æ³•ä¸åŒï¼š\n",
      "==================================================\n",
      "Metadata: {'original_text_length': 46, 'chunk_text_length': 46, 'full_content_length': 60, 'node_type': 'paragraph', 'has_tables': False, 'table_count': 0, 'is_root_content': False}\n",
      "\n",
      "--------------------------------------------------  # åˆ†éš”çº¿ï¼ŒåŒºåˆ†ä¸åŒæ–‡æ¡£\n",
      "  # æ’å…¥è¯¦ç»†æ–‡æ¡£ï¼ˆ300å­—+è·¯å¾„ï¼‰\n",
      "\n",
      "> æ³¨ï¼šè¯¦ç»†æ–‡æ¡£ä¸­çš„â€œå®Œæ•´è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰â€å¯ç›´æ¥å¤åˆ¶åˆ°æ–‡ä»¶ç®¡ç†å™¨æ‰“å¼€ï¼ŒæŸ¥çœ‹æ–‡æ¡£å…¨æ–‡ã€‚\n",
      "\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š é€€å‡º\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 11:39:44,521 - INFO - RAGæ£€ç´¢ç³»ç»Ÿèµ„æºå·²é‡Šæ”¾\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "æ„Ÿè°¢ä½¿ç”¨ï¼Œç¨‹åºå·²é€€å‡ºï¼\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import List, Dict, Any, Optional\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from pymilvus import MilvusClient\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.exceptions import RequestError, ConnectionError\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# -------------------------- 1. åŸºç¡€é…ç½®ï¼ˆç”¨æˆ·å¯æ ¹æ®å®é™…ç¯å¢ƒè°ƒæ•´ï¼‰ --------------------------\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# RAGæ ¸å¿ƒé…ç½®\n",
    "RAG_CONFIG = {\n",
    "    \"milvus_db_path\": \"./milvus_data.db\",\n",
    "    \"milvus_collection_name\": \"testchunks\",\n",
    "    \"embedding_model_path\": \"./embedding-model\",\n",
    "    \"reranker_model_path\": \"./reranker-model\",\n",
    "    \"es_url\": \"https://localhost:9200\",\n",
    "    \"es_username\": \"elastic\",\n",
    "    \"es_password\": \"vSCQnhBXoox0sRo7-U1x\",\n",
    "    \"es_index_name\": \"chunk_documents\",\n",
    "    \"max_seq_length\": 512,\n",
    "    \"rag_top_k\": 3,  # æ£€ç´¢è¿”å›çš„topç›¸å…³ç»“æœæ•°\n",
    "    \"doc_preview_length\": 1000  # æ–‡æ¡£å†…å®¹é¢„è§ˆé•¿åº¦ï¼ˆå›ºå®š300å­—ï¼‰\n",
    "}\n",
    "\n",
    "# DeepSeek-V3.1å¤§æ¨¡å‹é…ç½®\n",
    "LLM_CONFIG = {\n",
    "    \"api_url\": \"https://api.siliconflow.cn/v1/chat/completions\",\n",
    "    \"api_key\": \"sk-ionsbeieleeekwlstqotkyrmictdzshgnbaytavcudxkixcs\",\n",
    "    \"model_name\": \"deepseek-ai/DeepSeek-V3.1\",\n",
    "    \"max_tokens\": 2000,\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_retries\": 3,\n",
    "    \"retry_delay\": 5\n",
    "}\n",
    "\n",
    "# -------------------------- 2. æ··åˆRAGæ£€ç´¢ç³»ç»Ÿï¼ˆä¸å˜ï¼Œå¤ç”¨ä¹‹å‰ç¨³å®šç‰ˆæœ¬ï¼‰ --------------------------\n",
    "class HybridRAGSearcher:\n",
    "    def __init__(self, config: Dict[str, Any]):\n",
    "        self.config = config\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        logger.info(f\"RAGæ£€ç´¢ç³»ç»Ÿä½¿ç”¨è®¾å¤‡: {self.device}\")\n",
    "\n",
    "        self.embedding_tokenizer, self.embedding_model = self._init_embedding_model()\n",
    "        self.reranker_tokenizer, self.reranker_model = self._init_reranker_model()\n",
    "        self.milvus_client = self._init_milvus()\n",
    "        self.es_client = self._init_elasticsearch()\n",
    "\n",
    "        logger.info(\"æ··åˆRAGæ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ\")\n",
    "\n",
    "    def _init_embedding_model(self):\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(self.config[\"embedding_model_path\"])\n",
    "            model = AutoModel.from_pretrained(self.config[\"embedding_model_path\"]).to(self.device)\n",
    "            model.eval()\n",
    "            logger.info(\"BCEåµŒå…¥æ¨¡å‹åŠ è½½æˆåŠŸ\")\n",
    "            return tokenizer, model\n",
    "        except Exception as e:\n",
    "            logger.error(f\"åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _init_reranker_model(self):\n",
    "        try:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(self.config[\"reranker_model_path\"])\n",
    "            model = AutoModelForSequenceClassification.from_pretrained(self.config[\"reranker_model_path\"]).to(self.device)\n",
    "            model.eval()\n",
    "            logger.info(\"BCEé‡æ’åºæ¨¡å‹åŠ è½½æˆåŠŸ\")\n",
    "            return tokenizer, model\n",
    "        except Exception as e:\n",
    "            logger.error(f\"é‡æ’åºæ¨¡å‹åŠ è½½å¤±è´¥: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _init_milvus(self):\n",
    "        try:\n",
    "            client = MilvusClient(self.config[\"milvus_db_path\"])\n",
    "            collections = client.list_collections()\n",
    "            if self.config[\"milvus_collection_name\"] not in collections:\n",
    "                raise ValueError(f\"Milvusé›†åˆ '{self.config['milvus_collection_name']}' ä¸å­˜åœ¨\")\n",
    "            logger.info(\"Milvusè¿æ¥æˆåŠŸ\")\n",
    "            return client\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Milvusåˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _init_elasticsearch(self):\n",
    "        try:\n",
    "            client = Elasticsearch(\n",
    "                [self.config[\"es_url\"]],\n",
    "                basic_auth=(self.config[\"es_username\"], self.config[\"es_password\"]),\n",
    "                verify_certs=False,\n",
    "                ssl_show_warn=False\n",
    "            )\n",
    "            if client.ping():\n",
    "                logger.info(\"Elasticsearchè¿æ¥æˆåŠŸ\")\n",
    "                return client\n",
    "            else:\n",
    "                raise ConnectionError(\"Elasticsearch pingå¤±è´¥\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Elasticsearchåˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _generate_embedding(self, texts: List[str]) -> np.ndarray:\n",
    "        with torch.no_grad():\n",
    "            inputs = self.embedding_tokenizer(\n",
    "                texts, padding=True, truncation=True, max_length=self.config[\"max_seq_length\"], return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            outputs = self.embedding_model(**inputs)\n",
    "            cls_emb = outputs.last_hidden_state[:, 0, :]\n",
    "            return (cls_emb / cls_emb.norm(dim=1, keepdim=True)).cpu().numpy()\n",
    "\n",
    "    def _vector_search(self, query: str) -> List[Dict]:\n",
    "        try:\n",
    "            query_emb = self._generate_embedding([query])[0].tolist()\n",
    "            results = self.milvus_client.search(\n",
    "                collection_name=self.config[\"milvus_collection_name\"],\n",
    "                anns_field=\"vector\",\n",
    "                data=[query_emb],\n",
    "                limit=self.config[\"rag_top_k\"] * 2,\n",
    "                output_fields=[\"text\", \"folder\", \"file\", \"timestamp\"]\n",
    "            )\n",
    "            return [\n",
    "                {\n",
    "                    \"id\": f\"milvus_{hit['id']}\",\n",
    "                    \"text\": hit[\"entity\"][\"text\"],\n",
    "                    \"source\": f\"{hit['entity']['folder']}/{hit['entity']['file']}\",  # å®Œæ•´è·¯å¾„ï¼ˆæ–‡ä»¶å¤¹/æ–‡ä»¶åï¼‰\n",
    "                    \"full_source_path\": os.path.abspath(f\"{hit['entity']['folder']}/{hit['entity']['file']}\"),  # ç»å¯¹è·¯å¾„\n",
    "                    \"score\": 1.0 / (1.0 + hit[\"distance\"]),\n",
    "                    \"search_type\": \"å‘é‡æ£€ç´¢\",\n",
    "                    \"timestamp\": hit[\"entity\"].get(\"timestamp\", \"\")\n",
    "                }\n",
    "                for hit in results[0]\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"å‘é‡æœç´¢å¤±è´¥: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _keyword_search(self, query: str) -> List[Dict]:\n",
    "        try:\n",
    "            response = self.es_client.search(\n",
    "                index=self.config[\"es_index_name\"],\n",
    "                body={\"query\": {\"match\": {\"content\": query}}, \"size\": self.config[\"rag_top_k\"] * 2}\n",
    "            )\n",
    "            return [\n",
    "                {\n",
    "                    \"id\": f\"es_{hit['_id']}\",\n",
    "                    \"text\": hit[\"_source\"][\"content\"],\n",
    "                    \"source\": f\"{hit['_source']['folder']}/{hit['_source']['filename']}\",  # å®Œæ•´è·¯å¾„ï¼ˆæ–‡ä»¶å¤¹/æ–‡ä»¶åï¼‰\n",
    "                    \"full_source_path\": os.path.abspath(f\"{hit['_source']['folder']}/{hit['_source']['filename']}\"),  # ç»å¯¹è·¯å¾„\n",
    "                    \"score\": hit[\"_score\"] / 100.0,\n",
    "                    \"search_type\": \"å…³é”®è¯æ£€ç´¢\",\n",
    "                    \"timestamp\": hit[\"_source\"].get(\"import_time\", \"\")\n",
    "                }\n",
    "                for hit in response[\"hits\"][\"hits\"]\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"å…³é”®è¯æœç´¢å¤±è´¥: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _merge_deduplicate(self, vector_res: List[Dict], keyword_res: List[Dict]) -> List[Dict]:\n",
    "        seen = set()\n",
    "        merged = []\n",
    "        for res in vector_res + keyword_res:\n",
    "            text_key = res[\"text\"][:100]\n",
    "            if text_key not in seen:\n",
    "                seen.add(text_key)\n",
    "                merged.append(res)\n",
    "        return sorted(merged, key=lambda x: x[\"score\"], reverse=True)[:self.config[\"rag_top_k\"] * 3]\n",
    "\n",
    "    def _rerank(self, query: str, results: List[Dict]) -> List[Dict]:\n",
    "        if not results:\n",
    "            return []\n",
    "        try:\n",
    "            max_doc_len = self.config[\"max_seq_length\"] - len(self.reranker_tokenizer.tokenize(query)) - 3\n",
    "            pairs = [[query, res[\"text\"][:max_doc_len]] for res in results]\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                inputs = self.reranker_tokenizer(\n",
    "                    pairs, padding=True, truncation=True, max_length=self.config[\"max_seq_length\"], return_tensors=\"pt\"\n",
    "                ).to(self.device)\n",
    "                scores = torch.sigmoid(self.reranker_model(**inputs).logits.view(-1,)).cpu().tolist()\n",
    "            \n",
    "            for res, score in zip(results, scores):\n",
    "                res[\"rerank_score\"] = score\n",
    "            return sorted(results, key=lambda x: x[\"rerank_score\"], reverse=True)[:self.config[\"rag_top_k\"]]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"é‡æ’åºå¤±è´¥: {e}\")\n",
    "            return sorted(results, key=lambda x: x[\"score\"], reverse=True)[:self.config[\"rag_top_k\"]]\n",
    "\n",
    "    def search(self, query: str) -> List[Dict]:\n",
    "        logger.info(f\"å¼€å§‹RAGæ£€ç´¢ï¼ŒæŸ¥è¯¢: {query[:50]}...\")\n",
    "        start = datetime.now()\n",
    "        vector_res = self._vector_search(query)\n",
    "        keyword_res = self._keyword_search(query)\n",
    "        merged_res = self._merge_deduplicate(vector_res, keyword_res)\n",
    "        final_res = self._rerank(query, merged_res)\n",
    "        \n",
    "        # è¡¥å……æ£€ç´¢è€—æ—¶ï¼ˆæ¯ä¸ªæ–‡æ¡£éƒ½å¸¦ï¼Œä¾¿äºåç»­å±•ç¤ºï¼‰\n",
    "        retrieval_time = (datetime.now() - start).total_seconds()\n",
    "        for res in final_res:\n",
    "            res[\"retrieval_time\"] = retrieval_time\n",
    "        logger.info(f\"RAGæ£€ç´¢å®Œæˆï¼Œè·å– {len(final_res)} æ¡ç›¸å…³ç»“æœï¼Œè€—æ—¶ {retrieval_time:.2f} ç§’\")\n",
    "        return final_res\n",
    "\n",
    "    def close(self):\n",
    "        if self.milvus_client:\n",
    "            self.milvus_client.close()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        logger.info(\"RAGæ£€ç´¢ç³»ç»Ÿèµ„æºå·²é‡Šæ”¾\")\n",
    "\n",
    "# -------------------------- 3. æ–°å¢ï¼šæ–‡æ¡£ä¿¡æ¯æ ¼å¼åŒ–ï¼ˆé‡ç‚¹ï¼š300å­—å†…å®¹+å®Œæ•´è·¯å¾„ï¼‰ --------------------------\n",
    "def format_detailed_documents(rag_results: List[Dict], preview_length: int = 300) -> str:\n",
    "    \"\"\"\n",
    "    æ ¼å¼åŒ–æ£€ç´¢æ–‡æ¡£è¯¦æƒ…ï¼šåŒ…å«å®Œæ•´è·¯å¾„ï¼ˆç›¸å¯¹+ç»å¯¹ï¼‰ã€æ£€ç´¢ç±»å‹ã€åˆ†æ•°ã€300å­—å†…å®¹\n",
    "    Args:\n",
    "        rag_results: RAGæ£€ç´¢ç»“æœåˆ—è¡¨\n",
    "        preview_length: æ–‡æ¡£å†…å®¹é¢„è§ˆé•¿åº¦ï¼ˆé»˜è®¤300å­—ï¼‰\n",
    "    Returns:\n",
    "        ç»“æ„åŒ–çš„æ–‡æ¡£è¯¦æƒ…å­—ç¬¦ä¸²\n",
    "    \"\"\"\n",
    "    if not rag_results:\n",
    "        return \"ã€æœªæ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ã€‘\\n\"\n",
    "    \n",
    "    detailed_str = \"### è¯¦ç»†æ£€ç´¢æ–‡æ¡£ï¼ˆå…±{}æ¡ï¼‰\\n\".format(len(rag_results))\n",
    "    for idx, doc in enumerate(rag_results, 1):\n",
    "        # 1. å¤„ç†æ–‡æ¡£å†…å®¹ï¼šå›ºå®šæˆªå–300å­—ï¼Œä¸è¶³åˆ™å…¨æ˜¾ï¼Œè¶…è¿‡åŠ çœç•¥å·\n",
    "        doc_text = doc[\"text\"].strip()\n",
    "        if len(doc_text) <= preview_length:\n",
    "            preview_text = doc_text\n",
    "            ellipsis = \"\"\n",
    "        else:\n",
    "            # æˆªå–å‰preview_lengthå­—ï¼Œé¿å…æˆªæ–­åœ¨ä¸­é—´ï¼ˆç®€å•å¤„ç†ï¼šä»¥ä¸­æ–‡å¥æœ«ç¬¦å·ç»“æŸï¼‰\n",
    "            preview_text = doc_text[:preview_length]\n",
    "            # è‹¥æœ€åä¸€ä¸ªå­—ç¬¦ä¸æ˜¯å¥æœ«ç¬¦å·ï¼Œæ‰¾æœ€è¿‘çš„å¥æœ«ç¬¦å·æˆªæ–­\n",
    "            end_symbols = [\"ã€‚\", \"ï¼\", \"ï¼Ÿ\", \"ï¼›\", \"ã€‘\", \")\", \"}\"]\n",
    "            last_symbol_idx = max([preview_text.rfind(s) for s in end_symbols if s in preview_text], default=-1)\n",
    "            if last_symbol_idx != -1 and last_symbol_idx > preview_length * 0.7:  # ç¡®ä¿ä¿ç•™70%ä»¥ä¸Šå†…å®¹\n",
    "                preview_text = preview_text[:last_symbol_idx + 1]\n",
    "            ellipsis = \"...\"\n",
    "        \n",
    "        # 2. æ‹¼æ¥æ–‡æ¡£è¯¦æƒ…ï¼ˆè·¯å¾„åˆ†ç›¸å¯¹+ç»å¯¹ï¼Œä¾¿äºç”¨æˆ·å®šä½æ–‡ä»¶ï¼‰\n",
    "        detailed_str += f\"\"\"\n",
    "#### æ–‡æ¡£{idx}\n",
    "- **æ¥æºè·¯å¾„ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰**: {doc[\"source\"]}\n",
    "- **å®Œæ•´è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰**: {doc[\"full_source_path\"]}\n",
    "- **æ£€ç´¢ç±»å‹**: {doc[\"search_type\"]}\n",
    "- **ç›¸å…³æ€§åˆ†æ•°**: {doc.get(\"rerank_score\", doc[\"score\"]):.4f}ï¼ˆåˆ†æ•°è¶Šé«˜è¶Šç›¸å…³ï¼‰\n",
    "- **æ–‡æ¡£æ—¶é—´æˆ³**: {doc[\"timestamp\"] if doc[\"timestamp\"] else \"æœªè®°å½•\"}\n",
    "- **300å­—å†…å®¹é¢„è§ˆ**: {preview_text}{ellipsis}\n",
    "\n",
    "{\"-\"*50}  # åˆ†éš”çº¿ï¼ŒåŒºåˆ†ä¸åŒæ–‡æ¡£\n",
    "\"\"\"\n",
    "    return detailed_str\n",
    "\n",
    "# -------------------------- 4. å¤§æ¨¡å‹è°ƒç”¨ä¸ç»“æœæ•´åˆï¼ˆæ–°å¢æ–‡æ¡£è¯¦æƒ…å±•ç¤ºï¼‰ --------------------------\n",
    "def format_rag_for_llm(rag_results: List[Dict]) -> str:\n",
    "    \"\"\"ç»™å¤§æ¨¡å‹çš„å‚è€ƒæ–‡æ¡£ï¼ˆç®€æ´ç‰ˆï¼Œä¸å½±å“å›ç­”ç”Ÿæˆï¼‰\"\"\"\n",
    "    if not rag_results:\n",
    "        return \"ã€æ— ç›¸å…³å‚è€ƒæ–‡æ¡£ã€‘\"\n",
    "    llm_ref = \"ã€ç›¸å…³å‚è€ƒæ–‡æ¡£ã€‘\\n\"\n",
    "    for i, res in enumerate(rag_results, 1):\n",
    "        preview = res[\"text\"]\n",
    "        llm_ref += f\"{i}. æ¥æºï¼š{res['source']} | å†…å®¹ï¼š{preview}\\n\"\n",
    "    return llm_ref\n",
    "\n",
    "def call_deepseek_v31(prompt: str, system_prompt: str) -> str:\n",
    "    \"\"\"è°ƒç”¨DeepSeek-V3.1å¤§æ¨¡å‹ç”Ÿæˆå›ç­”\"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {LLM_CONFIG['api_key']}\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": LLM_CONFIG[\"model_name\"],\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": LLM_CONFIG[\"max_tokens\"],\n",
    "        \"temperature\": LLM_CONFIG[\"temperature\"],\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    for retry in range(LLM_CONFIG[\"max_retries\"]):\n",
    "        try:\n",
    "            response = requests.post(LLM_CONFIG[\"api_url\"], json=payload, headers=headers, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            if \"choices\" in result and result[\"choices\"]:\n",
    "                return result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            else:\n",
    "                raise ValueError(f\"å¤§æ¨¡å‹è¿”å›æ ¼å¼å¼‚å¸¸: {result}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"å¤§æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼ˆç¬¬{retry+1}/{LLM_CONFIG['max_retries']}æ¬¡ï¼‰: {e}\")\n",
    "            if retry < LLM_CONFIG[\"max_retries\"] - 1:\n",
    "                time.sleep(LLM_CONFIG[\"retry_delay\"])\n",
    "    return \"æŠ±æ­‰ï¼Œå¤§æ¨¡å‹è°ƒç”¨å¤šæ¬¡å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•ã€‚\"\n",
    "\n",
    "def rag_qa_pipeline(query: str, rag_searcher: HybridRAGSearcher) -> str:\n",
    "    \"\"\"ç«¯åˆ°ç«¯RAGé—®ç­”æµç¨‹ï¼šæ£€ç´¢â†’ç”Ÿæˆå›ç­”â†’é™„åŠ è¯¦ç»†æ–‡æ¡£\"\"\"\n",
    "    # 1. RAGæ£€ç´¢è·å–ç»“æœ\n",
    "    rag_results = rag_searcher.search(query)\n",
    "    # 2. ç”Ÿæˆå¤§æ¨¡å‹æ‰€éœ€çš„ç®€æ´å‚è€ƒ\n",
    "    llm_ref = format_rag_for_llm(rag_results)\n",
    "    # 3. ç”Ÿæˆç”¨æˆ·æ‰€éœ€çš„è¯¦ç»†æ–‡æ¡£è¯¦æƒ…ï¼ˆ300å­—+è·¯å¾„ï¼‰\n",
    "    detailed_docs = format_detailed_documents(\n",
    "        rag_results, \n",
    "        preview_length=RAG_CONFIG[\"doc_preview_length\"]\n",
    "    )\n",
    "\n",
    "    # 4. å¤§æ¨¡å‹Promptï¼ˆå¼•å¯¼åŸºäºå‚è€ƒå›ç­”ï¼‰\n",
    "    system_prompt = \"\"\"ä½ æ˜¯åŸºäºæ£€ç´¢å¢å¼ºï¼ˆRAGï¼‰çš„ä¸“ä¸šé—®ç­”åŠ©æ‰‹ï¼Œä¸¥æ ¼éµå¾ªï¼š\n",
    "1. å¿…é¡»ä¼˜å…ˆä½¿ç”¨ã€ç›¸å…³å‚è€ƒæ–‡æ¡£æ‘˜è¦ã€‘ä¸­çš„ä¿¡æ¯å›ç­”ï¼Œæ¯ä¸ªç»“è®ºéœ€æ ‡æ³¨å¯¹åº”æ–‡æ¡£ç¼–å·ï¼ˆå¦‚ã€å‚è€ƒ1ã€‘ï¼‰ï¼›\n",
    "2. è‹¥å‚è€ƒæ–‡æ¡£ä¿¡æ¯ä¸è¶³ï¼Œè¡¥å……çŸ¥è¯†æ—¶éœ€æ ‡æ³¨â€œæ³¨ï¼šä»¥ä¸‹å†…å®¹åŸºäºæ¨¡å‹è‡ªèº«çŸ¥è¯†è¡¥å……â€ï¼›\n",
    "3. å›ç­”é€»è¾‘æ¸…æ™°ï¼Œåˆ†ç‚¹è¯´æ˜ï¼ˆé€‚ç”¨æ—¶ï¼‰ï¼Œä¸ç¼–é€ ä¿¡æ¯ï¼Œæ— æ³•å›ç­”åˆ™ç›´æ¥è¯´æ˜ã€‚\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"ç”¨æˆ·é—®é¢˜ï¼š{query}\n",
    "\n",
    "{llm_ref}\n",
    "\n",
    "è¯·åŸºäºä¸Šè¿°å‚è€ƒæ–‡æ¡£ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ã€‚\"\"\"\n",
    "\n",
    "    # 5. è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”\n",
    "    logger.info(\"è°ƒç”¨DeepSeek-V3.1ç”Ÿæˆå›ç­”...\")\n",
    "    answer = call_deepseek_v31(user_prompt, system_prompt)\n",
    "\n",
    "    # 6. æ•´åˆæœ€ç»ˆè¾“å‡ºï¼ˆå›ç­” + è¯¦ç»†æ–‡æ¡£ï¼‰\n",
    "    final_output = f\"\"\"\n",
    "# RAGé—®ç­”ç»“æœ\n",
    "## ä¸€ã€ç”¨æˆ·é—®é¢˜\n",
    "{query}\n",
    "\n",
    "## äºŒã€AIå›ç­”\n",
    "{answer}\n",
    "\n",
    "## ä¸‰ã€æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯\n",
    "- æ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£æ•°é‡ï¼š{len(rag_results)} æ¡\n",
    "- æ€»æ£€ç´¢è€—æ—¶ï¼š{rag_results[0][\"retrieval_time\"]:.2f} ç§’ï¼ˆå«å‘é‡æ£€ç´¢ã€å…³é”®è¯æ£€ç´¢ã€é‡æ’åºï¼‰\n",
    "- æ£€ç´¢ç±»å‹ï¼šå‘é‡æ£€ç´¢ï¼ˆè¯­ä¹‰åŒ¹é…ï¼‰+ å…³é”®è¯æ£€ç´¢ï¼ˆå­—é¢åŒ¹é…ï¼‰\n",
    "\n",
    "## å››ã€{detailed_docs}  # æ’å…¥è¯¦ç»†æ–‡æ¡£ï¼ˆ300å­—+è·¯å¾„ï¼‰\n",
    "\n",
    "> æ³¨ï¼šè¯¦ç»†æ–‡æ¡£ä¸­çš„â€œå®Œæ•´è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰â€å¯ç›´æ¥å¤åˆ¶åˆ°æ–‡ä»¶ç®¡ç†å™¨æ‰“å¼€ï¼ŒæŸ¥çœ‹æ–‡æ¡£å…¨æ–‡ã€‚\n",
    "\"\"\"\n",
    "    return final_output\n",
    "\n",
    "# -------------------------- 5. ä¸»å‡½æ•°ï¼ˆäº¤äº’å…¥å£ï¼‰ --------------------------\n",
    "def main():\n",
    "    rag_searcher = None\n",
    "    try:\n",
    "        rag_searcher = HybridRAGSearcher(RAG_CONFIG)\n",
    "        print(\"=\"*100)\n",
    "        print(\"===== å¸¦è¯¦ç»†æ–‡æ¡£å±•ç¤ºçš„RAGé—®ç­”ç³»ç»Ÿï¼ˆåŸºäºDeepSeek-V3.1ï¼‰ =====\")\n",
    "        print(\"è¯´æ˜ï¼šè¾“å…¥é—®é¢˜åï¼Œå°†è¿”å›AIå›ç­”+æ£€ç´¢æ–‡æ¡£è¯¦æƒ…ï¼ˆå«300å­—å†…å®¹+æ–‡ä»¶è·¯å¾„ï¼‰\")\n",
    "        print(\"è¾“å…¥'é€€å‡º'æˆ–'quit'å¯ç»“æŸç¨‹åº\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        while True:\n",
    "            user_query = input(\"\\nè¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š\").strip()\n",
    "            if user_query.lower() in [\"é€€å‡º\", \"quit\", \"exit\"]:\n",
    "                print(\"\\næ„Ÿè°¢ä½¿ç”¨ï¼Œç¨‹åºå·²é€€å‡ºï¼\")\n",
    "                break\n",
    "            if not user_query:\n",
    "                print(\"è¯·è¾“å…¥æœ‰æ•ˆçš„é—®é¢˜ï¼Œä¸èƒ½ä¸ºç©ºï¼\")\n",
    "                continue\n",
    "            \n",
    "            # æ‰§è¡Œé—®ç­”æµç¨‹å¹¶æ‰“å°ç»“æœ\n",
    "            print(\"\\n\" + \"=\"*50)\n",
    "            print(f\"æ­£åœ¨å¤„ç†é—®é¢˜ï¼š{user_query}\")\n",
    "            print(\"æ­¥éª¤1/2ï¼šRAGæ£€ç´¢ç›¸å…³æ–‡æ¡£...\")\n",
    "            print(\"æ­¥éª¤2/2ï¼šè°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›ç­”...\")\n",
    "            print(\"=\"*50)\n",
    "            \n",
    "            final_result = rag_qa_pipeline(user_query, rag_searcher)\n",
    "            print(\"\\n\" + \"=\"*100)\n",
    "            print(\"æœ€ç»ˆç»“æœï¼š\")\n",
    "            print(final_result)\n",
    "            print(\"=\"*100)\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}\")\n",
    "        print(f\"\\né”™è¯¯ï¼šç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥ï¼ŒåŸå› ï¼š{str(e)}\")\n",
    "        print(\"è¯·æ£€æŸ¥Milvus/ElasticsearchæœåŠ¡æ˜¯å¦å¯åŠ¨ï¼Œæˆ–é…ç½®å‚æ•°æ˜¯å¦æ­£ç¡®ã€‚\")\n",
    "    finally:\n",
    "        if rag_searcher:\n",
    "            rag_searcher.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2e226-1c50-4122-96b0-43c2ff68bede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
